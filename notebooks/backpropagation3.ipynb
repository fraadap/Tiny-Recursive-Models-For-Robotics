{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libero\n",
    "\n",
    "download del dataset Libero \n",
    "python3 ./LIBERO/benchmark_scripts/download_libero_datasets.py --datasets libero_goal --use-huggingface \n",
    "mv ./LIBERO/libero/datasets/* ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# The Patch\n",
    "mock_mpl = MagicMock()\n",
    "sys.modules[\"matplotlib\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.pyplot\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.cm\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.colors\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.transforms\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.ticker\"] = mock_mpl\n",
    "sys.modules[\"matplotlib._path\"] = mock_mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file: dataset/libero_goal/pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate_demo.hdf5\n",
      "An error occurred: [Errno 2] Unable to synchronously open file (unable to open file: name = 'dataset/libero_goal/pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate_demo.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Path to your demo file\n",
    "file_path = \"dataset/libero_goal/pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate_demo.hdf5\"\n",
    "\n",
    "print(f\"Opening file: {file_path}\")\n",
    "\n",
    "try:\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        # Loop through all demos in the file\n",
    "        for demo_name in f[\"data\"]:\n",
    "            print(f\"\\n=== Demo: {demo_name} ===\")\n",
    "            \n",
    "            # Access the image data (AgentView RGB)\n",
    "            # Note: Ensure this path exists in your specific HDF5 structure\n",
    "            if \"obs/agentview_rgb\" in f[f\"data/{demo_name}\"]:\n",
    "                dataset = f[f\"data/{demo_name}/obs/agentview_rgb\"]\n",
    "                num_images = dataset.shape[0]\n",
    "                print(f\"Total frames: {num_images}\")\n",
    "                \n",
    "                # Pick indices: every 15th frame + the last one\n",
    "                indices = list(range(0, num_images, 15))\n",
    "                if num_images - 1 not in indices:\n",
    "                    indices.append(num_images - 1)\n",
    "                \n",
    "                # Display images horizontally using HTML/PIL (No Matplotlib)\n",
    "                images_html = []\n",
    "                for idx in indices:\n",
    "                    img_array = dataset[idx]\n",
    "                    \n",
    "                    # Convert numpy array to PIL Image\n",
    "                    # (Robosuite images are usually already correct, but sometimes flipped)\n",
    "                    img = Image.fromarray(img_array)\n",
    "                    \n",
    "                    # Resize for smaller display if needed\n",
    "                    img_small = img.resize((128, 128)) \n",
    "                    \n",
    "                    # Hack to display inline in loop\n",
    "                    print(f\"Frame {idx}:\")\n",
    "                    #display(img)\n",
    "            else:\n",
    "                print(f\"Skipping {demo_name}: 'obs/agentview_rgb' not found.\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.1+cu128\n",
      "âœ… GPU Disponibile: NVIDIA GeForce RTX 3090 Ti\n",
      "Einops installato correttamente.\n",
      "âœ… Ambiente pronto per il modello TinyRecursive.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Check Einops (we'll use it a lot in the recursive model)\n",
    "try:\n",
    "    from einops import rearrange\n",
    "    print(\"âœ… Einops available\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Please install einops: pip install einops\")\n",
    "\n",
    "# Check timm for vision encoder\n",
    "try:\n",
    "    import timm\n",
    "    print(\"âœ… timm available\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Please install timm: pip install timm\")\n",
    "\n",
    "# Check transformers for text encoder\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"âœ… transformers available\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Please install transformers: pip install transformers\")\n",
    "\n",
    "print(\"âœ… Environment ready for the TinyRecursive model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Base configuration for reproducibility (important for thesis)\n",
    "torch.manual_seed(12345)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(12345)\n",
    "    torch.cuda.manual_seed_all(12345)  # If using multiple GPUs\n",
    "\n",
    "print(\"âœ… Environment ready for the TinyRecursive model.\")\n",
    "\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Structured container for training/model hyperparameters.\"\"\"\n",
    "\n",
    "    lr: float = 3e-4\n",
    "    hidden_dim: int = 256\n",
    "    num_recursions: int = 8\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 64\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    sched_T0: Optional[int] = None\n",
    "    sched_T_mult: int = 1\n",
    "    lr_min: float = 1e-6\n",
    "    warmup_epochs: int = 3\n",
    "    early_stop_patience: Optional[int] = None\n",
    "    save_path: str = 'best_model.pt'\n",
    "    freeze_backbone: bool = True\n",
    "    augmentation: bool = False\n",
    "    attention_heads: int = 8\n",
    "    attention_dropout: float = 0.1\n",
    "    text_encoder: str = 'bert-base-uncased'\n",
    "    max_text_length: int = 256\n",
    "    scheduler: str = 'warmup_cosine'\n",
    "    dropout: float = 0.2\n",
    "\n",
    "    # -- PREVIOUS ACTION SUPPORT --\n",
    "    use_previous_actions: bool = False\n",
    "    previous_action_steps: int = 4\n",
    "    action_fusion_strategy: str = 'weighted_sum'  \n",
    "    weight_learning_rate: float = 1e-3\n",
    "    weight_decay_fusion: float = 1e-3\n",
    "    freeze_weights_epochs: int = 5\n",
    "\n",
    "    # Seed for reproducibility\n",
    "    seed: int = 12345\n",
    "    \n",
    "    # Benchmark\n",
    "    benchmark: str = 'libero_goal'\n",
    "    task_filter: Optional[List[str]] = None\n",
    "    dataset_path: str = 'dataset/libero_goal'\n",
    "    demo_split_ratio: float = 0.8\n",
    "    max_demos_per_task: Optional[int] = None\n",
    "    \n",
    "    # Loss function\n",
    "    loss_type: str = 'mse'\n",
    "    reconstruction_weight: float = 0.0\n",
    "    \n",
    "    # Model specification\n",
    "    model_type: str = 'text_encoder_plus'\n",
    "    \n",
    "    # Validation\n",
    "    val_frequency: int = 5\n",
    "    video_frequency: int = 50\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer: str = 'AdamW'\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.999\n",
    "    eps: float = 1e-8\n",
    "    \n",
    "    # Performance\n",
    "    device: Optional[str] = None\n",
    "    mixed_precision: bool = False\n",
    "    num_workers: int = 4\n",
    "    pin_memory: bool = True\n",
    "    cache_data: bool = False\n",
    "\n",
    "@dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Nessun file HDF5 trovato in dataset/libero_spatia\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# --- HELPER FUNCTIONS (Kept largely the same) ---\n",
    "\n",
    "def load_images_robust(dataset):\n",
    "    \"\"\"\n",
    "    Carica immagini da dataset HDF5 usando metodo robusto.\n",
    "    \"\"\"\n",
    "    shape = dataset.shape\n",
    "    \n",
    "    # METODO 1: Lettura diretta uint8\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.uint8)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # METODO 2: Float32 -> Uint8\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float32)\n",
    "        dataset.read_direct(buffer)\n",
    "        if buffer.max() <= 1.0:\n",
    "            buffer = (buffer * 255).astype(np.uint8)\n",
    "        else:\n",
    "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # METODO 3: Float64 -> Uint8\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float64)\n",
    "        dataset.read_direct(buffer)\n",
    "        if buffer.max() <= 1.0:\n",
    "            buffer = (buffer * 255).astype(np.uint8)\n",
    "        else:\n",
    "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # METODO 4: Fallback bytes\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.uint8)\n",
    "        dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
    "        return buffer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Impossibile leggere il dataset: {e}\")\n",
    "\n",
    "def load_actions_robust(dataset):\n",
    "    \"\"\"\n",
    "    Carica azioni da dataset HDF5.\n",
    "    \"\"\"\n",
    "    shape = dataset.shape\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float32)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float64)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Impossibile leggere le azioni: {e}\")\n",
    "\n",
    "# --- MODIFIED EXPLORATION FUNCTION (No Matplotlib) ---\n",
    "\n",
    "def explore_libero_dataset(data_path: Path):\n",
    "    \n",
    "    # Trova file\n",
    "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
    "    \n",
    "    if not hdf5_files:\n",
    "        print(f\"âš ï¸ Nessun file HDF5 trovato in {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"âœ… Trovati {len(hdf5_files)} file HDF5\")\n",
    "    \n",
    "    # Analizza il primo file\n",
    "    demo_file = hdf5_files[0]\n",
    "    print(f\"\\nðŸ“„ Analizzando: {demo_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(demo_file, 'r') as f:\n",
    "            if 'data' not in f:\n",
    "                print(\"âš ï¸ Chiave 'data' non trovata\")\n",
    "                return hdf5_files\n",
    "            \n",
    "            data_group = f['data']\n",
    "            demo_keys = list(data_group.keys())\n",
    "            first_demo_key = demo_keys[0]\n",
    "            demo_0 = data_group[first_demo_key]\n",
    "            \n",
    "            imgs = None\n",
    "            \n",
    "            # 1. Caricamento Immagini\n",
    "            if 'obs' in demo_0:\n",
    "                obs_group = demo_0['obs']\n",
    "                \n",
    "                # Strategia di ricerca chiave immagine\n",
    "                image_keys = ['agentview_rgb', 'agentview_image', 'rgb', 'image', 'robot0_eye_in_hand_image']\n",
    "                img_key = next((k for k in image_keys if k in obs_group), None)\n",
    "                \n",
    "                # Fallback ricerca generica\n",
    "                if img_key is None:\n",
    "                    img_key = next((k for k in obs_group.keys() if 'rgb' in k.lower() or 'image' in k.lower()), None)\n",
    "                \n",
    "                if img_key:\n",
    "                    print(f\"\\nðŸ–¼ï¸ Usando chiave immagini: '{img_key}'\")\n",
    "                    try:\n",
    "                        imgs = load_images_robust(obs_group[img_key])\n",
    "                        print(f\"  âœ… Immagini caricate: {imgs.shape}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Errore immagini: {e}\")\n",
    "            \n",
    "            # 2. Caricamento Azioni\n",
    "            if 'actions' in demo_0:\n",
    "                try:\n",
    "                    actions = load_actions_robust(demo_0['actions'])\n",
    "                    print(f\"\\nðŸŽ® Azioni caricate: {actions.shape}\")\n",
    "                    print(f\"  Range: [{actions.min():.3f}, {actions.max():.3f}]\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ Errore azioni: {e}\")\n",
    "\n",
    "            # 3. VISUALIZZAZIONE (Senza Matplotlib)\n",
    "            if imgs is not None and len(imgs) > 0:\n",
    "                print(\"\\nðŸŽ¬ Visualizzazione frame esempio (PIL/IPython):\")\n",
    "                \n",
    "                num_frames = min(4, len(imgs))\n",
    "                indices = np.linspace(0, len(imgs) - 1, num_frames, dtype=int)\n",
    "                \n",
    "                for idx in indices:\n",
    "                    img_array = imgs[idx]\n",
    "                    \n",
    "                    # Se l'immagine Ã¨ float [0,1], converti a uint8\n",
    "                    if img_array.dtype != np.uint8:\n",
    "                         img_array = (np.clip(img_array, 0, 1) * 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Crea immagine PIL\n",
    "                    pil_img = Image.fromarray(img_array)\n",
    "                    \n",
    "                    # (Opzionale) Resize per non occupare troppo spazio\n",
    "                    # pil_img = pil_img.resize((128, 128))\n",
    "                    \n",
    "                    print(f\"--- Frame {idx} ---\")\n",
    "                    display(pil_img)\n",
    "            else:\n",
    "                print(\"\\nâš ï¸ Nessuna immagine valida da visualizzare\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore critico durante l'apertura del file: {e}\")\n",
    "    \n",
    "    return hdf5_files\n",
    "\n",
    "# Esegui\n",
    "hdf5_files = explore_libero_dataset(Path('dataset/libero_spatia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTIONS (Kept largely the same) ---\n",
    "\n",
    "def _load_images_robust(obs_group):\n",
    "    \"\"\"\n",
    "    Load images from HDF5 dataset using robust method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try multiple possible image keys\n",
    "    possible_image_keys = [\n",
    "        'agentview_rgb', 'rgb', 'agentview_image', \n",
    "        'camera0_rgb', 'camera_0_rgb', 'frontview_rgb',\n",
    "        'image', 'obs_rgb', 'robot0_eye_in_hand_image'\n",
    "    ]\n",
    "    \n",
    "    for img_key in possible_image_keys:\n",
    "        if img_key in obs_group:\n",
    "            try:\n",
    "                return np.array(obs_group[img_key])\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # If no image key is found, raise error\n",
    "    available_keys = list(obs_group.keys())\n",
    "    raise ValueError(f\"No valid image key found. Available keys: {available_keys}\")\n",
    "\n",
    "def _load_actions_robust(actions_group):\n",
    "    \"\"\"\n",
    "    Load actions from HDF5 dataset.\n",
    "    \"\"\"\n",
    "    return np.array(actions_group).astype(np.float32)\n",
    "\n",
    "# --- IMPROVED DEMO LOADER ---\n",
    "\n",
    "def load_demonstrations_from_hdf5(hdf5_path, max_demos=None):\n",
    "    \"\"\"\n",
    "    Loads demonstrations from a single HDF5 file.\n",
    "    Returns: list of dicts {'obs': images_array, 'actions': actions_array}\n",
    "    \"\"\"\n",
    "    \n",
    "    demonstrations = []\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(hdf5_path, 'r') as f:\n",
    "            print(f\"ðŸ”“ Opening {Path(hdf5_path).name}\")\n",
    "            \n",
    "            # Check 'data' key exists\n",
    "            if 'data' not in f:\n",
    "                print(\"âš ï¸ 'data' key not found\")\n",
    "                return demonstrations\n",
    "            \n",
    "            data_group = f['data']\n",
    "            demo_keys = [k for k in data_group.keys() if 'demo' in k.lower()]\n",
    "            \n",
    "            if max_demos:\n",
    "                demo_keys = demo_keys[:max_demos]\n",
    "            \n",
    "            # 1. Image Loading\n",
    "            for demo_key in demo_keys:\n",
    "                try:\n",
    "                    \n",
    "                    # Image key search strategy\n",
    "                    demo_group = data_group[demo_key]\n",
    "                    obs_group = demo_group['obs']\n",
    "                    \n",
    "                    imgs = None\n",
    "                    for img_key in ['agentview_rgb', 'rgb', 'frontview_rgb']:\n",
    "                        if img_key in obs_group:\n",
    "                            print(f\"\\nðŸ–¼ï¸ Using image key: '{img_key}'\")\n",
    "                            try:\n",
    "                                imgs = _load_images_robust(obs_group)\n",
    "                                print(f\"  âœ… Images loaded: {imgs.shape}\")\n",
    "                                break\n",
    "                            except Exception as e:\n",
    "                                print(f\"  âŒ Image error: {e}\")\n",
    "                    \n",
    "                    # 2. Action Loading\n",
    "                    actions = None\n",
    "                    try:\n",
    "                        actions = _load_actions_robust(demo_group['actions'])\n",
    "                        print(f\"\\nðŸŽ® Actions loaded: {actions.shape}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Error loading actions: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if imgs is not None and actions is not None:\n",
    "                        demonstrations.append({\n",
    "                            'obs': imgs,\n",
    "                            'actions': actions\n",
    "                        })\n",
    "                        print(f\"âœ… Demo {demo_key} loaded successfully\")\n",
    "                    else:\n",
    "                        print(f\"âŒ Demo {demo_key} incomplete\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error in demo {demo_key}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"ðŸŽ¯ Loaded {len(demonstrations)} demonstrations from {Path(hdf5_path).name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ’¥ Critical error opening file: {e}\")\n",
    "    \n",
    "    return demonstrations\n",
    "\n",
    "# Load and test with one HDF5 file\n",
    "\n",
    "if True:  # Set to False to skip this test\n",
    "    test_file = \"dataset/libero_goal/put_the_bowl_on_the_plate_demo.hdf5\"\n",
    "    \n",
    "    if Path(test_file).exists():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ðŸ§ª TESTING DEMO LOADING\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        demos = load_demonstrations_from_hdf5(test_file, max_demos=2)\n",
    "        \n",
    "        if demos:\n",
    "            print(f\"\\nðŸ“Š SUMMARY:\")\n",
    "            print(f\"   Loaded {len(demos)} demonstrations\")\n",
    "            for i, demo in enumerate(demos):\n",
    "                print(f\"   Demo {i}: {demo['obs'].shape} obs, {demo['actions'].shape} actions\")\n",
    "        else:\n",
    "            print(\"âŒ No demonstrations loaded!\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Test file not found: {test_file}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Load visual observations and actions from HDF5 files.\n",
    "    \n",
    "    This dataset handles training/validation split in a more sophisticated way:\n",
    "    instead of splitting entire files, it splits the demos WITHIN each file\n",
    "    (e.g., 80% train, 20% val per file).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_name: str = 'resnet18',\n",
    "        output_dim: int = 256,\n",
    "        freeze: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained vision model\n",
    "        if 'resnet' in pretrained_name:\n",
    "            self.backbone = getattr(torchvision.models, pretrained_name)(pretrained=True)\n",
    "            # Remove final classification layer\n",
    "            backbone_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pretrained_name: {pretrained_name}\")\n",
    "        \n",
    "        # Freeze if requested\n",
    "        if freeze:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Adapter layer\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Normalization parameters for ImageNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "\n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = (x - self.mean) / self.std\n",
    "        features = self.backbone(x).flatten(start_dim=1) # [B, 512]\n",
    "        output = self.adapter(features) # try passing features directly instead of the adapter\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class PretrainedVisualEncoder(nn.Module):\n",
    "    \"\"\"Visual encoder based on ResNet18 with adaptive head.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int = 256, freeze_backbone: bool = True, dropout: float = 0.1, double_visual_features: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "        self.double_visual_features = double_visual_features\n",
    "\n",
    "        # Load pretrained ResNet18 from torchvision\n",
    "        resnet18 = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "        \n",
    "        # Remove final classification layer\n",
    "        self.backbone = nn.Sequential(*list(resnet18.children())[:-1])  # Remove last fc layer\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"ðŸ”’ Visual backbone frozen\")\n",
    "        \n",
    "        # Adaptive head\n",
    "        resnet_output_dim = 512  # ResNet18 fc input features\n",
    "        \n",
    "        if double_visual_features:\n",
    "            self.adaptive_head = nn.Sequential(\n",
    "                nn.Linear(resnet_output_dim, hidden_dim * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.adaptive_head = nn.Sequential(\n",
    "                nn.Linear(resnet_output_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "        # ImageNet normalization\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize adaptive head weights.\"\"\"\n",
    "        for m in self.adaptive_head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [B, C, H, W] (assumes 0-1 normalized)\n",
    "            \n",
    "        Returns:\n",
    "            Encoded visual features [B, hidden_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # ImageNet normalization\n",
    "        x = (x - self.mean) / self.std\n",
    "\n",
    "        # Pass through backbone\n",
    "        with torch.no_grad() if self.freeze_backbone else torch.enable_grad():\n",
    "            features = self.backbone(x)  # [B, 512, 1, 1] for ResNet18\n",
    "        \n",
    "        features = features.flatten(start_dim=1)  # [B, 512]\n",
    "        \n",
    "        # Apply adaptive head\n",
    "        visual_features = self.adaptive_head(features)  # [B, hidden_dim]\n",
    "        \n",
    "        return visual_features\n",
    "\n",
    "# =========================\n",
    "\n",
    "class PromptEncoder(nn.Module):\n",
    "    \"\"\"Encodes natural-language task prompts via CLIP ViT-L/14 text tower.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int = 256, pretrained_name: str = \"bert-base-uncased\", max_length: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_length = max_length\n",
    "        self.pretrained_name = pretrained_name\n",
    "\n",
    "        # BERT tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_name)\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_name)\n",
    "        \n",
    "        # Freeze BERT weights (optional)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"ðŸ”’ BERT model '{pretrained_name}' loaded and frozen\")\n",
    "\n",
    "        # Map BERT output to desired hidden dimension\n",
    "        bert_hidden_dim = self.bert.config.hidden_size  # 768 for base BERT\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize projection weights.\"\"\"\n",
    "        for m in self.projection:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, prompts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode a batch of text prompts.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of text strings\n",
    "            \n",
    "        Returns:\n",
    "            Text embeddings [B, hidden_dim]\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # Tokenize prompts\n",
    "        encoded = self.tokenizer(\n",
    "            prompts,\n",
    "            max_length=self.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Pass through BERT\n",
    "        with torch.no_grad():  # BERT is frozen\n",
    "            outputs = self.bert(**encoded)\n",
    "            # Use [CLS] token representation\n",
    "            text_features = outputs.last_hidden_state[:, 0]  # [B, 768]\n",
    "        \n",
    "        # Project to desired dimension\n",
    "        text_features = self.projection(text_features)  # [B, hidden_dim]\n",
    "        \n",
    "        return text_features\n",
    "\n",
    "# =========================\n",
    "\n",
    "class TinyRecursiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "    TinyRecursiveModel with integrated text prompts, visual observations, and action prediction.\n",
    "    \n",
    "    Key modifications:\n",
    "    - Added prompt encoding via BERT\n",
    "    - Supports both visual-only and vision+language modes\n",
    "    - Action prediction head with proper normalization\n",
    "    - Recursive transformer with cross-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.num_recursions = config.num_recursions\n",
    "        self.use_text_prompts = True  # Enable text prompts by default\n",
    "        \n",
    "        print(f\"ðŸ—ï¸ Building TinyRecursiveModel:\")\n",
    "        print(f\"   Hidden dim: {self.hidden_dim}\")\n",
    "        print(f\"   Recursions: {self.num_recursions}\")\n",
    "        print(f\"   Text prompts: {self.use_text_prompts}\")\n",
    "\n",
    "        # Vision encoder\n",
    "        self.visual_encoder = PretrainedVisualEncoder(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            freeze_backbone=config.freeze_backbone,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        \n",
    "        # Text encoder (conditional)\n",
    "        if self.use_text_prompts:\n",
    "            self.prompt_encoder = PromptEncoder(\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                pretrained_name=config.text_encoder,\n",
    "                max_length=config.max_text_length,\n",
    "                dropout=config.dropout\n",
    "            )\n",
    "\n",
    "        # Recursive Transformer\n",
    "        self.recursive_transformer = RecursiveTransformerWithAttention(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            num_recursions=self.num_recursions,\n",
    "            dropout=config.attention_dropout\n",
    "        )\n",
    "\n",
    "        # Action prediction head\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(self.hidden_dim, 7),  # 7D action space for robotic manipulation\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"ðŸ“Š Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize action head weights.\"\"\"\n",
    "        for m in self.action_head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, images: torch.Tensor, prompts: Optional[List[str]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            images: Visual observations [B, C, H, W]\n",
    "            prompts: Optional text prompts (list of strings)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted actions [B, action_dim]\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Encode visual observations\n",
    "        visual_features = self.visual_encoder(images)  # [B, hidden_dim]\n",
    "        \n",
    "        # Encode text prompts (if provided)\n",
    "        if self.use_text_prompts and prompts is not None:\n",
    "            text_features = self.prompt_encoder(prompts)  # [B, hidden_dim]\n",
    "            \n",
    "            # Combine visual and text features\n",
    "            # Simple concatenation + projection approach\n",
    "            combined_features = visual_features + text_features  # Element-wise addition\n",
    "        else:\n",
    "            combined_features = visual_features\n",
    "\n",
    "        # Apply recursive transformer\n",
    "        refined_features = self.recursive_transformer(combined_features)  # [B, hidden_dim]\n",
    "\n",
    "        # Predict actions\n",
    "        actions = self.action_head(refined_features)  # [B, 7]\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "\n",
    "class LiberoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for LIBERO demonstrations\n",
    "    \n",
    "    Supports both training/validation split with stratified sampling (some demonstrations\n",
    "    from each file go to training, others to validation), as well as split by ratio  \n",
    "    within each file (e.g., 80% train, 20% val per file).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        data_path: str, \n",
    "        split: str = 'train',\n",
    "        demo_split_ratio: float = 0.8,\n",
    "        max_demos_per_task: Optional[int] = None,\n",
    "        action_stats: Optional[Dict[str, np.ndarray]] = None,\n",
    "        normalize_actions: bool = True,\n",
    "        augment_images: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: path to folder with HDF5 files\n",
    "            split: 'train' or 'val' (ignored if demo_split_ratio is used)\n",
    "            max_demos_per_task: maximum limit of demos per task (for debugging)\n",
    "            demo_split_ratio: percentage of demos for training (default 0.8 = 80%)\n",
    "            action_stats: pre-computed statistics (mean, std) for action normalization\n",
    "            normalize_actions: whether to normalize actions using statistics\n",
    "            augment_images: apply augmentations to images during training\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_path = Path(data_path)\n",
    "        self.split = split\n",
    "        self.demo_split_ratio = demo_split_ratio\n",
    "        self.max_demos_per_task = max_demos_per_task\n",
    "        self.normalize_actions = normalize_actions\n",
    "        self.augment_images = augment_images and split == 'train'\n",
    "        \n",
    "        self.action_stats = action_stats.copy() if action_stats else {}\n",
    "        \n",
    "        # Load HDF5 files\n",
    "        hdf5_files = list(self.data_path.glob(\"*.hdf5\"))\n",
    "        if not hdf5_files:\n",
    "            raise FileNotFoundError(f\"No HDF5 files found in {data_path}\")\n",
    "        \n",
    "        print(f\"ðŸ“ Found {len(hdf5_files)} HDF5 files\")\n",
    "        \n",
    "        self.data = []\n",
    "        all_actions = [] \n",
    "        \n",
    "        for hdf5_file in hdf5_files:\n",
    "            try:\n",
    "                with h5py.File(hdf5_file, 'r') as f:\n",
    "                    prompt = self._prompt_from_filename(hdf5_file)\n",
    "                    \n",
    "                    demo_keys = [k for k in f['data'].keys() if k.startswith('demo_')]\n",
    "                    demo_keys.sort()\n",
    "                    \n",
    "                    if self.max_demos_per_task:\n",
    "                        demo_keys = demo_keys[:self.max_demos_per_task]\n",
    "                    \n",
    "                    # Split demos for train/val\n",
    "                    num_train_demos = int(len(demo_keys) * self.demo_split_ratio)\n",
    "                    \n",
    "                    if self.split == 'train':\n",
    "                        selected_demos = demo_keys[:num_train_demos]\n",
    "                    elif self.split == 'val':\n",
    "                        selected_demos = demo_keys[num_train_demos:]\n",
    "                    else:\n",
    "                        selected_demos = demo_keys\n",
    "                    \n",
    "                    if not selected_demos:\n",
    "                        print(f\"âš ï¸ No demos for {self.split} split in {hdf5_file.name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    task_actions = []\n",
    "                    for demo_name in selected_demos:\n",
    "                        try:\n",
    "                            demo_path = f'data/{demo_name}'\n",
    "                            obs_path = f'{demo_path}/obs'\n",
    "                            \n",
    "                            if 'agentview_rgb' not in f[obs_path]:\n",
    "                                print(f\"âš ï¸ No 'agentview_rgb' in {demo_name}\")\n",
    "                                continue\n",
    "                                \n",
    "                            images = f[f'{obs_path}/agentview_rgb'][:]\n",
    "                            actions = f[f'{demo_path}/actions'][:]\n",
    "                            \n",
    "                            if len(images) != len(actions):\n",
    "                                print(f\"âš ï¸ Length mismatch: images={len(images)}, actions={len(actions)} for {demo_name}\")\n",
    "                                continue\n",
    "                            \n",
    "                            for i in range(len(images)):\n",
    "                                self.data.append({\n",
    "                                    'image': images[i],\n",
    "                                    'action': actions[i].astype(np.float32),\n",
    "                                    'prompt': prompt,\n",
    "                                    'demo_id': demo_name,\n",
    "                                    'timestep': i\n",
    "                                })\n",
    "                                \n",
    "                            task_actions.append(actions)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"âš ï¸ Error loading {demo_name}: {e}\")\n",
    "                    \n",
    "                    all_actions.extend(task_actions)\n",
    "                    print(f\"âœ… Loaded {len(selected_demos)} demos from {hdf5_file.name} for {self.split}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Critical error opening file: {e}\")\n",
    "                continue\n",
    "        \n",
    "        split_name = f\"{self.split} ({len(self.data)} samples)\"\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(f\"No valid demonstrations loaded for {split_name}! Check your data files.\")\n",
    "        \n",
    "        # Calculate action statistics for normalization (only for training set or if not provided)\n",
    "        if self.normalize_actions and len(all_actions) > 0 and action_stats is None:\n",
    "            all_actions_concat = np.concatenate(all_actions, axis=0)\n",
    "        \n",
    "            mean = all_actions_concat.mean(axis=0).astype(np.float32)\n",
    "            std  = all_actions_concat.std(axis=0).astype(np.float32)\n",
    "        \n",
    "            # âš ï¸ Safety floor: avoid too small std that explodes normalization\n",
    "            std_clipped = np.clip(std, 0.1, None)\n",
    "        \n",
    "            # Detailed logging\n",
    "            print(f\"ðŸ“Š Action statistics computed from {split_name} set:\")\n",
    "            print(f\"   Mean:        {np.round(mean, 3)}\")\n",
    "            print(f\"   Std (raw):   {np.round(std, 3)}\")\n",
    "            print(f\"   Std (clipped to >=0.1): {np.round(std_clipped, 3)}\")\n",
    "        \n",
    "            self.action_stats['mean'] = mean\n",
    "            self.action_stats['std']  = std_clipped\n",
    "        \n",
    "        elif action_stats is not None:\n",
    "            print(f\"ðŸ“Š Using provided action statistics\")\n",
    "            self.action_stats = {\n",
    "                'mean': action_stats['mean'].astype(np.float32),\n",
    "                'std':  np.clip(action_stats['std'], 0.1, None).astype(np.float32)\n",
    "            }\n",
    "\n",
    "        # Build transition index for O(1) access\n",
    "        self.samples = self._build_sample_index()\n",
    "        print(f\"ðŸ“¦ Generated {len(self.samples)} transitions for {split_name}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _prompt_from_filename(hdf5_file: Path) -> str:\n",
    "        \"\"\"Converts the HDF5 filename to a natural language prompt.\"\"\"\n",
    "        name = hdf5_file.stem\n",
    "        if name.endswith('_demo'):\n",
    "            name = name[:-5]\n",
    "        name = name.replace('_', ' ').replace('-', ' ')\n",
    "        return name.title()\n",
    "        \n",
    "    def _build_sample_index(self) -> List[int]:\n",
    "        \"\"\"Build a list of valid sample indices.\"\"\"\n",
    "        return list(range(len(self.data)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = self.samples[idx]\n",
    "        item = self.data[sample_idx]\n",
    "        \n",
    "        image = item['image'].copy()\n",
    "        action = item['action'].copy()\n",
    "        prompt = item['prompt']\n",
    "        \n",
    "        # Convert image from HWC to CHW and normalize to [0, 1]\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:\n",
    "            image = image.transpose(2, 0, 1)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment_images:\n",
    "            image = self._apply_augmentation(image)\n",
    "        \n",
    "        # Normalize actions if statistics are available\n",
    "        if self.normalize_actions and 'mean' in self.action_stats:\n",
    "            action = (action - self.action_stats['mean']) / self.action_stats['std']\n",
    "        \n",
    "        return {\n",
    "            'image': torch.from_numpy(image),\n",
    "            'action': torch.from_numpy(action),\n",
    "            'prompt': prompt,\n",
    "            'demo_id': item['demo_id'],\n",
    "            'timestep': item['timestep']\n",
    "        }\n",
    "    \n",
    "    def _apply_augmentation(self, image):\n",
    "        \"\"\"Apply random augmentations to image tensor (CHW format).\"\"\"\n",
    "        # Simple random horizontal flip\n",
    "        if np.random.random() > 0.5:\n",
    "            image = image[:, :, ::-1].copy()  # Flip along width dimension\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tiny_recursive_model(\n",
    "    base_config: TrainingConfig,\n",
    "    train_dataset: LiberoDataset,\n",
    "    val_dataset: LiberoDataset,\n",
    ") -> TinyRecursiveModel:\n",
    "    \"\"\"Perform final training starting from the chosen configuration.\"\"\"\n",
    "\n",
    "    print(f\"\\nðŸš‚ FINAL TRAINING - TinyRecursiveModel\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Hidden Dim: {base_config.hidden_dim}\")\n",
    "    print(f\"   Recursions: {base_config.num_recursions}\")\n",
    "    print(f\"   Learning Rate: {base_config.lr}\")\n",
    "    print(f\"   Batch Size: {base_config.batch_size}\")\n",
    "    print(f\"   Epochs: {base_config.epochs}\")\n",
    "    print(f\"   Text Encoder: {base_config.text_encoder}\")\n",
    "    print(f\"   Model: {base_config.model_type}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"   Train Action Stats: mean={np.round(train_dataset.action_stats['mean'], 3)}\")\n",
    "    print(f\"                       std={np.round(train_dataset.action_stats['std'], 3)}\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"   Device: {device}\")\n",
    "\n",
    "    # Create model\n",
    "    model = TinyRecursiveModel(base_config).to(device)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=base_config.batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=base_config.num_workers,\n",
    "        pin_memory=base_config.pin_memory\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=base_config.batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=base_config.num_workers,\n",
    "        pin_memory=base_config.pin_memory\n",
    "    )\n",
    "\n",
    "    # Training setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_config.lr, weight_decay=base_config.weight_decay)\n",
    "    \n",
    "    # Scheduler\n",
    "    if base_config.scheduler == 'warmup_cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=base_config.sched_T0 if base_config.sched_T0 else base_config.epochs,\n",
    "            T_mult=base_config.sched_T_mult,\n",
    "            eta_min=base_config.lr_min\n",
    "        )\n",
    "    elif base_config.scheduler == 'step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Starting Training Loop...\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(base_config.epochs):\n",
    "        \n",
    "        # === TRAINING PHASE ===\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            actions = batch['action'].to(device)\n",
    "            prompts = batch['prompt']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predicted_actions = model(images, prompts)\n",
    "            loss = criterion(predicted_actions, actions)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if base_config.grad_clip:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), base_config.grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / num_train_batches if num_train_batches > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                actions = batch['action'].to(device)\n",
    "                prompts = batch['prompt']\n",
    "                \n",
    "                predicted_actions = model(images, prompts)\n",
    "                loss = criterion(predicted_actions, actions)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = epoch_val_loss / num_val_batches if num_val_batches > 0 else float('inf')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch+1:2d}/{base_config.epochs}: \"\n",
    "              f\"Train Loss = {avg_train_loss:.6f}, \"\n",
    "              f\"Val Loss = {avg_val_loss:.6f}, \"\n",
    "              f\"LR = {current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': base_config,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'epoch': epoch,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'action_stats': train_dataset.action_stats\n",
    "            }, base_config.save_path)\n",
    "            print(f\"         âœ¨ New best model saved! Val Loss: {best_val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping check\n",
    "        if base_config.early_stop_patience and patience_counter >= base_config.early_stop_patience:\n",
    "            print(f\"ðŸ›‘ Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nâœ… Training completed!\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   Model saved to: {base_config.save_path}\")\n",
    "    \n",
    "    # Load best model for return\n",
    "    best_checkpoint = torch.load(base_config.save_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Quick test to create datasets and verify they work\n",
    "    print(\"ðŸ§ª TESTING DATASET CREATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Parameters\n",
    "    data_path = 'dataset/libero_goal'  # Use libero_goal for testing\n",
    "    \n",
    "    if not Path(data_path).exists():\n",
    "        print(f\"âŒ Data path not found: {data_path}\")\n",
    "        print(\"   Please ensure you have the LIBERO dataset downloaded\")\n",
    "    else:\n",
    "        print(f\"âœ… Data path found: {data_path}\")\n",
    "        \n",
    "        # Find HDF5 files\n",
    "        hdf5_files = list(Path(data_path).glob(\"*.hdf5\"))\n",
    "        print(f\"ðŸ“ Found {len(hdf5_files)} HDF5 files\")\n",
    "        \n",
    "        if len(hdf5_files) == 0:\n",
    "            print(\"âŒ No HDF5 files found!\")\n",
    "        else:\n",
    "            # Demo-level split: use ALL files for both datasets\n",
    "            # but split the demos WITHIN each file (80% train, 20% val per task)\n",
    "            demo_split_ratio = 0.8\n",
    "            print(f\"\\nðŸ“Š Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per task\")\n",
    "            print(f\"   All {len(hdf5_files)} tasks present in both train and val\")\n",
    "            \n",
    "            # Create datasets with demo-level split\n",
    "            print(\"\\nCreating TRAIN dataset...\")\n",
    "            train_dataset = LiberoDataset(\n",
    "                data_path,  # Use ALL files\n",
    "                split='train',\n",
    "                demo_split_ratio=demo_split_ratio,\n",
    "                max_demos_per_task=50,  # Limit for speed\n",
    "                normalize_actions=True,\n",
    "                augment_images=False\n",
    "            )\n",
    "            \n",
    "            # Use the same statistics from training set for validation\n",
    "            train_action_stats = train_dataset.action_stats\n",
    "            \n",
    "            print(\"\\nCreating VAL dataset...\")\n",
    "            val_dataset = LiberoDataset(\n",
    "                data_path,  # Use ALL files\n",
    "                split='val',\n",
    "                demo_split_ratio=demo_split_ratio,\n",
    "                action_stats=train_action_stats,  # Use training stats\n",
    "                normalize_actions=True,\n",
    "                augment_images=False\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nâœ… Dataset creation completed!\")\n",
    "            print(f\"   Training samples: {len(train_dataset)}\")\n",
    "            print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "            print(f\"   Action dimension: {len(train_action_stats['mean'])}\")\n",
    "            \n",
    "            # Test a few samples\n",
    "            print(f\"\\nðŸ§ª Testing sample loading...\")\n",
    "            \n",
    "            # Test training dataset\n",
    "            for i in range(min(3, len(train_dataset))):\n",
    "                sample = train_dataset[i]\n",
    "                print(f\"   Train sample {i}: image={sample['image'].shape}, action={sample['action'].shape}\")\n",
    "                print(f\"                     prompt='{sample['prompt'][:50]}...'\")\n",
    "            \n",
    "            # Test validation dataset\n",
    "            for i in range(min(2, len(val_dataset))):\n",
    "                sample = val_dataset[i]\n",
    "                print(f\"   Val sample {i}: image={sample['image'].shape}, action={sample['action'].shape}\")\n",
    "                \n",
    "            print(f\"\\nâœ… Sample loading test passed!\")\n",
    "            \n",
    "            # Quick training test with minimal configuration\n",
    "            print(f\"\\nðŸš‚ QUICK TRAINING TEST\")\n",
    "            print(\"=\"*40)\n",
    "            \n",
    "            config = TrainingConfig(\n",
    "                lr=1e-3,\n",
    "                hidden_dim=128,  # Reduced for speed\n",
    "                num_recursions=2,  # Reduced for speed\n",
    "                epochs=3,  # Very few epochs for testing\n",
    "                batch_size=8,   # Small batch for testing\n",
    "                text_encoder='bert-base-uncased',\n",
    "                save_path='test_model.pt',\n",
    "                early_stop_patience=None  # Disable early stopping for test\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                model = train_tiny_recursive_model(config, train_dataset, val_dataset)\n",
    "                print(f\"âœ… Quick training test completed successfully!\")\n",
    "                \n",
    "                # Test inference\n",
    "                print(f\"\\nðŸ”® Testing inference...\")\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    sample = train_dataset[0]\n",
    "                    test_image = sample['image'].unsqueeze(0)  # Add batch dimension\n",
    "                    test_prompt = [sample['prompt']]\n",
    "                    \n",
    "                    device = next(model.parameters()).device\n",
    "                    test_image = test_image.to(device)\n",
    "                    \n",
    "                    predicted_action = model(test_image, test_prompt)\n",
    "                    print(f\"   Input image shape: {test_image.shape}\")\n",
    "                    print(f\"   Input prompt: '{test_prompt[0]}'\")\n",
    "                    print(f\"   Predicted action: {predicted_action[0].cpu().numpy()}\")\n",
    "                    print(f\"   Ground truth action: {sample['action'].numpy()}\")\n",
    "                    \n",
    "                print(f\"âœ… Inference test completed!\")\n",
    "                \n",
    "                # Clean up test model\n",
    "                if Path(config.save_path).exists():\n",
    "                    Path(config.save_path).unlink()\n",
    "                    print(f\"ðŸ—‘ï¸ Cleaned up test model file\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Training test failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "    print(f\"\\nðŸŽ¯ Dataset testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ================================================================================\n",
      "    ðŸ¤– TinyRecursiveModels per Controllo Robotico\n",
      "    ================================================================================\n",
      "    \n",
      "    Obiettivi:\n",
      "    1. Adattare architettura TRM per robotica\n",
      "    2. Training con Behavior Cloning su LIBERO\n",
      "    3. Valutazione in simulazione con metriche quantitative e qualitative\n",
      "    \n",
      "    \n",
      "âœ“ Using device: cuda\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Caricamento Dataset\n",
      "================================================================================\n",
      "âœ“ Trovati 10 file HDF5 (task)\n",
      "\n",
      "ðŸ“Š Demo-level split: 80% train / 20% val per ogni task\n",
      "   Tutti i 10 task presenti in entrambi train e val\n",
      "\n",
      "Creating TRAIN dataset...\n",
      "Loading 10 HDF5 files for TRAIN (demo split: 80%)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 400 demonstrations for TRAIN\n",
      "ðŸ“Š Action statistics computed from TRAIN set:\n",
      "   Mean:        [ 0.15   0.134 -0.155 -0.005 -0.011 -0.02   0.093]\n",
      "   Std (raw):   [0.414 0.347 0.508 0.038 0.072 0.058 0.996]\n",
      "   Std (clipped to >=0.1): [0.414 0.347 0.508 0.1   0.1   0.1   0.996]\n",
      "ðŸ“¦ Generated 49735 transitions for TRAIN\n",
      "\n",
      "Creating VAL dataset...\n",
      "Loading 10 HDF5 files for VAL (demo split: 80%)...\n",
      "âœ… Loaded 100 demonstrations for VAL\n",
      "ðŸ“Š Using provided action statistics\n",
      "ðŸ“¦ Generated 12515 transitions for VAL\n",
      "\n",
      "âœ“ Dataset creati con demo-level split\n",
      "  Train samples: 49735\n",
      "  Val samples: 12515\n",
      "\n",
      "================================================================================\n",
      "âœ… Pipeline completata!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "main_pipeline(quick_search=False, train_final=False, evaluate=False, data_path='dataset/libero_spatial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from unittest.mock import MagicMock\n",
    "from dataclasses import fields, dataclass\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "class TextExplainer:\n",
    "    \"\"\"\n",
    "    Class to calculate gradient-based explainability for text prompts.\n",
    "    \n",
    "    Calculates which prompt tokens are most important for action prediction,\n",
    "    using backpropagation through the text encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, prompt_encoder: PromptEncoder, device: torch.device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: TRMPolicy model\n",
    "            prompt_encoder: PromptEncoder for tokenization and encoding\n",
    "            device: device for computation\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "        self.device = device\n",
    "        \n",
    "    def compute_token_saliency(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        target_action: Optional[torch.Tensor] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calculate the saliency of each prompt token with respect to action prediction.\n",
    "        \n",
    "        Args:\n",
    "            obs: Visual observation [1, C, H, W]\n",
    "            prompt: Text prompt string\n",
    "            target_action: Optional target action for supervised saliency [1, action_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing token strings, saliency scores, and metadata\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        tokenizer = self.prompt_encoder.tokenizer\n",
    "        encoded = tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.prompt_encoder.max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        input_ids = encoded['input_ids']  # [1, seq_len]\n",
    "        attention_mask = encoded['attention_mask']  # [1, seq_len]\n",
    "        \n",
    "        # Extract token strings for interpretation\n",
    "        token_strings = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        \n",
    "        # Get text embeddings with gradient computation enabled\n",
    "        with torch.no_grad():\n",
    "            # Get BERT embeddings\n",
    "            bert_outputs = self.prompt_encoder.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            bert_embeddings = bert_outputs.last_hidden_state  # [1, seq_len, 768]\n",
    "            \n",
    "        # Extract [CLS] token and project\n",
    "        cls_embedding = bert_embeddings[:, 0]  # [1, 768]\n",
    "        cls_embedding.requires_grad_(True)\n",
    "        \n",
    "        # Project to model hidden dim\n",
    "        text_features = self.prompt_encoder.projection(cls_embedding)  # [1, hidden_dim]\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            visual_features = self.model.visual_encoder(obs)  # [1, hidden_dim]\n",
    "        \n",
    "        # Combine features and predict action\n",
    "        combined_features = visual_features + text_features\n",
    "        refined_features = self.model.recursive_transformer(combined_features)\n",
    "        predicted_actions = self.model.action_head(refined_features)  # [1, action_dim]\n",
    "        \n",
    "        # Compute loss for gradient computation\n",
    "        if target_action is not None:\n",
    "            # Supervised: use provided target\n",
    "            loss = nn.functional.mse_loss(predicted_actions, target_action)\n",
    "        else:\n",
    "            # Unsupervised: use norm of predicted actions as proxy\n",
    "            loss = torch.norm(predicted_actions, dim=1).mean()\n",
    "        \n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get gradients w.r.t. CLS embedding\n",
    "        cls_gradients = cls_embedding.grad  # [1, 768]\n",
    "        \n",
    "        # Compute importance scores\n",
    "        # Use gradient x input as attribution measure\n",
    "        attribution = torch.abs(cls_gradients * cls_embedding.detach())  # [1, 768]\n",
    "        token_importance = attribution.sum(dim=-1).squeeze().cpu().numpy()  # [seq_len,] proxy\n",
    "        \n",
    "        # Since we only have CLS gradients, we'll distribute importance across all tokens\n",
    "        # This is a simplified approach - more sophisticated methods exist\n",
    "        num_tokens = attention_mask.sum().item()\n",
    "        if num_tokens > 0:\n",
    "            # Distribute CLS importance across all active tokens\n",
    "            distributed_importance = np.ones(len(token_strings)) * (token_importance / num_tokens)\n",
    "            # Mask out padding tokens\n",
    "            mask = attention_mask[0].cpu().numpy()\n",
    "            distributed_importance = distributed_importance * mask\n",
    "        else:\n",
    "            distributed_importance = np.zeros(len(token_strings))\n",
    "        \n",
    "        # Normalize scores\n",
    "        if distributed_importance.max() > 0:\n",
    "            normalized_scores = distributed_importance / distributed_importance.max()\n",
    "        else:\n",
    "            normalized_scores = distributed_importance\n",
    "            \n",
    "        return {\n",
    "            'token_strings': token_strings,\n",
    "            'raw_scores': distributed_importance,\n",
    "            'normalized_scores': normalized_scores,\n",
    "            'attention_mask': attention_mask[0].cpu().numpy(),\n",
    "            'predicted_actions': predicted_actions.detach().cpu().numpy(),\n",
    "            'loss': loss.item()\n",
    "        }\n",
    "    \n",
    "    def get_top_k_tokens(\n",
    "        self,\n",
    "        saliency_result: Dict[str, Any],\n",
    "        k: int = 10,\n",
    "        filter_special: bool = True\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract top-k most important tokens from saliency result.\n",
    "        \n",
    "        Args:\n",
    "            saliency_result: Output from compute_token_saliency\n",
    "            k: Number of top tokens to return\n",
    "            filter_special: Whether to filter out special tokens ([CLS], [SEP], [PAD])\n",
    "        \n",
    "        Returns:\n",
    "            List of (token, score) tuples sorted by importance\n",
    "        \"\"\"\n",
    "        tokens = saliency_result['token_strings']\n",
    "        scores = saliency_result['normalized_scores']\n",
    "        mask = saliency_result['attention_mask']\n",
    "        \n",
    "        # Filter out padding tokens and special tokens if requested\n",
    "        filtered_tokens_scores = []\n",
    "        special_tokens = ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>']\n",
    "        \n",
    "        for i, (token, score) in enumerate(zip(tokens, scores)):\n",
    "            # Skip padding tokens\n",
    "            if mask[i] == 0:\n",
    "                continue\n",
    "                \n",
    "            # Skip special tokens if requested\n",
    "            if filter_special and token in special_tokens:\n",
    "                continue\n",
    "            \n",
    "            filtered_tokens_scores.append((token, float(score)))\n",
    "        \n",
    "        # Sort by score (descending) and return top-k\n",
    "        filtered_tokens_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return filtered_tokens_scores[:k]\n",
    "\n",
    "\n",
    "def visualize_explainability_results(\n",
    "    json_path: str,\n",
    "    top_n_frames: int = 5,\n",
    "    top_n_tokens: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize explainability results from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to explainability JSON file\n",
    "        top_n_frames: Number of frames to display\n",
    "        top_n_tokens: Number of tokens to display per frame\n",
    "    \"\"\"\n",
    "    \n",
    "    if not Path(json_path).exists():\n",
    "        print(f\"âŒ File not found: {json_path}\")\n",
    "        return\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metadata = data['metadata']\n",
    "    frames = data['frames']\n",
    "    \n",
    "    print(f\"ðŸ“Š Explainability Results for Task {metadata['task_id']}\")\n",
    "    print(f\"   Benchmark: {metadata['benchmark']}\")\n",
    "    print(f\"   Task prompt: '{metadata['task_prompt']}'\")\n",
    "    print(f\"   Success rate: {metadata['success_rate']:.2%}\")\n",
    "    print(f\"   Frames analyzed: {metadata['num_frames_analyzed']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show top N frames\n",
    "    for i, frame in enumerate(frames[:top_n_frames]):\n",
    "        print(f\"\\nðŸ“½ï¸ Frame {i+1} (Step {frame['step']}):\")\n",
    "        print(f\"   Top {top_n_tokens} important words:\")\n",
    "        \n",
    "        for j, token_data in enumerate(frame['top_tokens'][:top_n_tokens], 1):\n",
    "            token = token_data['token']\n",
    "            score = token_data['score']\n",
    "            print(f\"     {j:2d}. '{token}': {score:.4f}\")\n",
    "        \n",
    "        if i < len(frames) - 1:\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "\n",
    "def analyze_explainability_across_tasks(\n",
    "    video_dir: str,\n",
    "    task_ids: List[int]\n",
    ") -> Tuple[Dict[str, Any], Dict[int, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Analyze explainability patterns across multiple tasks.\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory containing explainability JSON files\n",
    "        task_ids: List of task IDs to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (aggregated_token_stats, task_summaries)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_tokens = {}\n",
    "    task_summaries = {}\n",
    "    \n",
    "    for task_id in task_ids:\n",
    "        json_path = Path(video_dir) / f\"task_{task_id:02d}_explainability.json\"\n",
    "        \n",
    "        if not json_path.exists():\n",
    "            print(f\"âš ï¸ Skipping task {task_id}: file not found\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract task summary\n",
    "            metadata = data['metadata']\n",
    "            task_summaries[task_id] = {\n",
    "                'prompt': metadata['task_prompt'],\n",
    "                'success_rate': metadata['success_rate'],\n",
    "                'frames_analyzed': metadata['num_frames_analyzed']\n",
    "            }\n",
    "            \n",
    "            # Aggregate token statistics\n",
    "            for frame in data['frames']:\n",
    "                for token_data in frame['top_tokens'][:5]:  # Top 5 per frame\n",
    "                    token = token_data['token']\n",
    "                    score = token_data['score']\n",
    "                    \n",
    "                    if token not in all_tokens:\n",
    "                        all_tokens[token] = {\n",
    "                            'scores': [],\n",
    "                            'task_appearances': set(),\n",
    "                            'total_appearances': 0\n",
    "                        }\n",
    "                    \n",
    "                    all_tokens[token]['scores'].append(score)\n",
    "                    all_tokens[token]['task_appearances'].add(task_id)\n",
    "                    all_tokens[token]['total_appearances'] += 1\n",
    "            \n",
    "            print(f\"âœ… Processed task {task_id}: {len(data['frames'])} frames\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing task {task_id}: {e}\")\n",
    "    \n",
    "    # Calculate aggregate statistics\n",
    "    token_stats = {}\n",
    "    for token, data in all_tokens.items():\n",
    "        scores = data['scores']\n",
    "        token_stats[token] = {\n",
    "            'mean_score': np.mean(scores),\n",
    "            'std_score': np.std(scores),\n",
    "            'max_score': np.max(scores),\n",
    "            'min_score': np.min(scores),\n",
    "            'total_appearances': data['total_appearances'],\n",
    "            'task_count': len(data['task_appearances']),\n",
    "            'task_ids': sorted(list(data['task_appearances']))\n",
    "        }\n",
    "    \n",
    "    # Sort by mean score\n",
    "    sorted_tokens = sorted(\n",
    "        token_stats.items(),\n",
    "        key=lambda x: x[1]['mean_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Cross-task Token Analysis:\")\n",
    "    print(f\"   Total unique tokens: {len(token_stats)}\")\n",
    "    print(f\"   Most important tokens across all tasks:\")\n",
    "    \n",
    "    for i, (token, stats) in enumerate(sorted_tokens[:15], 1):\n",
    "        print(f\"     {i:2d}. '{token}': avg={stats['mean_score']:.4f}, \"\n",
    "              f\"appeared {stats['total_appearances']} times across {stats['task_count']} tasks\")\n",
    "    \n",
    "    return token_stats, task_summaries\n",
    "\n",
    "\n",
    "def compare_explainability_success_correlation(\n",
    "    video_dir: str,\n",
    "    min_appearances: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze correlation between token importance and task success rates.\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory containing explainability JSON files\n",
    "        min_appearances: Minimum appearances required for token to be analyzed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all explainability files\n",
    "    json_files = list(Path(video_dir).glob(\"*_explainability.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"âŒ No explainability files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ“Š Analyzing {len(json_files)} tasks for success correlation...\")\n",
    "    \n",
    "    # Collect data\n",
    "    task_data = []\n",
    "    all_tokens = {}\n",
    "    \n",
    "    for json_path in json_files:\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            metadata = data['metadata']\n",
    "            task_id = metadata['task_id']\n",
    "            success_rate = metadata['success_rate']\n",
    "            \n",
    "            # Get average token scores for this task\n",
    "            task_token_scores = {}\n",
    "            for frame in data['frames']:\n",
    "                for token_data in frame['top_tokens'][:5]:\n",
    "                    token = token_data['token']\n",
    "                    score = token_data['score']\n",
    "                    \n",
    "                    if token not in task_token_scores:\n",
    "                        task_token_scores[token] = []\n",
    "                    task_token_scores[token].append(score)\n",
    "            \n",
    "            # Average scores per token for this task\n",
    "            task_avg_scores = {\n",
    "                token: np.mean(scores)\n",
    "                for token, scores in task_token_scores.items()\n",
    "            }\n",
    "            \n",
    "            task_data.append({\n",
    "                'task_id': task_id,\n",
    "                'success_rate': success_rate,\n",
    "                'token_scores': task_avg_scores,\n",
    "                'prompt': metadata['task_prompt']\n",
    "            })\n",
    "            \n",
    "            # Aggregate across all tasks\n",
    "            for token, avg_score in task_avg_scores.items():\n",
    "                if token not in all_tokens:\n",
    "                    all_tokens[token] = {\n",
    "                        'success_rates': [],\n",
    "                        'importance_scores': []\n",
    "                    }\n",
    "                all_tokens[token]['success_rates'].append(success_rate)\n",
    "                all_tokens[token]['importance_scores'].append(avg_score)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {json_path.name}: {e}\")\n",
    "    \n",
    "    # Analyze correlations\n",
    "    print(f\"\\nðŸ“ˆ Token-Success Correlation Analysis:\")\n",
    "    print(f\"   Minimum appearances threshold: {min_appearances}\")\n",
    "    \n",
    "    correlations = []\n",
    "    for token, data in all_tokens.items():\n",
    "        if len(data['success_rates']) >= min_appearances:\n",
    "            success_rates = np.array(data['success_rates'])\n",
    "            importance_scores = np.array(data['importance_scores'])\n",
    "            \n",
    "            # Calculate Pearson correlation\n",
    "            if len(success_rates) > 1 and np.std(success_rates) > 0 and np.std(importance_scores) > 0:\n",
    "                correlation = np.corrcoef(success_rates, importance_scores)[0, 1]\n",
    "                correlations.append((token, correlation, len(success_rates)))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f\"\\n   Top positively correlated tokens (higher importance â†’ higher success):\")\n",
    "    positive_corrs = [c for c in correlations if c[1] > 0][:10]\n",
    "    for i, (token, corr, count) in enumerate(positive_corrs, 1):\n",
    "        print(f\"     {i:2d}. '{token}': r={corr:.3f} (n={count})\")\n",
    "    \n",
    "    print(f\"\\n   Top negatively correlated tokens (higher importance â†’ lower success):\")\n",
    "    negative_corrs = [c for c in correlations if c[1] < 0][:10]\n",
    "    for i, (token, corr, count) in enumerate(negative_corrs, 1):\n",
    "        print(f\"     {i:2d}. '{token}': r={corr:.3f} (n={count})\")\n",
    "    \n",
    "    # Task performance summary\n",
    "    print(f\"\\nðŸ“Š Task Performance Summary:\")\n",
    "    task_data.sort(key=lambda x: x['success_rate'], reverse=True)\n",
    "    for task_info in task_data:\n",
    "        print(f\"   Task {task_info['task_id']}: {task_info['success_rate']:.2%} - {task_info['prompt']}\")\n",
    "\n",
    "print(\"\\nâœ… Explainability analysis functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUAL EXPLAINABILITY: Gradient-based Saliency Maps & Heatmap Videos\n",
    "# ============================================================================\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "class VisualExplainer:\n",
    "    \"\"\"\n",
    "    Classe per calcolare l'explainability visuale basata sui gradienti.\n",
    "    \n",
    "    Calcola quali regioni dell'immagine sono piÃ¹ importanti per la predizione\n",
    "    dell'azione usando backpropagation attraverso il visual encoder.\n",
    "    \n",
    "    Metodi disponibili:\n",
    "    - Vanilla Gradient: gradienti diretti sull'input\n",
    "    - Integrated Gradients: integrazione lungo un path baseline->input\n",
    "    - SmoothGrad: media di gradienti con rumore\n",
    "    - GradCAM-like: gradienti pesati sulle feature maps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: TRMPolicy model\n",
    "            device: device per computazione\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Hook per GradCAM-like saliency\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Registra hooks per catturare feature maps e gradienti dal backbone\"\"\"\n",
    "        def forward_hook(module, input, output):\n",
    "            self.feature_maps = output.detach()\n",
    "        \n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        \n",
    "        # Trova l'ultimo layer convoluzionale nel backbone\n",
    "        if hasattr(self.model, 'encoder') and hasattr(self.model.encoder, 'backbone'):\n",
    "            backbone = self.model.encoder.backbone\n",
    "            # ResNet backbone - ultimo layer prima del pooling\n",
    "            for name, module in backbone.named_modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    self.last_conv = module\n",
    "            \n",
    "            if hasattr(self, 'last_conv'):\n",
    "                self.last_conv.register_forward_hook(forward_hook)\n",
    "                self.last_conv.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    def compute_visual_saliency(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        method: str = 'gradcam',\n",
    "        target_action: Optional[torch.Tensor] = None,\n",
    "        smooth_samples: int = 20,\n",
    "        noise_level: float = 0.1\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calcola la saliency map per un'immagine di input.\n",
    "        \n",
    "        Args:\n",
    "            obs: Immagine di input (1, C, H, W)\n",
    "            prompt: Prompt testuale per il task\n",
    "            method: 'vanilla', 'smoothgrad', 'gradcam', 'integrated'\n",
    "            target_action: Azione target opzionale\n",
    "            smooth_samples: Numero di campioni per SmoothGrad\n",
    "            noise_level: Livello di rumore per SmoothGrad\n",
    "            \n",
    "        Returns:\n",
    "            Dict contenente:\n",
    "                - saliency_map: Mappa di saliency normalizzata (H, W)\n",
    "                - importance_score: Score totale dell'importanza visiva\n",
    "                - raw_gradients: Gradienti grezzi\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        if method == 'vanilla':\n",
    "            saliency = self._vanilla_gradient(obs, prompt, target_action)\n",
    "        elif method == 'smoothgrad':\n",
    "            saliency = self._smoothgrad(obs, prompt, target_action, smooth_samples, noise_level)\n",
    "        elif method == 'gradcam':\n",
    "            saliency = self._gradcam(obs, prompt, target_action)\n",
    "        elif method == 'integrated':\n",
    "            saliency = self._integrated_gradients(obs, prompt, target_action)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        # Normalizza saliency map in [0, 1]\n",
    "        saliency_norm = saliency - saliency.min()\n",
    "        if saliency_norm.max() > 0:\n",
    "            saliency_norm = saliency_norm / saliency_norm.max()\n",
    "        \n",
    "        # Calcola score totale (somma pesata dell'importanza)\n",
    "        importance_score = float(saliency.sum())\n",
    "        \n",
    "        return {\n",
    "            'saliency_map': saliency_norm,\n",
    "            'importance_score': importance_score,\n",
    "            'raw_saliency': saliency,\n",
    "            'method': method\n",
    "        }\n",
    "    \n",
    "    def _forward_with_grad(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        target_action: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass con gradienti abilitati\"\"\"\n",
    "        obs = obs.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Forward\n",
    "        if hasattr(self.model, 'use_text_prompts') and self.model.use_text_prompts:\n",
    "            actions = self.model(obs, [prompt])\n",
    "        else:\n",
    "            actions = self.model(obs, None)\n",
    "        \n",
    "        # Loss\n",
    "        if target_action is not None:\n",
    "            loss = F.mse_loss(actions, target_action)\n",
    "        else:\n",
    "            loss = actions.norm()\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        return obs.grad\n",
    "    \n",
    "    def _vanilla_gradient(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        target_action: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Calcola vanilla gradient saliency\"\"\"\n",
    "        grad = self._forward_with_grad(obs, prompt, target_action)\n",
    "        \n",
    "        # Aggrega sui canali con valore assoluto\n",
    "        saliency = grad.abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Smooth per visualizzazione\n",
    "        saliency = gaussian_filter(saliency, sigma=2)\n",
    "        \n",
    "        return saliency\n",
    "    \n",
    "    def _smoothgrad(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        target_action: Optional[torch.Tensor] = None,\n",
    "        n_samples: int = 20,\n",
    "        noise_level: float = 0.1\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"SmoothGrad: media di gradienti con input rumorosi\"\"\"\n",
    "        saliency_sum = None\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Aggiungi rumore\n",
    "            noise = torch.randn_like(obs) * noise_level\n",
    "            noisy_obs = obs + noise\n",
    "            \n",
    "            grad = self._forward_with_grad(noisy_obs, prompt, target_action)\n",
    "            saliency = grad.abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            if saliency_sum is None:\n",
    "                saliency_sum = saliency\n",
    "            else:\n",
    "                saliency_sum += saliency\n",
    "        \n",
    "        saliency = saliency_sum / n_samples\n",
    "        saliency = gaussian_filter(saliency, sigma=2)\n",
    "        \n",
    "        return saliency\n",
    "    \n",
    "    def _gradcam(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        target_action: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"GradCAM-like saliency usando feature maps del backbone\"\"\"\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        obs_grad = obs.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Forward\n",
    "        if hasattr(self.model, 'use_text_prompts') and self.model.use_text_prompts:\n",
    "            actions = self.model(obs_grad, [prompt])\n",
    "        else:\n",
    "            actions = self.model(obs_grad, None)\n",
    "        \n",
    "        # Loss\n",
    "        if target_action is not None:\n",
    "            loss = F.mse_loss(actions, target_action)\n",
    "        else:\n",
    "            loss = actions.norm()\n",
    "        \n",
    "        # Backward\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Se abbiamo feature maps e gradienti dai hooks\n",
    "        if self.feature_maps is not None and self.gradients is not None:\n",
    "            # Global average pooling dei gradienti\n",
    "            weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "            \n",
    "            # Weighted sum delle feature maps\n",
    "            cam = (weights * self.feature_maps).sum(dim=1).squeeze()\n",
    "            cam = F.relu(cam)\n",
    "            \n",
    "            # Resize alla dimensione dell'input\n",
    "            cam = cam.cpu().numpy()\n",
    "            cam = cv2.resize(cam, (obs.shape[3], obs.shape[2]))\n",
    "            \n",
    "            cam = gaussian_filter(cam, sigma=3)\n",
    "        else:\n",
    "            # Fallback a vanilla gradient\n",
    "            cam = self._vanilla_gradient(obs, prompt, target_action)\n",
    "        \n",
    "        return cam\n",
    "    \n",
    "    def _integrated_gradients(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        target_action: Optional[torch.Tensor] = None,\n",
    "        steps: int = 50\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Integrated Gradients con baseline nero\"\"\"\n",
    "        baseline = torch.zeros_like(obs)\n",
    "        scaled_inputs = [baseline + (float(i) / steps) * (obs - baseline) \n",
    "                         for i in range(1, steps + 1)]\n",
    "        \n",
    "        grads_sum = None\n",
    "        \n",
    "        for scaled_input in scaled_inputs:\n",
    "            grad = self._forward_with_grad(scaled_input, prompt, target_action)\n",
    "            grad_np = grad.abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            if grads_sum is None:\n",
    "                grads_sum = grad_np\n",
    "            else:\n",
    "                grads_sum += grad_np\n",
    "        \n",
    "        # Media dei gradienti * (input - baseline)\n",
    "        avg_grads = grads_sum / steps\n",
    "        integrated = avg_grads * (obs - baseline).abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        integrated = gaussian_filter(integrated, sigma=2)\n",
    "        \n",
    "        return integrated\n",
    "    \n",
    "    def generate_heatmap_overlay(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        saliency_map: np.ndarray,\n",
    "        alpha: float = 0.5,\n",
    "        colormap: int = cv2.COLORMAP_JET\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Genera un'immagine con heatmap sovrapposta.\n",
    "        \n",
    "        Args:\n",
    "            obs: Immagine originale (1, C, H, W) o (C, H, W) in [0, 1]\n",
    "            saliency_map: Mappa di saliency normalizzata (H, W)\n",
    "            alpha: Trasparenza della heatmap\n",
    "            colormap: Colormap OpenCV\n",
    "            \n",
    "        Returns:\n",
    "            Immagine con heatmap sovrapposta (H, W, 3) in formato uint8\n",
    "        \"\"\"\n",
    "        # Converti osservazione in numpy (H, W, 3)\n",
    "        if obs.dim() == 4:\n",
    "            obs = obs.squeeze(0)\n",
    "        \n",
    "        img = obs.permute(1, 2, 0).cpu().numpy()\n",
    "        img = (img * 255).clip(0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Assicurati che la saliency abbia la stessa dimensione\n",
    "        if saliency_map.shape != (img.shape[0], img.shape[1]):\n",
    "            saliency_map = cv2.resize(saliency_map, (img.shape[1], img.shape[0]))\n",
    "        \n",
    "        # Converti saliency in heatmap colorata\n",
    "        heatmap = (saliency_map * 255).astype(np.uint8)\n",
    "        heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)\n",
    "        \n",
    "        return overlay\n",
    "    \n",
    "    def compute_visual_importance_score(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        method: str = 'gradcam'\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calcola uno score scalare dell'importanza visiva totale.\n",
    "        \n",
    "        Utile per confrontare l'importanza visiva vs testuale.\n",
    "        \"\"\"\n",
    "        result = self.compute_visual_saliency(obs, prompt, method=method)\n",
    "        return result['importance_score']\n",
    "\n",
    "\n",
    "class MultimodalExplainer:\n",
    "    \"\"\"\n",
    "    Combina TextExplainer e VisualExplainer per analisi multimodale.\n",
    "    \n",
    "    Calcola:\n",
    "    - Importanza relativa text vs visual\n",
    "    - Correlazione con success rate\n",
    "    - Pattern temporali di attenzione multimodale\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        prompt_encoder: 'PromptEncoder',\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.text_explainer = TextExplainer(model, prompt_encoder, device)\n",
    "        self.visual_explainer = VisualExplainer(model, device)\n",
    "    \n",
    "    def compute_multimodal_importance(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        visual_method: str = 'gradcam'\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calcola importanza multimodale (text + visual) con normalizzazione comparabile.\n",
    "        \n",
    "        Utilizza Mean-Pooling normalization come in:\n",
    "        - Chefer et al. (2021) \"Generic Attention-model Explainability for Interpreting Bi-Modal \n",
    "          and Encoder-Decoder Transformers\"\n",
    "        - Sundararajan et al. (2017) \"Axiomatic Attribution for Deep Networks\"\n",
    "        \n",
    "        La normalizzazione per dimensione rende comparabili i contributi delle due modalitÃ :\n",
    "        - Text: media delle norme L2 sui token (invece di somma)\n",
    "        - Visual: media dei valori di saliency sui pixel (invece di somma)\n",
    "        \n",
    "        Returns:\n",
    "            Dict con:\n",
    "                - text_importance: Score importanza testuale (media per token)\n",
    "                - visual_importance: Score importanza visiva (media per pixel)\n",
    "                - text_ratio: Proporzione del contributo testuale [0, 1]\n",
    "                - visual_ratio: Proporzione del contributo visivo [0, 1]\n",
    "                - text_visual_ratio: Rapporto text/visual\n",
    "                - top_tokens: Token piÃ¹ importanti\n",
    "                - saliency_map: Mappa saliency visiva\n",
    "        \"\"\"\n",
    "        # Text importance\n",
    "        text_result = self.text_explainer.compute_token_saliency(obs, prompt)\n",
    "        \n",
    "        # Visual importance\n",
    "        visual_result = self.visual_explainer.compute_visual_saliency(\n",
    "            obs, prompt, method=visual_method\n",
    "        )\n",
    "        \n",
    "        # =========================================================================\n",
    "        # NORMALIZZAZIONE PER DIMENSIONE (Mean-Pooling)\n",
    "        # Approccio standard in letteratura per confronto multimodale equo\n",
    "        # =========================================================================\n",
    "        \n",
    "        # Numero di elementi per ciascuna modalitÃ \n",
    "        num_tokens = sum(text_result['attention_mask'])  # Solo token validi (no padding)\n",
    "        saliency_map = visual_result['saliency_map']\n",
    "        num_pixels = saliency_map.size  # H * W\n",
    "        \n",
    "        # Importanza MEDIA per elemento (invece di somma)\n",
    "        # Text: media delle norme L2 sui token\n",
    "        text_importance_mean = float(text_result['total_saliency']) / max(num_tokens, 1)\n",
    "        \n",
    "        # Visual: media dei valori di saliency sui pixel\n",
    "        visual_importance_mean = float(saliency_map.sum()) / max(num_pixels, 1)\n",
    "        \n",
    "        # Calcola ratio normalizzati\n",
    "        total = text_importance_mean + visual_importance_mean + 1e-8\n",
    "        text_ratio = text_importance_mean / total\n",
    "        visual_ratio = visual_importance_mean / total\n",
    "        \n",
    "        # Top tokens\n",
    "        top_tokens = self.text_explainer.get_top_k_tokens(text_result, k=5, filter_special=True)\n",
    "        \n",
    "        return {\n",
    "            'text_importance': text_importance_mean,\n",
    "            'visual_importance': visual_importance_mean,\n",
    "            'text_importance_raw': float(text_result['total_saliency']),  # Valore originale\n",
    "            'visual_importance_raw': float(saliency_map.sum()),  # Valore originale\n",
    "            'num_tokens': num_tokens,\n",
    "            'num_pixels': num_pixels,\n",
    "            'text_ratio': text_ratio,\n",
    "            'visual_ratio': visual_ratio,\n",
    "            'text_visual_ratio': text_importance_mean / (visual_importance_mean + 1e-8),\n",
    "            'top_tokens': top_tokens,\n",
    "            'saliency_map': visual_result['saliency_map'],\n",
    "            'token_saliency': text_result\n",
    "        }\n",
    "    \n",
    "    def compute_all_visual_methods(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calcola saliency maps con tutti i metodi disponibili.\n",
    "        \n",
    "        Metodi implementati secondo standard in letteratura:\n",
    "        - Vanilla Gradient (Simonyan et al., 2014)\n",
    "        - SmoothGrad (Smilkov et al., 2017)\n",
    "        - Grad-CAM (Selvaraju et al., 2017)\n",
    "        - Integrated Gradients (Sundararajan et al., 2017)\n",
    "        \n",
    "        Returns:\n",
    "            Dict con saliency map per ogni metodo\n",
    "        \"\"\"\n",
    "        methods = ['vanilla', 'smoothgrad', 'gradcam', 'integrated']\n",
    "        results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            try:\n",
    "                result = self.visual_explainer.compute_visual_saliency(\n",
    "                    obs, prompt, method=method\n",
    "                )\n",
    "                results[method] = result['saliency_map']\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error computing {method}: {e}\")\n",
    "                results[method] = np.zeros((obs.shape[2], obs.shape[3]), dtype=np.float32)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def generate_grid_explainability_video(\n",
    "    frames_data: List[Dict],\n",
    "    original_frames: List[np.ndarray],\n",
    "    output_path: str,\n",
    "    fps: int = 5,\n",
    "    include_text_overlay: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera un video con griglia 2x3 contenente tutte le metodologie di explainability.\n",
    "    \n",
    "    Layout griglia:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    Original     â”‚ Vanilla Gradientâ”‚    SmoothGrad   â”‚\n",
    "    â”‚                 â”‚ (Simonyan 2014) â”‚ (Smilkov 2017)  â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚    Grad-CAM     â”‚   Integrated    â”‚  Text Saliency  â”‚\n",
    "    â”‚ (Selvaraju 2017)â”‚   Gradients     â”‚   (Top Tokens)  â”‚\n",
    "    â”‚                 â”‚(Sundararajan17) â”‚                 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    Args:\n",
    "        frames_data: Lista di dict con saliency maps per ogni metodo\n",
    "        original_frames: Lista di frame originali (H, W, 3)\n",
    "        output_path: Path per il video output\n",
    "        fps: Frame per secondo\n",
    "        include_text_overlay: Se mostrare i top token\n",
    "    \"\"\"\n",
    "    if len(frames_data) == 0 or len(original_frames) == 0:\n",
    "        print(\"âš ï¸ No frames to generate video\")\n",
    "        return\n",
    "    \n",
    "    n_frames = min(len(frames_data), len(original_frames))\n",
    "    print(f\"ðŸ“¹ Generating grid video with {n_frames} frames...\")\n",
    "    \n",
    "    # Colori per i titoli\n",
    "    title_bg_color = (40, 40, 40)\n",
    "    title_text_color = (255, 255, 255)\n",
    "    title_height = 25\n",
    "    \n",
    "    # Dimensioni griglia: 2 righe x 3 colonne\n",
    "    # IMPORTANTE: Considerare altezza barra titolo per ogni pannello\n",
    "    h, w = original_frames[0].shape[:2]\n",
    "    panel_h = h + title_height  # Altezza pannello CON barra titolo\n",
    "    panel_w = w\n",
    "    grid_h = panel_h * 2  # 2 righe\n",
    "    grid_w = panel_w * 3  # 3 colonne\n",
    "    \n",
    "    print(f\"   Frame size: {w}x{h}, Panel size: {panel_w}x{panel_h}, Grid size: {grid_w}x{grid_h}\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (grid_w, grid_h))\n",
    "    \n",
    "    if not out.isOpened():\n",
    "        print(f\"âŒ Failed to open VideoWriter for {output_path}\")\n",
    "        return\n",
    "    \n",
    "    method_info = {\n",
    "        'original': ('Original Input', (200, 200, 200)),\n",
    "        'vanilla': ('Vanilla Gradient', (255, 100, 100)),\n",
    "        'smoothgrad': ('SmoothGrad', (100, 255, 100)),\n",
    "        'gradcam': ('Grad-CAM', (100, 100, 255)),\n",
    "        'integrated': ('Integrated Grad', (255, 255, 100)),\n",
    "        'text': ('Token Importance', (255, 150, 255))\n",
    "    }\n",
    "    \n",
    "    def add_title_bar(img: np.ndarray, title: str, color: Tuple[int, int, int]) -> np.ndarray:\n",
    "        \"\"\"Aggiunge una barra titolo all'immagine\"\"\"\n",
    "        result = np.zeros((img.shape[0] + title_height, img.shape[1], 3), dtype=np.uint8)\n",
    "        # Barra titolo\n",
    "        result[:title_height, :] = title_bg_color\n",
    "        # Linea colorata sotto il titolo\n",
    "        result[title_height-3:title_height, :] = color\n",
    "        # Immagine\n",
    "        result[title_height:, :] = img\n",
    "        # Testo\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.4\n",
    "        text_size = cv2.getTextSize(title, font, font_scale, 1)[0]\n",
    "        text_x = (img.shape[1] - text_size[0]) // 2\n",
    "        cv2.putText(result, title, (text_x, 17), font, font_scale, title_text_color, 1, cv2.LINE_AA)\n",
    "        return result\n",
    "    \n",
    "    def create_heatmap_overlay(frame: np.ndarray, saliency: np.ndarray, alpha: float = 0.4) -> np.ndarray:\n",
    "        \"\"\"Crea overlay heatmap su immagine\"\"\"\n",
    "        if saliency is None or saliency.size == 0:\n",
    "            return frame.copy()\n",
    "        \n",
    "        # Normalizza saliency\n",
    "        saliency = np.array(saliency, dtype=np.float32)\n",
    "        if saliency.shape[:2] != frame.shape[:2]:\n",
    "            saliency = cv2.resize(saliency, (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        if saliency.max() > saliency.min():\n",
    "            saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
    "        \n",
    "        heatmap = (saliency * 255).astype(np.uint8)\n",
    "        heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        overlay = cv2.addWeighted(frame, 1 - alpha, heatmap_colored, alpha, 0)\n",
    "        return overlay\n",
    "    \n",
    "    def create_text_importance_panel(frame: np.ndarray, top_tokens: List[Tuple[str, float]]) -> np.ndarray:\n",
    "        \"\"\"Crea pannello con importanza dei token testuali\"\"\"\n",
    "        panel = frame.copy()\n",
    "        \n",
    "        if not top_tokens:\n",
    "            return panel\n",
    "        \n",
    "        # Background semi-trasparente\n",
    "        overlay = panel.copy()\n",
    "        cv2.rectangle(overlay, (5, 5), (w-5, min(len(top_tokens)*22 + 10, h-5)), (0, 0, 0), -1)\n",
    "        panel = cv2.addWeighted(overlay, 0.7, panel, 0.3, 0)\n",
    "        \n",
    "        # Disegna barre di importanza per ogni token\n",
    "        max_score = max(score for _, score in top_tokens) if top_tokens else 1.0\n",
    "        y_offset = 20\n",
    "        bar_max_width = w - 80\n",
    "        \n",
    "        for token, score in top_tokens[:8]:  # Max 8 token\n",
    "            # Normalizza per larghezza barra\n",
    "            bar_width = int((score / max_score) * bar_max_width)\n",
    "            \n",
    "            # Colore barra (gradiente rosso-giallo-verde basato su score)\n",
    "            hue = int((score / max_score) * 60)  # Da rosso a giallo\n",
    "            bar_color = tuple(int(c) for c in cv2.cvtColor(\n",
    "                np.uint8([[[hue, 255, 255]]]), cv2.COLOR_HSV2RGB)[0][0])\n",
    "            \n",
    "            # Disegna barra\n",
    "            cv2.rectangle(panel, (10, y_offset-12), (10 + bar_width, y_offset+2), bar_color, -1)\n",
    "            \n",
    "            # Testo token\n",
    "            text = f\"{token[:10]}: {score:.3f}\"\n",
    "            cv2.putText(panel, text, (15, y_offset), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            y_offset += 22\n",
    "        \n",
    "        return panel\n",
    "    \n",
    "    frames_written = 0\n",
    "    for i in range(n_frames):\n",
    "        try:\n",
    "            frame = original_frames[i].copy()\n",
    "            data = frames_data[i]\n",
    "            \n",
    "            # Assicurati formato corretto\n",
    "            if frame.dtype != np.uint8:\n",
    "                if frame.max() <= 1.0:\n",
    "                    frame = (frame * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    frame = np.clip(frame, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            if len(frame.shape) == 2:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "            elif frame.shape[2] == 4:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "            \n",
    "            if frame.shape[0] != h or frame.shape[1] != w:\n",
    "                frame = cv2.resize(frame, (w, h))\n",
    "            \n",
    "            # Crea i 6 pannelli\n",
    "            panels = []\n",
    "            \n",
    "            # 1. Original\n",
    "            original_panel = add_title_bar(frame.copy(), *method_info['original'][:2])\n",
    "            panels.append(original_panel)\n",
    "            \n",
    "            # 2. Vanilla Gradient\n",
    "            vanilla_saliency = data.get('vanilla_saliency', data.get('saliency_map', None))\n",
    "            vanilla_overlay = create_heatmap_overlay(frame, vanilla_saliency)\n",
    "            vanilla_panel = add_title_bar(vanilla_overlay, *method_info['vanilla'][:2])\n",
    "            panels.append(vanilla_panel)\n",
    "            \n",
    "            # 3. SmoothGrad\n",
    "            smoothgrad_saliency = data.get('smoothgrad_saliency', data.get('saliency_map', None))\n",
    "            smoothgrad_overlay = create_heatmap_overlay(frame, smoothgrad_saliency)\n",
    "            smoothgrad_panel = add_title_bar(smoothgrad_overlay, *method_info['smoothgrad'][:2])\n",
    "            panels.append(smoothgrad_panel)\n",
    "            \n",
    "            # 4. Grad-CAM\n",
    "            gradcam_saliency = data.get('gradcam_saliency', data.get('saliency_map', None))\n",
    "            gradcam_overlay = create_heatmap_overlay(frame, gradcam_saliency)\n",
    "            gradcam_panel = add_title_bar(gradcam_overlay, *method_info['gradcam'][:2])\n",
    "            panels.append(gradcam_panel)\n",
    "            \n",
    "            # 5. Integrated Gradients\n",
    "            integrated_saliency = data.get('integrated_saliency', data.get('saliency_map', None))\n",
    "            integrated_overlay = create_heatmap_overlay(frame, integrated_saliency)\n",
    "            integrated_panel = add_title_bar(integrated_overlay, *method_info['integrated'][:2])\n",
    "            panels.append(integrated_panel)\n",
    "            \n",
    "            # 6. Text Token Importance\n",
    "            top_tokens = data.get('top_tokens', [])\n",
    "            text_panel = create_text_importance_panel(frame, top_tokens)\n",
    "            text_panel = add_title_bar(text_panel, *method_info['text'][:2])\n",
    "            panels.append(text_panel)\n",
    "            \n",
    "            # Verifica dimensioni pannelli prima di costruire griglia\n",
    "            for idx, p in enumerate(panels):\n",
    "                if p.shape[0] != panel_h or p.shape[1] != panel_w:\n",
    "                    panels[idx] = cv2.resize(p, (panel_w, panel_h))\n",
    "            \n",
    "            # Costruisci griglia 2x3\n",
    "            row1 = np.hstack([panels[0], panels[1], panels[2]])\n",
    "            row2 = np.hstack([panels[3], panels[4], panels[5]])\n",
    "            grid = np.vstack([row1, row2])\n",
    "            \n",
    "            # Verifica dimensioni griglia finale\n",
    "            if grid.shape[0] != grid_h or grid.shape[1] != grid_w:\n",
    "                grid = cv2.resize(grid, (grid_w, grid_h))\n",
    "            \n",
    "            # Aggiungi step counter\n",
    "            step = data.get('step', i)\n",
    "            cv2.putText(grid, f\"Step: {step}\", (10, grid.shape[0] - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Converti per OpenCV (BGR)\n",
    "            grid_bgr = cv2.cvtColor(grid, cv2.COLOR_RGB2BGR)\n",
    "            out.write(grid_bgr)\n",
    "            frames_written += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing frame {i}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    out.release()\n",
    "    \n",
    "    if frames_written > 0:\n",
    "        print(f\"âœ… Grid explainability video saved to {output_path} ({frames_written} frames)\")\n",
    "    else:\n",
    "        print(f\"âŒ No frames written to video!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LIBERO imports successful\n"
     ]
    }
   ],
   "source": [
    "# --- MONKEY PATCH (CRITICAL FIX) ---\n",
    "# FIX: Mock di Matplotlib per prevenire il kernel crash dovuto a conflitti di NumPy/ABI.\n",
    "mock_mpl = MagicMock()\n",
    "sys.modules[\"matplotlib\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.pyplot\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.cm\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.colors\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.transforms\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.ticker\"] = mock_mpl\n",
    "sys.modules[\"matplotlib._path\"] = mock_mpl\n",
    "# --------------------------------------\n",
    "\n",
    "# --- SETUP PATHS ---\n",
    "LIBERO_REPO_ROOT = Path('LIBERO')\n",
    "if LIBERO_REPO_ROOT.exists() and str(LIBERO_REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(LIBERO_REPO_ROOT))\n",
    "\n",
    "# Fix for Numba\n",
    "try:\n",
    "    from robosuite.utils.numba import jit_decorator\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Now these imports will work because matplotlib is mocked\n",
    "from libero.libero import get_libero_path\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "\n",
    "print(\"âœ“ LIBERO imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions per evaluation\n",
    "def _merge_training_config(stored: Dict[str, Any]) -> TrainingConfig:\n",
    "    class ConfigObj:\n",
    "        def __init__(self, **entries): self.__dict__.update(entries)\n",
    "    return ConfigObj(**stored)\n",
    "\n",
    "def _stack_vector_obs(obs: Any) -> Dict[str, np.ndarray]:\n",
    "    if isinstance(obs, list):\n",
    "        keys = obs[0].keys()\n",
    "        return {k: np.stack([o[k] for o in obs], axis=0) for k in keys}\n",
    "    return obs\n",
    "\n",
    "def _select_camera_key(obs_batch: Dict[str, np.ndarray]) -> str:\n",
    "    for key in ('agentview_rgb', 'agentview_image', 'robot0_agentview_image'):\n",
    "        if key in obs_batch: return key\n",
    "    return list(obs_batch.keys())[0]\n",
    "\n",
    "def _prepare_policy_input(images: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    imgs = torch.from_numpy(images).to(device=device, dtype=torch.float32) / 255.0\n",
    "    return imgs.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "class SequentialVectorEnv:\n",
    "    def __init__(self, env_fns: List[Callable]):\n",
    "        self.envs = [fn() for fn in env_fns]\n",
    "    def step(self, actions):\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        obs_list, rews_list, dones_list, infos_list = zip(*results)\n",
    "        return list(obs_list), np.array(rews_list), np.array(dones_list), list(infos_list)\n",
    "    def reset(self):\n",
    "        return [env.reset() for env in self.envs]\n",
    "    def seed(self, seed):\n",
    "        for i, env in enumerate(self.envs):\n",
    "            if hasattr(env, 'seed'):\n",
    "                env.seed(seed + i)\n",
    "    def set_init_state(self, states):\n",
    "        return [env.set_init_state(s) for env, s in zip(self.envs, states)]\n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EVALUATION FUNCTION (FIXED) ---\n",
    "def evaluate_model(\n",
    "    checkpoint_path: str = 'models/back.pt',\n",
    "    action_stats_path: str = 'action_stats.json',\n",
    "    benchmark: str = 'libero_spatial',\n",
    "    task_id: int = 6,\n",
    "    env_num: int = 10,\n",
    "    max_steps: int = 800,\n",
    "    seed: int = 42,\n",
    "    save_videos: bool = True,\n",
    "    video_dir: str = 'evaluation_videos',\n",
    "    camera_height: int = 128,\n",
    "    camera_width: int = 128,\n",
    "    video_skip: int = 1,\n",
    "    enable_explainability: bool = True,\n",
    "    explainability_interval: int = 10,\n",
    "    top_k_tokens: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    print(f\"Starting evaluation on {benchmark} Task {task_id} (Envs: {env_num}, Max Steps: {max_steps})...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    cfg = _merge_training_config(ckpt.get('config', {}))\n",
    "    policy = build_policy_from_config(cfg, obs_shape=(3, camera_height, camera_width)).to(device)\n",
    "    policy.load_state_dict(ckpt['model_state_dict'])\n",
    "    policy.eval()\n",
    "\n",
    "    stats = json.load(open(action_stats_path))\n",
    "    action_mean = torch.tensor(stats['mean'], device=device).unsqueeze(0)\n",
    "    action_std = torch.tensor(stats['std'], device=device).unsqueeze(0)\n",
    "    action_dim = int(action_mean.shape[-1])\n",
    "\n",
    "    benchmark_map = {'libero_10': 'LIBERO_10', 'libero_spatial': 'LIBERO_SPATIAL', 'libero_goal': 'LIBERO_GOAL'}\n",
    "    suite = get_benchmark(benchmark_map.get(benchmark, benchmark))(0)\n",
    "    task = suite.get_task(task_id)\n",
    "    task_prompt = task.language\n",
    "    #task_prompt = \"pick up the black bowl next to the ramekin and place it on the plate\"\n",
    "    use_prompts = getattr(policy, 'use_text_prompts', False)\n",
    "    if use_prompts:\n",
    "        print(f\"Using language prompt: '{task_prompt}'\")\n",
    "    \n",
    "    # Initialize TextExplainer if requested\n",
    "    text_explainer = None\n",
    "    explainability_data = []\n",
    "    if enable_explainability and use_prompts and hasattr(policy, 'prompt_encoder'):\n",
    "        text_explainer = TextExplainer(policy, policy.prompt_encoder, device)\n",
    "        print(f\"âœ… Explainability enabled (interval: every {explainability_interval} steps, top-{top_k_tokens} tokens)\")\n",
    "\n",
    "    env_args = {\n",
    "        'bddl_file_name': str(Path(get_libero_path('bddl_files')) / task.problem_folder / task.bddl_file),\n",
    "        'camera_heights': camera_height,\n",
    "        'camera_widths': camera_width\n",
    "    }\n",
    "    env = SequentialVectorEnv([lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)])\n",
    "\n",
    "    try:\n",
    "        init_states= torch.load(str(Path(get_libero_path('init_states')) / task.problem_folder / task.init_states_file), map_location='cpu', weights_only=False)\n",
    "        obs = env.reset()\n",
    "        env.seed(seed)\n",
    "        env.set_init_state(init_states[0:env_num])\n",
    "\n",
    "        dones = [False] * env_num\n",
    "        successes = np.zeros(env_num, dtype=bool)\n",
    "        video_path = Path(video_dir) / f\"task_{task_id:02d}\"\n",
    "        \n",
    "        \n",
    "        with VideoWriter(str(video_path), save_videos) as video_writer:\n",
    "            for step in range(max_steps):\n",
    "                obs_batch = _stack_vector_obs(obs)\n",
    "                cam_key = _select_camera_key(obs_batch)\n",
    "\n",
    "                alive = [i for i, d in enumerate(dones) if not d]\n",
    "                if not alive:\n",
    "                    break\n",
    "\n",
    "                vis_batch = obs_batch[cam_key][alive]\n",
    "                p_in = _prepare_policy_input(vis_batch, device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prompt_batch = [task_prompt for _ in alive] if use_prompts else None\n",
    "                    actions_alive = policy(p_in, prompt_batch)\n",
    "                    actions_alive = actions_alive * action_std + action_mean\n",
    "                    full_actions = np.zeros((env_num, action_dim), dtype=np.float32)\n",
    "                    full_actions[alive] = actions_alive.detach().cpu().numpy().astype(np.float32)\n",
    "                \n",
    "                # Explainability: calculate token saliency\n",
    "                if text_explainer is not None and step % explainability_interval == 0 and len(alive) > 0:\n",
    "                    try:\n",
    "                        # Use the first alive environment for analysis\n",
    "                        first_alive = alive[0]\n",
    "                        single_obs = p_in[0:1]  # First batch element\n",
    "                        \n",
    "                        # Calculate saliency\n",
    "                        saliency_result = text_explainer.compute_token_saliency(\n",
    "                            single_obs,\n",
    "                            task_prompt,\n",
    "                            target_action=None  # Unsupervised saliency\n",
    "                        )\n",
    "                        \n",
    "                        # Extract top-k tokens\n",
    "                        top_tokens = text_explainer.get_top_k_tokens(\n",
    "                            saliency_result,\n",
    "                            k=top_k_tokens,\n",
    "                            filter_special=True\n",
    "                        )\n",
    "                        \n",
    "                        # Save data for this frame\n",
    "                        frame_data = {\n",
    "                            'step': step,\n",
    "                            'prompt': task_prompt,\n",
    "                            'top_tokens': [\n",
    "                                {'token': token, 'score': float(score)}\n",
    "                                for token, score in top_tokens\n",
    "                            ],\n",
    "                            'all_tokens': {\n",
    "                                'tokens': saliency_result['token_strings'],\n",
    "                                'scores': saliency_result['normalized_scores'],\n",
    "                                'attention_mask': saliency_result['attention_mask']\n",
    "                            }\n",
    "                        }\n",
    "                        explainability_data.append(frame_data)\n",
    "                        \n",
    "                        # Debug logging\n",
    "                        if step % (explainability_interval * 5) == 0:\n",
    "                            print(f\"\\n  [Step {step}] Top important words:\")\n",
    "                            for token, score in top_tokens[:5]:\n",
    "                                print(f\"    '{token}': {score:.4f}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ Explainability error at step {step}: {e}\")\n",
    "\n",
    "                obs, reward, done_batch, info = env.step(full_actions)\n",
    "                \n",
    "                for i in alive:\n",
    "                    if reward[i] != 0.0:\n",
    "                        successes[i] = True\n",
    "                        #print(f\"âœ“ Success detected for env {i} at step {step}, reward = {reward[i]}\")\n",
    "                    dones[i] = dones[i] or bool(done_batch[i])\n",
    "                \n",
    "                if save_videos and step % video_skip == 0:\n",
    "                    video_writer.append_vector_obs(obs, dones, camera_name=cam_key)\n",
    "\n",
    "        success_rate = float(successes.mean())\n",
    "        print(f\"\\nðŸ“Š Final Results on task {task_id}: {successes.sum()}/{env_num} successes\")\n",
    "        results = {\n",
    "            'success_rate': success_rate,\n",
    "            'episodes': int(env_num),\n",
    "            'max_steps': int(max_steps)\n",
    "        }\n",
    "    finally:\n",
    "        env.close()\n",
    "    \n",
    "    # Save explainability data\n",
    "    if explainability_data:\n",
    "        explainability_json_path = Path(video_dir) / f\"task_{task_id:02d}_explainability.json\"\n",
    "        explainability_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Add metadata\n",
    "        explainability_output = {\n",
    "            'metadata': {\n",
    "                'task_id': task_id,\n",
    "                'benchmark': benchmark,\n",
    "                'task_prompt': task_prompt,\n",
    "                'num_frames_analyzed': len(explainability_data),\n",
    "                'success_rate': success_rate,\n",
    "                'explainability_interval': explainability_interval,\n",
    "                'top_k_tokens': top_k_tokens\n",
    "            },\n",
    "            'frames': explainability_data\n",
    "        }\n",
    "        \n",
    "        with open(explainability_json_path, 'w') as f:\n",
    "            json.dump(explainability_output, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ“ Explainability data saved to {explainability_json_path}\")\n",
    "        print(f\"  Analyzed {len(explainability_data)} frames\")\n",
    "        \n",
    "        # Print summary of most common words among top tokens\n",
    "        if explainability_data:\n",
    "            all_top_tokens = {}\n",
    "            for frame in explainability_data:\n",
    "                for token_data in frame['top_tokens'][:5]:  # Only top 5 per frame\n",
    "                    token = token_data['token']\n",
    "                    score = token_data['score']\n",
    "                    if token in all_top_tokens:\n",
    "                        all_top_tokens[token].append(score)\n",
    "                    else:\n",
    "                        all_top_tokens[token] = [score]\n",
    "            \n",
    "            # Calculate average per token\n",
    "            token_avg_scores = {\n",
    "                token: np.mean(scores)\n",
    "                for token, scores in all_top_tokens.items()\n",
    "            }\n",
    "            \n",
    "            # Sort by average score\n",
    "            sorted_tokens = sorted(token_avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\n  ðŸ“Š Most consistently important words across all frames:\")\n",
    "            for i, (token, avg_score) in enumerate(sorted_tokens[:10], 1):\n",
    "                freq = len(all_top_tokens[token])\n",
    "                print(f\"    {i}. '{token}': avg={avg_score:.4f}, appeared in top-5 {freq} times\")\n",
    "    \n",
    "    if save_videos:\n",
    "        src = Path(video_dir) / f\"task_{task_id:02d}\" / \"video.mp4\"\n",
    "        dst = Path(video_dir) / f\"task_{task_id:02d}.mp4\"\n",
    "        if src.exists():\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            src.rename(dst)\n",
    "            print(f\"âœ“ Videos saved to {dst}\")   \n",
    "            try:\n",
    "                src.parent.rmdir()\n",
    "            except OSError:\n",
    "                pass \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_explainability_results(\n",
    "    json_path: str,\n",
    "    top_n_frames: int = 5,\n",
    "    top_n_tokens: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizza i risultati dell'explainability da un file JSON.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path al file JSON con i risultati\n",
    "        top_n_frames: Numero di frame da mostrare\n",
    "        top_n_tokens: Numero di token da mostrare per frame\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metadata = data['metadata']\n",
    "    frames = data['frames']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPLAINABILITY RESULTS: Task {metadata['task_id']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Task Prompt: '{metadata['task_prompt']}'\")\n",
    "    print(f\"Success Rate: {metadata['success_rate']:.2%}\")\n",
    "    print(f\"Frames Analyzed: {metadata['num_frames_analyzed']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Mostra i frame piÃ¹ significativi\n",
    "    print(f\"Top {top_n_frames} analyzed frames:\\n\")\n",
    "    for i, frame in enumerate(frames[:top_n_frames], 1):\n",
    "        print(f\"\\nFrame #{i} (Step {frame['step']}):\")\n",
    "        print(f\"  Most Important Words:\")\n",
    "        for j, token_data in enumerate(frame['top_tokens'][:top_n_tokens], 1):\n",
    "            token = token_data['token']\n",
    "            score = token_data['score']\n",
    "            bar = 'â–ˆ' * int(score * 50)\n",
    "            print(f\"    {j:2d}. '{token:15s}' {score:6.4f} {bar}\")\n",
    "    \n",
    "    # Calcola statistiche aggregate\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"AGGREGATE STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Conta frequenza dei token nei top-k\n",
    "    token_frequencies = {}\n",
    "    token_total_scores = {}\n",
    "    \n",
    "    for frame in frames:\n",
    "        for token_data in frame['top_tokens'][:5]:  # Top 5 per frame\n",
    "            token = token_data['token']\n",
    "            score = token_data['score']\n",
    "            \n",
    "            if token in token_frequencies:\n",
    "                token_frequencies[token] += 1\n",
    "                token_total_scores[token] += score\n",
    "            else:\n",
    "                token_frequencies[token] = 1\n",
    "                token_total_scores[token] = score\n",
    "    \n",
    "    # Calcola medie e ordina\n",
    "    token_avg_scores = {\n",
    "        token: token_total_scores[token] / token_frequencies[token]\n",
    "        for token in token_frequencies\n",
    "    }\n",
    "    \n",
    "    sorted_by_freq = sorted(token_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_by_score = sorted(token_avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nMost Frequent Important Words:\")\n",
    "    for i, (token, freq) in enumerate(sorted_by_freq[:10], 1):\n",
    "        avg_score = token_avg_scores[token]\n",
    "        pct = (freq / len(frames)) * 100\n",
    "        print(f\"  {i:2d}. '{token:15s}': appeared {freq:3d} times ({pct:5.1f}%), avg score={avg_score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nHighest Average Importance Scores:\")\n",
    "    for i, (token, avg_score) in enumerate(sorted_by_score[:10], 1):\n",
    "        freq = token_frequencies[token]\n",
    "        print(f\"  {i:2d}. '{token:15s}': avg score={avg_score:.4f}, appeared {freq:3d} times\")\n",
    "\n",
    "\n",
    "def analyze_explainability_across_tasks(\n",
    "    video_dir: str = 'evaluation_videos',\n",
    "    task_ids: Optional[List[int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Analizza i risultati dell'explainability attraverso piÃ¹ task.\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory contenente i file JSON\n",
    "        task_ids: Lista di task ID da analizzare (None = tutti)\n",
    "    \"\"\"\n",
    "    video_path = Path(video_dir)\n",
    "    \n",
    "    # Trova tutti i file JSON di explainability\n",
    "    if task_ids is None:\n",
    "        json_files = list(video_path.glob('task_*_explainability.json'))\n",
    "    else:\n",
    "        json_files = [video_path / f'task_{tid:02d}_explainability.json' for tid in task_ids]\n",
    "        json_files = [f for f in json_files if f.exists()]\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"âš ï¸ No explainability JSON files found in {video_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CROSS-TASK EXPLAINABILITY ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Analyzing {len(json_files)} tasks\\n\")\n",
    "    \n",
    "    # Aggregazione dati da tutti i task\n",
    "    all_token_scores = {}\n",
    "    all_token_tasks = {}\n",
    "    task_summaries = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        task_id = data['metadata']['task_id']\n",
    "        task_prompt = data['metadata']['task_prompt']\n",
    "        success_rate = data['metadata']['success_rate']\n",
    "        \n",
    "        task_summaries.append({\n",
    "            'task_id': task_id,\n",
    "            'prompt': task_prompt,\n",
    "            'success_rate': success_rate\n",
    "        })\n",
    "        \n",
    "        # Aggrega token da questo task\n",
    "        for frame in data['frames']:\n",
    "            for token_data in frame['top_tokens'][:5]:\n",
    "                token = token_data['token']\n",
    "                score = token_data['score']\n",
    "                \n",
    "                if token in all_token_scores:\n",
    "                    all_token_scores[token].append(score)\n",
    "                    all_token_tasks[token].add(task_id)\n",
    "                else:\n",
    "                    all_token_scores[token] = [score]\n",
    "                    all_token_tasks[token] = {task_id}\n",
    "    \n",
    "    # Calcola statistiche globali\n",
    "    token_stats = []\n",
    "    for token, scores in all_token_scores.items():\n",
    "        token_stats.append({\n",
    "            'token': token,\n",
    "            'avg_score': np.mean(scores),\n",
    "            'std_score': np.std(scores),\n",
    "            'total_appearances': len(scores),\n",
    "            'num_tasks': len(all_token_tasks[token])\n",
    "        })\n",
    "    \n",
    "    # Ordina per importanza media\n",
    "    token_stats.sort(key=lambda x: x['avg_score'], reverse=True)\n",
    "    \n",
    "    print(f\"Most Important Words Across All Tasks:\")\n",
    "    print(f\"{'Rank':<6} {'Token':<20} {'Avg Score':<12} {'Std':<10} {'Appearances':<13} {'Tasks':<8}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    for i, stat in enumerate(token_stats[:20], 1):\n",
    "        print(f\"{i:<6} '{stat['token']:<18}' {stat['avg_score']:<12.4f} {stat['std_score']:<10.4f} \"\n",
    "              f\"{stat['total_appearances']:<13} {stat['num_tasks']:<8}\")\n",
    "    \n",
    "    # Analizza per success rate\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Task Success Rates:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    task_summaries.sort(key=lambda x: x['success_rate'], reverse=True)\n",
    "    for i, task in enumerate(task_summaries, 1):\n",
    "        print(f\"{i:2d}. Task {task['task_id']:2d} ({task['success_rate']:6.1%}): {task['prompt']}\")\n",
    "    \n",
    "    return token_stats, task_summaries\n",
    "\n",
    "\n",
    "def compare_explainability_success_correlation(\n",
    "    video_dir: str = 'evaluation_videos',\n",
    "    min_appearances: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Analizza la correlazione tra token importanti e success rate dei task.\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory con i file JSON\n",
    "        min_appearances: Minimo numero di apparizioni per considerare un token\n",
    "    \"\"\"\n",
    "    video_path = Path(video_dir)\n",
    "    json_files = list(video_path.glob('task_*_explainability.json'))\n",
    "    \n",
    "    if len(json_files) < 2:\n",
    "        print(\"âš ï¸ Need at least 2 tasks for correlation analysis\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPLAINABILITY vs SUCCESS RATE CORRELATION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Raccogli dati\n",
    "    task_data = []\n",
    "    all_tokens = set()\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        task_id = data['metadata']['task_id']\n",
    "        success_rate = data['metadata']['success_rate']\n",
    "        \n",
    "        # Calcola token importance per questo task\n",
    "        token_importance = {}\n",
    "        for frame in data['frames']:\n",
    "            for token_data in frame['top_tokens'][:5]:\n",
    "                token = token_data['token']\n",
    "                score = token_data['score']\n",
    "                \n",
    "                if token in token_importance:\n",
    "                    token_importance[token].append(score)\n",
    "                else:\n",
    "                    token_importance[token] = [score]\n",
    "                \n",
    "                all_tokens.add(token)\n",
    "        \n",
    "        # Media per token\n",
    "        token_avg = {token: np.mean(scores) for token, scores in token_importance.items()}\n",
    "        \n",
    "        task_data.append({\n",
    "            'task_id': task_id,\n",
    "            'success_rate': success_rate,\n",
    "            'token_importance': token_avg\n",
    "        })\n",
    "    \n",
    "    # Filtra token con poche apparizioni\n",
    "    token_task_count = {token: 0 for token in all_tokens}\n",
    "    for task in task_data:\n",
    "        for token in task['token_importance']:\n",
    "            token_task_count[token] += 1\n",
    "    \n",
    "    frequent_tokens = [token for token, count in token_task_count.items() \n",
    "                       if count >= min_appearances]\n",
    "    \n",
    "    print(f\"Analyzing {len(frequent_tokens)} tokens that appear in >={min_appearances} tasks\")\n",
    "    \n",
    "    # Per ogni token, calcola correlazione con success rate\n",
    "    correlations = []\n",
    "    for token in frequent_tokens:\n",
    "        success_rates = []\n",
    "        importance_scores = []\n",
    "        \n",
    "        for task in task_data:\n",
    "            if token in task['token_importance']:\n",
    "                success_rates.append(task['success_rate'])\n",
    "                importance_scores.append(task['token_importance'][token])\n",
    "        \n",
    "        if len(success_rates) >= min_appearances:\n",
    "            # Calcola correlazione di Pearson\n",
    "            corr = np.corrcoef(success_rates, importance_scores)[0, 1]\n",
    "            correlations.append({\n",
    "                'token': token,\n",
    "                'correlation': corr,\n",
    "                'num_tasks': len(success_rates),\n",
    "                'avg_importance': np.mean(importance_scores)\n",
    "            })\n",
    "    \n",
    "    # Ordina per correlazione assoluta\n",
    "    correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "    \n",
    "    print(f\"\\nTokens Most Correlated with Success (positive = important for success):\")\n",
    "    print(f\"{'Rank':<6} {'Token':<20} {'Correlation':<13} {'Avg Importance':<16} {'Tasks':<8}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    for i, stat in enumerate(correlations[:15], 1):\n",
    "        corr_str = f\"{stat['correlation']:+.4f}\"\n",
    "        print(f\"{i:<6} '{stat['token']:<18}' {corr_str:<13} {stat['avg_importance']:<16.4f} {stat['num_tasks']:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on libero_spatial Task 0 (Envs: 5, Max Steps: 600)...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Using language prompt: 'pick up the black bowl between the plate and the ramekin and place it on the plate'\n",
      "âœ… Explainability enabled (interval: every 5 steps, top-5 tokens)\n",
      "\n",
      "  [Step 0] Top important words:\n",
      "    'plate': 0.1070\n",
      "    'ram': 0.0831\n",
      "    'between': 0.0802\n",
      "    'it': 0.0584\n",
      "    'the': 0.0526\n",
      "\n",
      "  [Step 25] Top important words:\n",
      "    'ram': 0.1644\n",
      "    'kin': 0.0891\n",
      "    'plate': 0.0876\n",
      "    'between': 0.0785\n",
      "    'e': 0.0743\n",
      "\n",
      "  [Step 50] Top important words:\n",
      "    'ram': 0.1419\n",
      "    'plate': 0.0912\n",
      "    'between': 0.0777\n",
      "    'kin': 0.0662\n",
      "    'e': 0.0617\n",
      "\n",
      "  [Step 75] Top important words:\n",
      "    'ram': 0.1194\n",
      "    'plate': 0.0852\n",
      "    'the': 0.0724\n",
      "    'between': 0.0706\n",
      "    'the': 0.0547\n",
      "\n",
      "  [Step 100] Top important words:\n",
      "    'plate': 0.1369\n",
      "    'between': 0.1366\n",
      "    'and': 0.0906\n",
      "    'and': 0.0855\n",
      "    'ram': 0.0795\n",
      "\n",
      "  [Step 125] Top important words:\n",
      "    'between': 0.1233\n",
      "    'plate': 0.1020\n",
      "    'ram': 0.0818\n",
      "    'and': 0.0751\n",
      "    'and': 0.0641\n",
      "\n",
      "  [Step 150] Top important words:\n",
      "    'between': 0.0971\n",
      "    'plate': 0.0689\n",
      "    'ram': 0.0674\n",
      "    'the': 0.0670\n",
      "    'and': 0.0641\n",
      "\n",
      "  [Step 175] Top important words:\n",
      "    'between': 0.1001\n",
      "    'the': 0.0712\n",
      "    'plate': 0.0636\n",
      "    'and': 0.0621\n",
      "    'ram': 0.0589\n",
      "\n",
      "  [Step 200] Top important words:\n",
      "    'between': 0.1083\n",
      "    'the': 0.0700\n",
      "    'plate': 0.0694\n",
      "    'and': 0.0689\n",
      "    'and': 0.0601\n",
      "\n",
      "  [Step 225] Top important words:\n",
      "    'between': 0.1021\n",
      "    'plate': 0.0784\n",
      "    'the': 0.0741\n",
      "    'ram': 0.0649\n",
      "    'and': 0.0639\n",
      "\n",
      "  [Step 250] Top important words:\n",
      "    'ram': 0.1095\n",
      "    'between': 0.0985\n",
      "    'plate': 0.0890\n",
      "    'and': 0.0585\n",
      "    'the': 0.0585\n",
      "\n",
      "  [Step 275] Top important words:\n",
      "    'plate': 0.1082\n",
      "    'between': 0.0834\n",
      "    'ram': 0.0825\n",
      "    'the': 0.0683\n",
      "    'kin': 0.0554\n",
      "\n",
      "  [Step 300] Top important words:\n",
      "    'plate': 0.1603\n",
      "    'kin': 0.0934\n",
      "    'ram': 0.0917\n",
      "    'the': 0.0624\n",
      "    'e': 0.0606\n",
      "\n",
      "  [Step 325] Top important words:\n",
      "    'plate': 0.1377\n",
      "    'ram': 0.0994\n",
      "    'kin': 0.0983\n",
      "    'e': 0.0658\n",
      "    'between': 0.0577\n",
      "\n",
      "  [Step 350] Top important words:\n",
      "    'plate': 0.1032\n",
      "    'ram': 0.0871\n",
      "    'kin': 0.0756\n",
      "    'the': 0.0714\n",
      "    'between': 0.0619\n",
      "\n",
      "  [Step 375] Top important words:\n",
      "    'plate': 0.1022\n",
      "    'ram': 0.0875\n",
      "    'the': 0.0738\n",
      "    'kin': 0.0733\n",
      "    'between': 0.0652\n",
      "\n",
      "  [Step 400] Top important words:\n",
      "    'plate': 0.1490\n",
      "    'ram': 0.0746\n",
      "    'the': 0.0700\n",
      "    'kin': 0.0682\n",
      "    'between': 0.0671\n",
      "\n",
      "  [Step 425] Top important words:\n",
      "    'plate': 0.1106\n",
      "    'the': 0.0835\n",
      "    'between': 0.0812\n",
      "    'ram': 0.0642\n",
      "    'kin': 0.0608\n",
      "\n",
      "  [Step 450] Top important words:\n",
      "    'plate': 0.0965\n",
      "    'the': 0.0806\n",
      "    'between': 0.0773\n",
      "    'ram': 0.0700\n",
      "    'kin': 0.0663\n",
      "\n",
      "  [Step 475] Top important words:\n",
      "    'plate': 0.0978\n",
      "    'the': 0.0856\n",
      "    'between': 0.0850\n",
      "    'ram': 0.0582\n",
      "    'and': 0.0560\n",
      "\n",
      "  [Step 500] Top important words:\n",
      "    'plate': 0.1053\n",
      "    'the': 0.0875\n",
      "    'between': 0.0803\n",
      "    'ram': 0.0710\n",
      "    'the': 0.0531\n",
      "\n",
      "  [Step 525] Top important words:\n",
      "    'plate': 0.1094\n",
      "    'the': 0.0876\n",
      "    'between': 0.0695\n",
      "    'ram': 0.0672\n",
      "    'kin': 0.0649\n",
      "\n",
      "  [Step 550] Top important words:\n",
      "    'plate': 0.1471\n",
      "    'ram': 0.0819\n",
      "    'the': 0.0800\n",
      "    'between': 0.0741\n",
      "    'kin': 0.0639\n",
      "\n",
      "  [Step 575] Top important words:\n",
      "    'plate': 0.1484\n",
      "    'the': 0.0845\n",
      "    'ram': 0.0776\n",
      "    'between': 0.0648\n",
      "    'kin': 0.0564\n",
      "Saved videos to evaluation_videos/task_00.\n",
      "\n",
      "ðŸ“Š Final Results on task 0: 2/5 successes\n",
      "âœ“ Explainability data saved to evaluation_videos/task_00_explainability.json\n",
      "  Analyzed 120 frames\n",
      "\n",
      "  ðŸ“Š Most consistently important words across all frames:\n",
      "    1. 'plate': avg=0.1057, appeared in top-5 120 times\n",
      "    2. 'ram': avg=0.0823, appeared in top-5 116 times\n",
      "    3. 'between': avg=0.0820, appeared in top-5 117 times\n",
      "    4. 'the': avg=0.0733, appeared in top-5 100 times\n",
      "    5. 'kin': avg=0.0704, appeared in top-5 70 times\n",
      "    6. 'and': avg=0.0645, appeared in top-5 47 times\n",
      "    7. 'e': avg=0.0642, appeared in top-5 21 times\n",
      "    8. 'it': avg=0.0582, appeared in top-5 6 times\n",
      "    9. 'on': avg=0.0528, appeared in top-5 2 times\n",
      "    10. 'place': avg=0.0521, appeared in top-5 1 times\n",
      "âœ“ Videos saved to evaluation_videos/task_00.mp4\n",
      "Starting evaluation on libero_spatial Task 1 (Envs: 5, Max Steps: 600)...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Using language prompt: 'pick up the black bowl next to the ramekin and place it on the plate'\n",
      "âœ… Explainability enabled (interval: every 5 steps, top-5 tokens)\n",
      "\n",
      "  [Step 0] Top important words:\n",
      "    'plate': 0.0915\n",
      "    'ram': 0.0759\n",
      "    'and': 0.0739\n",
      "    'on': 0.0645\n",
      "    'pick': 0.0605\n",
      "\n",
      "  [Step 25] Top important words:\n",
      "    'ram': 0.1968\n",
      "    'plate': 0.0744\n",
      "    'kin': 0.0607\n",
      "    'the': 0.0583\n",
      "    'on': 0.0581\n",
      "\n",
      "  [Step 50] Top important words:\n",
      "    'ram': 0.0941\n",
      "    'plate': 0.0854\n",
      "    'the': 0.0832\n",
      "    'on': 0.0607\n",
      "    'the': 0.0539\n",
      "\n",
      "  [Step 75] Top important words:\n",
      "    'plate': 0.0866\n",
      "    'on': 0.0773\n",
      "    'the': 0.0586\n",
      "    'and': 0.0572\n",
      "    'ram': 0.0567\n",
      "\n",
      "  [Step 100] Top important words:\n",
      "    'ram': 0.1310\n",
      "    'plate': 0.1002\n",
      "    'kin': 0.0809\n",
      "    'the': 0.0657\n",
      "    'and': 0.0625\n",
      "\n",
      "  [Step 125] Top important words:\n",
      "    'ram': 0.1111\n",
      "    'plate': 0.0922\n",
      "    'and': 0.0813\n",
      "    'pick': 0.0660\n",
      "    'up': 0.0590\n",
      "\n",
      "  [Step 150] Top important words:\n",
      "    'on': 0.0851\n",
      "    'plate': 0.0836\n",
      "    'ram': 0.0798\n",
      "    'kin': 0.0654\n",
      "    'the': 0.0602\n",
      "\n",
      "  [Step 175] Top important words:\n",
      "    'on': 0.0808\n",
      "    'plate': 0.0791\n",
      "    'ram': 0.0746\n",
      "    'kin': 0.0714\n",
      "    'the': 0.0666\n",
      "\n",
      "  [Step 200] Top important words:\n",
      "    'ram': 0.1023\n",
      "    'plate': 0.0849\n",
      "    'kin': 0.0704\n",
      "    'the': 0.0667\n",
      "    'on': 0.0657\n",
      "\n",
      "  [Step 225] Top important words:\n",
      "    'plate': 0.0863\n",
      "    'on': 0.0849\n",
      "    'ram': 0.0796\n",
      "    'the': 0.0687\n",
      "    'kin': 0.0661\n",
      "\n",
      "  [Step 250] Top important words:\n",
      "    'ram': 0.0926\n",
      "    'the': 0.0743\n",
      "    'plate': 0.0734\n",
      "    'kin': 0.0725\n",
      "    'on': 0.0606\n",
      "\n",
      "  [Step 275] Top important words:\n",
      "    'ram': 0.0847\n",
      "    'the': 0.0758\n",
      "    'kin': 0.0744\n",
      "    'plate': 0.0727\n",
      "    'pick': 0.0575\n",
      "\n",
      "  [Step 300] Top important words:\n",
      "    'ram': 0.1146\n",
      "    'plate': 0.0917\n",
      "    'kin': 0.0721\n",
      "    'the': 0.0626\n",
      "    'e': 0.0597\n",
      "\n",
      "  [Step 325] Top important words:\n",
      "    'ram': 0.1259\n",
      "    'plate': 0.1112\n",
      "    'e': 0.0570\n",
      "    'and': 0.0543\n",
      "    'the': 0.0533\n",
      "\n",
      "  [Step 350] Top important words:\n",
      "    'ram': 0.1423\n",
      "    'plate': 0.1067\n",
      "    'and': 0.0573\n",
      "    'e': 0.0561\n",
      "    'on': 0.0524\n",
      "\n",
      "  [Step 375] Top important words:\n",
      "    'plate': 0.1132\n",
      "    'ram': 0.1006\n",
      "    'and': 0.0575\n",
      "    'on': 0.0570\n",
      "    'e': 0.0556\n",
      "\n",
      "  [Step 400] Top important words:\n",
      "    'ram': 0.1137\n",
      "    'plate': 0.0961\n",
      "    'e': 0.0608\n",
      "    'kin': 0.0598\n",
      "    'the': 0.0592\n",
      "\n",
      "  [Step 425] Top important words:\n",
      "    'ram': 0.1510\n",
      "    'plate': 0.0878\n",
      "    'kin': 0.0842\n",
      "    'e': 0.0576\n",
      "    'the': 0.0538\n",
      "\n",
      "  [Step 450] Top important words:\n",
      "    'ram': 0.1321\n",
      "    'kin': 0.0867\n",
      "    'plate': 0.0801\n",
      "    'on': 0.0604\n",
      "    'the': 0.0575\n",
      "\n",
      "  [Step 475] Top important words:\n",
      "    'ram': 0.1476\n",
      "    'plate': 0.0829\n",
      "    'kin': 0.0788\n",
      "    'e': 0.0602\n",
      "    'it': 0.0555\n",
      "\n",
      "  [Step 500] Top important words:\n",
      "    'ram': 0.1252\n",
      "    'plate': 0.1083\n",
      "    'and': 0.0595\n",
      "    'e': 0.0576\n",
      "    'on': 0.0530\n",
      "\n",
      "  [Step 525] Top important words:\n",
      "    'plate': 0.0815\n",
      "    'ram': 0.0695\n",
      "    'the': 0.0671\n",
      "    'the': 0.0627\n",
      "    'it': 0.0546\n",
      "\n",
      "  [Step 550] Top important words:\n",
      "    'ram': 0.0813\n",
      "    'plate': 0.0774\n",
      "    'kin': 0.0704\n",
      "    'the': 0.0672\n",
      "    'on': 0.0668\n",
      "\n",
      "  [Step 575] Top important words:\n",
      "    'ram': 0.1183\n",
      "    'plate': 0.0864\n",
      "    'kin': 0.0681\n",
      "    'the': 0.0606\n",
      "    'the': 0.0588\n",
      "Saved videos to evaluation_videos/task_01.\n",
      "\n",
      "ðŸ“Š Final Results on task 1: 3/5 successes\n",
      "âœ“ Explainability data saved to evaluation_videos/task_01_explainability.json\n",
      "  Analyzed 120 frames\n",
      "\n",
      "  ðŸ“Š Most consistently important words across all frames:\n",
      "    1. 'ram': avg=0.1091, appeared in top-5 120 times\n",
      "    2. 'plate': avg=0.0900, appeared in top-5 120 times\n",
      "    3. 'kin': avg=0.0686, appeared in top-5 77 times\n",
      "    4. 'on': avg=0.0641, appeared in top-5 60 times\n",
      "    5. 'the': avg=0.0640, appeared in top-5 110 times\n",
      "    6. 'and': avg=0.0629, appeared in top-5 39 times\n",
      "    7. 'pick': avg=0.0587, appeared in top-5 9 times\n",
      "    8. 'e': avg=0.0578, appeared in top-5 48 times\n",
      "    9. 'it': avg=0.0557, appeared in top-5 14 times\n",
      "    10. 'up': avg=0.0556, appeared in top-5 2 times\n",
      "âœ“ Videos saved to evaluation_videos/task_01.mp4\n",
      "Starting evaluation on libero_spatial Task 2 (Envs: 5, Max Steps: 600)...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Using language prompt: 'pick up the black bowl from table center and place it on the plate'\n",
      "âœ… Explainability enabled (interval: every 5 steps, top-5 tokens)\n",
      "\n",
      "  [Step 0] Top important words:\n",
      "    'pick': 0.0857\n",
      "    'plate': 0.0811\n",
      "    'the': 0.0762\n",
      "    'the': 0.0743\n",
      "    'up': 0.0696\n",
      "\n",
      "  [Step 25] Top important words:\n",
      "    'plate': 0.0948\n",
      "    'pick': 0.0820\n",
      "    'on': 0.0757\n",
      "    'the': 0.0651\n",
      "    'up': 0.0627\n",
      "\n",
      "  [Step 50] Top important words:\n",
      "    'plate': 0.1090\n",
      "    'pick': 0.0840\n",
      "    'on': 0.0784\n",
      "    'the': 0.0780\n",
      "    'up': 0.0666\n",
      "\n",
      "  [Step 75] Top important words:\n",
      "    'plate': 0.0851\n",
      "    'center': 0.0786\n",
      "    'on': 0.0772\n",
      "    'pick': 0.0725\n",
      "    'the': 0.0608\n",
      "\n",
      "  [Step 100] Top important words:\n",
      "    'plate': 0.1121\n",
      "    'pick': 0.1074\n",
      "    'up': 0.0798\n",
      "    'on': 0.0564\n",
      "    'place': 0.0553\n",
      "\n",
      "  [Step 125] Top important words:\n",
      "    'plate': 0.1371\n",
      "    'pick': 0.0849\n",
      "    'up': 0.0659\n",
      "    'center': 0.0580\n",
      "    'place': 0.0560\n",
      "\n",
      "  [Step 150] Top important words:\n",
      "    'plate': 0.1115\n",
      "    'center': 0.0838\n",
      "    'pick': 0.0802\n",
      "    'table': 0.0667\n",
      "    'on': 0.0583\n",
      "\n",
      "  [Step 175] Top important words:\n",
      "    'center': 0.0936\n",
      "    'plate': 0.0881\n",
      "    'pick': 0.0824\n",
      "    'table': 0.0700\n",
      "    'up': 0.0581\n",
      "\n",
      "  [Step 200] Top important words:\n",
      "    'center': 0.0871\n",
      "    'plate': 0.0867\n",
      "    'pick': 0.0788\n",
      "    'table': 0.0623\n",
      "    'the': 0.0618\n",
      "\n",
      "  [Step 225] Top important words:\n",
      "    'plate': 0.1022\n",
      "    'pick': 0.0822\n",
      "    'center': 0.0797\n",
      "    'place': 0.0626\n",
      "    'and': 0.0617\n",
      "\n",
      "  [Step 250] Top important words:\n",
      "    'plate': 0.0943\n",
      "    'center': 0.0897\n",
      "    'pick': 0.0775\n",
      "    'and': 0.0669\n",
      "    'up': 0.0662\n",
      "\n",
      "  [Step 275] Top important words:\n",
      "    'plate': 0.0831\n",
      "    'center': 0.0827\n",
      "    'pick': 0.0781\n",
      "    'up': 0.0691\n",
      "    'the': 0.0627\n",
      "\n",
      "  [Step 300] Top important words:\n",
      "    'plate': 0.0919\n",
      "    'center': 0.0850\n",
      "    'pick': 0.0801\n",
      "    'up': 0.0726\n",
      "    'the': 0.0601\n",
      "\n",
      "  [Step 325] Top important words:\n",
      "    'plate': 0.0927\n",
      "    'center': 0.0843\n",
      "    'pick': 0.0828\n",
      "    'up': 0.0764\n",
      "    'table': 0.0592\n",
      "\n",
      "  [Step 350] Top important words:\n",
      "    'plate': 0.1001\n",
      "    'pick': 0.0853\n",
      "    'center': 0.0810\n",
      "    'up': 0.0689\n",
      "    'table': 0.0644\n",
      "\n",
      "  [Step 375] Top important words:\n",
      "    'pick': 0.0874\n",
      "    'plate': 0.0862\n",
      "    'up': 0.0798\n",
      "    'the': 0.0696\n",
      "    'on': 0.0665\n",
      "\n",
      "  [Step 400] Top important words:\n",
      "    'plate': 0.0898\n",
      "    'pick': 0.0776\n",
      "    'the': 0.0700\n",
      "    'up': 0.0689\n",
      "    'center': 0.0658\n",
      "\n",
      "  [Step 425] Top important words:\n",
      "    'center': 0.1161\n",
      "    'table': 0.0860\n",
      "    'plate': 0.0814\n",
      "    'the': 0.0666\n",
      "    'from': 0.0596\n",
      "\n",
      "  [Step 450] Top important words:\n",
      "    'center': 0.1139\n",
      "    'table': 0.0798\n",
      "    'plate': 0.0791\n",
      "    'up': 0.0651\n",
      "    'pick': 0.0649\n",
      "\n",
      "  [Step 475] Top important words:\n",
      "    'center': 0.1030\n",
      "    'plate': 0.0821\n",
      "    'table': 0.0743\n",
      "    'pick': 0.0674\n",
      "    'and': 0.0617\n",
      "\n",
      "  [Step 500] Top important words:\n",
      "    'center': 0.0961\n",
      "    'plate': 0.0879\n",
      "    'pick': 0.0698\n",
      "    'table': 0.0689\n",
      "    'up': 0.0616\n",
      "\n",
      "  [Step 525] Top important words:\n",
      "    'center': 0.1105\n",
      "    'plate': 0.0838\n",
      "    'table': 0.0797\n",
      "    'the': 0.0613\n",
      "    'pick': 0.0583\n",
      "\n",
      "  [Step 550] Top important words:\n",
      "    'center': 0.1081\n",
      "    'plate': 0.0863\n",
      "    'table': 0.0794\n",
      "    'the': 0.0626\n",
      "    'and': 0.0583\n",
      "\n",
      "  [Step 575] Top important words:\n",
      "    'plate': 0.0883\n",
      "    'center': 0.0834\n",
      "    'pick': 0.0742\n",
      "    'up': 0.0652\n",
      "    'the': 0.0625\n",
      "Saved videos to evaluation_videos/task_02.\n",
      "\n",
      "ðŸ“Š Final Results on task 2: 4/5 successes\n",
      "âœ“ Explainability data saved to evaluation_videos/task_02_explainability.json\n",
      "  Analyzed 120 frames\n",
      "\n",
      "  ðŸ“Š Most consistently important words across all frames:\n",
      "    1. 'plate': avg=0.0933, appeared in top-5 120 times\n",
      "    2. 'center': avg=0.0883, appeared in top-5 103 times\n",
      "    3. 'pick': avg=0.0795, appeared in top-5 106 times\n",
      "    4. 'table': avg=0.0703, appeared in top-5 70 times\n",
      "    5. 'up': avg=0.0692, appeared in top-5 69 times\n",
      "    6. 'the': avg=0.0662, appeared in top-5 63 times\n",
      "    7. 'on': avg=0.0654, appeared in top-5 28 times\n",
      "    8. 'it': avg=0.0643, appeared in top-5 7 times\n",
      "    9. 'and': avg=0.0642, appeared in top-5 19 times\n",
      "    10. 'from': avg=0.0594, appeared in top-5 3 times\n",
      "âœ“ Videos saved to evaluation_videos/task_02.mp4\n",
      "Starting evaluation on libero_spatial Task 3 (Envs: 5, Max Steps: 600)...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Using language prompt: 'pick up the black bowl on the cookie box and place it on the plate'\n",
      "âœ… Explainability enabled (interval: every 5 steps, top-5 tokens)\n",
      "\n",
      "  [Step 0] Top important words:\n",
      "    'the': 0.0799\n",
      "    'plate': 0.0788\n",
      "    'the': 0.0772\n",
      "    'pick': 0.0735\n",
      "    'up': 0.0659\n",
      "\n",
      "  [Step 25] Top important words:\n",
      "    'plate': 0.1005\n",
      "    'cookie': 0.0772\n",
      "    'pick': 0.0701\n",
      "    'it': 0.0585\n",
      "    'on': 0.0547\n",
      "\n",
      "  [Step 50] Top important words:\n",
      "    'the': 0.0944\n",
      "    'the': 0.0890\n",
      "    'plate': 0.0879\n",
      "    'on': 0.0780\n",
      "    'and': 0.0563\n",
      "\n",
      "  [Step 75] Top important words:\n",
      "    'pick': 0.0967\n",
      "    'plate': 0.0905\n",
      "    'it': 0.0685\n",
      "    'up': 0.0655\n",
      "    'the': 0.0627\n",
      "\n",
      "  [Step 100] Top important words:\n",
      "    'plate': 0.1008\n",
      "    'pick': 0.0998\n",
      "    'the': 0.0744\n",
      "    'up': 0.0698\n",
      "    'cookie': 0.0630\n",
      "\n",
      "  [Step 125] Top important words:\n",
      "    'plate': 0.0974\n",
      "    'on': 0.0769\n",
      "    'the': 0.0701\n",
      "    'and': 0.0630\n",
      "    'pick': 0.0606\n",
      "\n",
      "  [Step 150] Top important words:\n",
      "    'plate': 0.1498\n",
      "    'cookie': 0.0866\n",
      "    'pick': 0.0652\n",
      "    'the': 0.0562\n",
      "    'it': 0.0544\n",
      "\n",
      "  [Step 175] Top important words:\n",
      "    'the': 0.0930\n",
      "    'the': 0.0914\n",
      "    'plate': 0.0820\n",
      "    'on': 0.0740\n",
      "    'and': 0.0610\n",
      "\n",
      "  [Step 200] Top important words:\n",
      "    'the': 0.0915\n",
      "    'the': 0.0901\n",
      "    'plate': 0.0806\n",
      "    'on': 0.0773\n",
      "    'and': 0.0597\n",
      "\n",
      "  [Step 225] Top important words:\n",
      "    'the': 0.0939\n",
      "    'the': 0.0852\n",
      "    'on': 0.0792\n",
      "    'plate': 0.0761\n",
      "    'and': 0.0659\n",
      "\n",
      "  [Step 250] Top important words:\n",
      "    'the': 0.0950\n",
      "    'plate': 0.0864\n",
      "    'the': 0.0860\n",
      "    'on': 0.0786\n",
      "    'and': 0.0620\n",
      "\n",
      "  [Step 275] Top important words:\n",
      "    'plate': 0.0947\n",
      "    'the': 0.0929\n",
      "    'the': 0.0859\n",
      "    'and': 0.0729\n",
      "    'on': 0.0717\n",
      "\n",
      "  [Step 300] Top important words:\n",
      "    'plate': 0.0929\n",
      "    'the': 0.0921\n",
      "    'the': 0.0921\n",
      "    'on': 0.0711\n",
      "    'and': 0.0678\n",
      "\n",
      "  [Step 325] Top important words:\n",
      "    'plate': 0.1002\n",
      "    'the': 0.0904\n",
      "    'the': 0.0866\n",
      "    'on': 0.0723\n",
      "    'and': 0.0702\n",
      "\n",
      "  [Step 350] Top important words:\n",
      "    'plate': 0.0947\n",
      "    'the': 0.0926\n",
      "    'the': 0.0846\n",
      "    'and': 0.0650\n",
      "    'on': 0.0586\n",
      "\n",
      "  [Step 375] Top important words:\n",
      "    'plate': 0.0966\n",
      "    'the': 0.0899\n",
      "    'the': 0.0857\n",
      "    'and': 0.0676\n",
      "    'on': 0.0672\n",
      "\n",
      "  [Step 400] Top important words:\n",
      "    'plate': 0.0934\n",
      "    'the': 0.0808\n",
      "    'on': 0.0767\n",
      "    'the': 0.0712\n",
      "    'and': 0.0657\n",
      "\n",
      "  [Step 425] Top important words:\n",
      "    'plate': 0.0977\n",
      "    'the': 0.0790\n",
      "    'on': 0.0760\n",
      "    'the': 0.0723\n",
      "    'and': 0.0661\n",
      "\n",
      "  [Step 450] Top important words:\n",
      "    'plate': 0.0948\n",
      "    'the': 0.0797\n",
      "    'on': 0.0763\n",
      "    'the': 0.0685\n",
      "    'and': 0.0650\n",
      "\n",
      "  [Step 475] Top important words:\n",
      "    'plate': 0.0956\n",
      "    'the': 0.0789\n",
      "    'on': 0.0751\n",
      "    'the': 0.0675\n",
      "    'and': 0.0657\n",
      "\n",
      "  [Step 500] Top important words:\n",
      "    'plate': 0.0971\n",
      "    'the': 0.0769\n",
      "    'on': 0.0764\n",
      "    'box': 0.0636\n",
      "    'cookie': 0.0605\n",
      "\n",
      "  [Step 525] Top important words:\n",
      "    'plate': 0.0945\n",
      "    'on': 0.0773\n",
      "    'the': 0.0767\n",
      "    'box': 0.0611\n",
      "    'cookie': 0.0608\n",
      "\n",
      "  [Step 550] Top important words:\n",
      "    'plate': 0.0925\n",
      "    'the': 0.0784\n",
      "    'on': 0.0743\n",
      "    'the': 0.0627\n",
      "    'box': 0.0612\n",
      "\n",
      "  [Step 575] Top important words:\n",
      "    'plate': 0.0931\n",
      "    'the': 0.0791\n",
      "    'on': 0.0743\n",
      "    'and': 0.0630\n",
      "    'the': 0.0616\n",
      "Saved videos to evaluation_videos/task_03.\n",
      "\n",
      "ðŸ“Š Final Results on task 3: 4/5 successes\n",
      "âœ“ Explainability data saved to evaluation_videos/task_03_explainability.json\n",
      "  Analyzed 120 frames\n",
      "\n",
      "  ðŸ“Š Most consistently important words across all frames:\n",
      "    1. 'plate': avg=0.0925, appeared in top-5 120 times\n",
      "    2. 'pick': avg=0.0822, appeared in top-5 24 times\n",
      "    3. 'the': avg=0.0816, appeared in top-5 197 times\n",
      "    4. 'on': avg=0.0737, appeared in top-5 99 times\n",
      "    5. 'up': avg=0.0677, appeared in top-5 14 times\n",
      "    6. 'and': avg=0.0661, appeared in top-5 84 times\n",
      "    7. 'cookie': avg=0.0658, appeared in top-5 31 times\n",
      "    8. 'it': avg=0.0646, appeared in top-5 10 times\n",
      "    9. 'box': avg=0.0624, appeared in top-5 20 times\n",
      "    10. 'place': avg=0.0566, appeared in top-5 1 times\n",
      "âœ“ Videos saved to evaluation_videos/task_03.mp4\n",
      "Starting evaluation on libero_spatial Task 4 (Envs: 5, Max Steps: 600)...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Using language prompt: 'pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate'\n",
      "âœ… Explainability enabled (interval: every 5 steps, top-5 tokens)\n",
      "\n",
      "  [Step 0] Top important words:\n",
      "    'plate': 0.1382\n",
      "    'drawer': 0.0848\n",
      "    'cabinet': 0.0792\n",
      "    'wooden': 0.0714\n",
      "    'on': 0.0583\n",
      "\n",
      "  [Step 25] Top important words:\n",
      "    'plate': 0.1652\n",
      "    'cabinet': 0.1014\n",
      "    'wooden': 0.0844\n",
      "    'the': 0.0810\n",
      "    'drawer': 0.0660\n",
      "\n",
      "  [Step 50] Top important words:\n",
      "    'cabinet': 0.1261\n",
      "    'wooden': 0.1184\n",
      "    'plate': 0.1131\n",
      "    'drawer': 0.0770\n",
      "    'top': 0.0513\n",
      "\n",
      "  [Step 75] Top important words:\n",
      "    'plate': 0.1120\n",
      "    'drawer': 0.0831\n",
      "    'wooden': 0.0829\n",
      "    'cabinet': 0.0740\n",
      "    'the': 0.0611\n",
      "\n",
      "  [Step 100] Top important words:\n",
      "    'plate': 0.1369\n",
      "    'cabinet': 0.0967\n",
      "    'wooden': 0.0821\n",
      "    'drawer': 0.0819\n",
      "    'the': 0.0562\n",
      "\n",
      "  [Step 125] Top important words:\n",
      "    'plate': 0.1024\n",
      "    'cabinet': 0.0982\n",
      "    'wooden': 0.0760\n",
      "    'drawer': 0.0703\n",
      "    'it': 0.0551\n",
      "\n",
      "  [Step 150] Top important words:\n",
      "    'wooden': 0.1220\n",
      "    'plate': 0.0806\n",
      "    'the': 0.0765\n",
      "    'cabinet': 0.0756\n",
      "    'drawer': 0.0524\n",
      "\n",
      "  [Step 175] Top important words:\n",
      "    'plate': 0.0942\n",
      "    'cabinet': 0.0904\n",
      "    'wooden': 0.0790\n",
      "    'drawer': 0.0682\n",
      "    'it': 0.0587\n",
      "\n",
      "  [Step 200] Top important words:\n",
      "    'plate': 0.1396\n",
      "    'wooden': 0.1254\n",
      "    'the': 0.0873\n",
      "    'drawer': 0.0818\n",
      "    'cabinet': 0.0763\n",
      "\n",
      "  [Step 225] Top important words:\n",
      "    'plate': 0.1833\n",
      "    'drawer': 0.1318\n",
      "    'top': 0.0703\n",
      "    'the': 0.0573\n",
      "    'wooden': 0.0563\n",
      "\n",
      "  [Step 250] Top important words:\n",
      "    'plate': 0.1168\n",
      "    'cabinet': 0.0958\n",
      "    'wooden': 0.0906\n",
      "    'drawer': 0.0706\n",
      "    'it': 0.0509\n",
      "\n",
      "  [Step 275] Top important words:\n",
      "    'plate': 0.1731\n",
      "    'drawer': 0.1007\n",
      "    'cabinet': 0.0613\n",
      "    'wooden': 0.0560\n",
      "    'the': 0.0547\n",
      "\n",
      "  [Step 300] Top important words:\n",
      "    'plate': 0.0873\n",
      "    'drawer': 0.0785\n",
      "    'the': 0.0677\n",
      "    'cabinet': 0.0655\n",
      "    'the': 0.0620\n",
      "\n",
      "  [Step 325] Top important words:\n",
      "    'plate': 0.1099\n",
      "    'drawer': 0.0826\n",
      "    'cabinet': 0.0610\n",
      "    'wooden': 0.0606\n",
      "    'the': 0.0600\n",
      "\n",
      "  [Step 350] Top important words:\n",
      "    'plate': 0.1159\n",
      "    'drawer': 0.0858\n",
      "    'cabinet': 0.0589\n",
      "    'the': 0.0586\n",
      "    'wooden': 0.0584\n",
      "\n",
      "  [Step 375] Top important words:\n",
      "    'plate': 0.1567\n",
      "    'drawer': 0.0960\n",
      "    'cabinet': 0.0640\n",
      "    'wooden': 0.0557\n",
      "    'top': 0.0541\n",
      "\n",
      "  [Step 400] Top important words:\n",
      "    'plate': 0.1479\n",
      "    'drawer': 0.0966\n",
      "    'cabinet': 0.0674\n",
      "    'the': 0.0577\n",
      "    'top': 0.0551\n",
      "\n",
      "  [Step 425] Top important words:\n",
      "    'plate': 0.1124\n",
      "    'drawer': 0.0906\n",
      "    'the': 0.0675\n",
      "    'the': 0.0581\n",
      "    'wooden': 0.0572\n",
      "\n",
      "  [Step 450] Top important words:\n",
      "    'plate': 0.2168\n",
      "    'drawer': 0.0921\n",
      "    'cabinet': 0.0728\n",
      "    'top': 0.0543\n",
      "    'the': 0.0519\n",
      "\n",
      "  [Step 475] Top important words:\n",
      "    'plate': 0.1503\n",
      "    'drawer': 0.0982\n",
      "    'cabinet': 0.0568\n",
      "    'the': 0.0564\n",
      "    'wooden': 0.0557\n",
      "\n",
      "  [Step 500] Top important words:\n",
      "    'plate': 0.1282\n",
      "    'drawer': 0.1077\n",
      "    'the': 0.0575\n",
      "    'wooden': 0.0575\n",
      "    'top': 0.0571\n",
      "\n",
      "  [Step 525] Top important words:\n",
      "    'plate': 0.1056\n",
      "    'drawer': 0.0768\n",
      "    'cabinet': 0.0729\n",
      "    'wooden': 0.0683\n",
      "    'the': 0.0562\n",
      "\n",
      "  [Step 550] Top important words:\n",
      "    'plate': 0.1256\n",
      "    'drawer': 0.1018\n",
      "    'the': 0.0607\n",
      "    'cabinet': 0.0562\n",
      "    'top': 0.0561\n",
      "\n",
      "  [Step 575] Top important words:\n",
      "    'plate': 0.1359\n",
      "    'drawer': 0.0949\n",
      "    'the': 0.0604\n",
      "    'cabinet': 0.0579\n",
      "    'wooden': 0.0558\n",
      "Saved videos to evaluation_videos/task_04.\n",
      "\n",
      "ðŸ“Š Final Results on task 4: 1/5 successes\n",
      "âœ“ Explainability data saved to evaluation_videos/task_04_explainability.json\n",
      "  Analyzed 120 frames\n",
      "\n",
      "  ðŸ“Š Most consistently important words across all frames:\n",
      "    1. 'plate': avg=0.1329, appeared in top-5 120 times\n",
      "    2. 'drawer': avg=0.0877, appeared in top-5 119 times\n",
      "    3. 'cabinet': avg=0.0783, appeared in top-5 114 times\n",
      "    4. 'wooden': avg=0.0737, appeared in top-5 104 times\n",
      "    5. 'on': avg=0.0638, appeared in top-5 9 times\n",
      "    6. 'the': avg=0.0625, appeared in top-5 93 times\n",
      "    7. 'top': avg=0.0570, appeared in top-5 34 times\n",
      "    8. 'it': avg=0.0533, appeared in top-5 6 times\n",
      "    9. 'and': avg=0.0519, appeared in top-5 1 times\n",
      "âœ“ Videos saved to evaluation_videos/task_04.mp4\n",
      "Starting evaluation on libero_spatial Task 5 (Envs: 5, Max Steps: 600)...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Using language prompt: 'pick up the black bowl on the ramekin and place it on the plate'\n",
      "âœ… Explainability enabled (interval: every 5 steps, top-5 tokens)\n",
      "\n",
      "  [Step 0] Top important words:\n",
      "    'ram': 0.0818\n",
      "    'the': 0.0792\n",
      "    'plate': 0.0756\n",
      "    'pick': 0.0673\n",
      "    'the': 0.0594\n",
      "\n",
      "  [Step 25] Top important words:\n",
      "    'plate': 0.1083\n",
      "    'ram': 0.0813\n",
      "    'and': 0.0618\n",
      "    'the': 0.0618\n",
      "    'it': 0.0546\n",
      "\n",
      "  [Step 50] Top important words:\n",
      "    'plate': 0.0850\n",
      "    'ram': 0.0742\n",
      "    'kin': 0.0707\n",
      "    'on': 0.0653\n",
      "    'pick': 0.0640\n",
      "\n",
      "  [Step 75] Top important words:\n",
      "    'the': 0.0939\n",
      "    'plate': 0.0874\n",
      "    'the': 0.0785\n",
      "    'and': 0.0711\n",
      "    'on': 0.0591\n",
      "\n",
      "  [Step 100] Top important words:\n",
      "    'and': 0.1009\n",
      "    'plate': 0.0817\n",
      "    'ram': 0.0650\n",
      "    'the': 0.0642\n",
      "    'the': 0.0625\n",
      "\n",
      "  [Step 125] Top important words:\n",
      "    'plate': 0.1183\n",
      "    'ram': 0.1042\n",
      "    'the': 0.0782\n",
      "    'on': 0.0763\n",
      "    'kin': 0.0670\n",
      "\n",
      "  [Step 150] Top important words:\n",
      "    'ram': 0.1121\n",
      "    'plate': 0.1042\n",
      "    'on': 0.0700\n",
      "    'kin': 0.0665\n",
      "    'and': 0.0622\n",
      "\n",
      "  [Step 175] Top important words:\n",
      "    'ram': 0.1274\n",
      "    'plate': 0.1031\n",
      "    'kin': 0.0746\n",
      "    'on': 0.0688\n",
      "    'the': 0.0601\n",
      "\n",
      "  [Step 200] Top important words:\n",
      "    'ram': 0.1297\n",
      "    'plate': 0.1202\n",
      "    'kin': 0.0811\n",
      "    'on': 0.0679\n",
      "    'the': 0.0663\n",
      "\n",
      "  [Step 225] Top important words:\n",
      "    'plate': 0.1198\n",
      "    'ram': 0.1051\n",
      "    'on': 0.0757\n",
      "    'and': 0.0641\n",
      "    'kin': 0.0623\n",
      "\n",
      "  [Step 250] Top important words:\n",
      "    'plate': 0.1395\n",
      "    'ram': 0.1202\n",
      "    'kin': 0.0629\n",
      "    'and': 0.0628\n",
      "    'on': 0.0575\n",
      "\n",
      "  [Step 275] Top important words:\n",
      "    'plate': 0.1380\n",
      "    'and': 0.0898\n",
      "    'ram': 0.0884\n",
      "    'kin': 0.0670\n",
      "    'pick': 0.0622\n",
      "Saved videos to evaluation_videos/task_05.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m final_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#id = 8\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     final_results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43menable_explainability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Abilita explainability\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexplainability_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Analizza ogni 5 step\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Salva top 5 token\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m mean_success \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(final_results)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean success rate over all tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_success\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 132\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(checkpoint_path, action_stats_path, benchmark, task_id, env_num, max_steps, seed, save_videos, video_dir, camera_height, camera_width, video_skip, enable_explainability, explainability_interval, top_k_tokens)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš ï¸ Explainability error at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m obs, reward, done_batch, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m alive:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[67], line 26\u001b[0m, in \u001b[0;36mSequentialVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m---> 26\u001b[0m     results \u001b[38;5;241m=\u001b[39m [env\u001b[38;5;241m.\u001b[39mstep(a) \u001b[38;5;28;01mfor\u001b[39;00m env, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, actions)]\n\u001b[1;32m     27\u001b[0m     obs_list, rews_list, dones_list, infos_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(obs_list), np\u001b[38;5;241m.\u001b[39marray(rews_list), np\u001b[38;5;241m.\u001b[39marray(dones_list), \u001b[38;5;28mlist\u001b[39m(infos_list)\n",
      "Cell \u001b[0;32mIn[67], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions):\n\u001b[0;32m---> 26\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs, actions)]\n\u001b[1;32m     27\u001b[0m     obs_list, rews_list, dones_list, infos_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(obs_list), np\u001b[38;5;241m.\u001b[39marray(rews_list), np\u001b[38;5;241m.\u001b[39marray(dones_list), \u001b[38;5;28mlist\u001b[39m(infos_list)\n",
      "File \u001b[0;32m~/TinyRecursiveModels-for-Robotics/LIBERO/libero/libero/envs/env_wrapper.py:88\u001b[0m, in \u001b[0;36mControlEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TinyRecursiveModels-for-Robotics/LIBERO/libero/libero/envs/bddl_base_domain.py:806\u001b[0m, in \u001b[0;36mBDDLBaseDomain.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    803\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action)\n\u001b[1;32m    804\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((action[:\u001b[38;5;241m3\u001b[39m], action[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 806\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_success()\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, done, info\n",
      "File \u001b[0;32m~/TinyRecursiveModels-for-Robotics/.venv/lib/python3.10/site-packages/robosuite/environments/base.py:391\u001b[0m, in \u001b[0;36mMujocoEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mforward()\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_action(action, policy_step)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_observables()\n\u001b[1;32m    393\u001b[0m policy_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/TinyRecursiveModels-for-Robotics/.venv/lib/python3.10/site-packages/robosuite/utils/binding_utils.py:1092\u001b[0m, in \u001b[0;36mMjSim.step\u001b[0;34m(self, with_udd)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, with_udd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Step simulation.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1092\u001b[0m     \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "for id in range(10):\n",
    "    #id = 8\n",
    "    final_results.append(\n",
    "        evaluate_model(\n",
    "            task_id=id, \n",
    "            env_num=5, \n",
    "            max_steps=600, \n",
    "            save_videos=True,\n",
    "            enable_explainability=True,  # Abilita explainability\n",
    "            explainability_interval=5,   # Analizza ogni 5 step\n",
    "            top_k_tokens=5,             # Salva top 5 token\n",
    "        )[\"success_rate\"]\n",
    "    )\n",
    "\n",
    "mean_success = np.mean(final_results)\n",
    "print(f\"Mean success rate over all tasks: {mean_success:.2%}\")\n",
    "\n",
    "print(\"All success rates:\", final_results)\n",
    "\n",
    "top_3 = sorted(range(len(final_results)), key=lambda i: final_results[i], reverse=True)[:3]\n",
    "print(\"Top 3 tasks by success rate:\", top_3)\n",
    "\n",
    "worst_3 = sorted(range(len(final_results)), key=lambda i: final_results[i])[:3]\n",
    "print(\"Worst 3 tasks by success rate:\", worst_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizzazione Explainability Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPLAINABILITY RESULTS: Task 0\n",
      "================================================================================\n",
      "Task Prompt: 'pick up the black bowl between the plate and the ramekin and place it on the plate'\n",
      "Success Rate: 40.00%\n",
      "Frames Analyzed: 120\n",
      "================================================================================\n",
      "\n",
      "Top 5 analyzed frames:\n",
      "\n",
      "\n",
      "Frame #1 (Step 0):\n",
      "  Most Important Words:\n",
      "     1. 'plate          ' 0.1070 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     2. 'ram            ' 0.0831 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     3. 'between        ' 0.0802 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     4. 'it             ' 0.0584 â–ˆâ–ˆ\n",
      "     5. 'the            ' 0.0526 â–ˆâ–ˆ\n",
      "\n",
      "Frame #2 (Step 5):\n",
      "  Most Important Words:\n",
      "     1. 'ram            ' 0.1135 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     2. 'plate          ' 0.1055 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     3. 'between        ' 0.0894 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     4. 'e              ' 0.0684 â–ˆâ–ˆâ–ˆ\n",
      "     5. 'and            ' 0.0566 â–ˆâ–ˆ\n",
      "\n",
      "Frame #3 (Step 10):\n",
      "  Most Important Words:\n",
      "     1. 'between        ' 0.0983 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     2. 'ram            ' 0.0969 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     3. 'plate          ' 0.0798 â–ˆâ–ˆâ–ˆ\n",
      "     4. 'e              ' 0.0743 â–ˆâ–ˆâ–ˆ\n",
      "     5. 'and            ' 0.0634 â–ˆâ–ˆâ–ˆ\n",
      "\n",
      "Frame #4 (Step 15):\n",
      "  Most Important Words:\n",
      "     1. 'plate          ' 0.1425 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     2. 'between        ' 0.0826 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     3. 'ram            ' 0.0662 â–ˆâ–ˆâ–ˆ\n",
      "     4. 'e              ' 0.0619 â–ˆâ–ˆâ–ˆ\n",
      "     5. 'on             ' 0.0533 â–ˆâ–ˆ\n",
      "\n",
      "Frame #5 (Step 20):\n",
      "  Most Important Words:\n",
      "     1. 'ram            ' 0.1534 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     2. 'plate          ' 0.1204 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     3. 'between        ' 0.0905 â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "     4. 'kin            ' 0.0685 â–ˆâ–ˆâ–ˆ\n",
      "     5. 'and            ' 0.0550 â–ˆâ–ˆ\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Most Frequent Important Words:\n",
      "   1. 'plate          ': appeared 120 times (100.0%), avg score=0.1057\n",
      "   2. 'between        ': appeared 117 times ( 97.5%), avg score=0.0820\n",
      "   3. 'ram            ': appeared 116 times ( 96.7%), avg score=0.0823\n",
      "   4. 'the            ': appeared 100 times ( 83.3%), avg score=0.0733\n",
      "   5. 'kin            ': appeared  70 times ( 58.3%), avg score=0.0704\n",
      "   6. 'and            ': appeared  47 times ( 39.2%), avg score=0.0645\n",
      "   7. 'e              ': appeared  21 times ( 17.5%), avg score=0.0642\n",
      "   8. 'it             ': appeared   6 times (  5.0%), avg score=0.0582\n",
      "   9. 'on             ': appeared   2 times (  1.7%), avg score=0.0528\n",
      "  10. 'place          ': appeared   1 times (  0.8%), avg score=0.0521\n",
      "\n",
      "Highest Average Importance Scores:\n",
      "   1. 'plate          ': avg score=0.1057, appeared 120 times\n",
      "   2. 'ram            ': avg score=0.0823, appeared 116 times\n",
      "   3. 'between        ': avg score=0.0820, appeared 117 times\n",
      "   4. 'the            ': avg score=0.0733, appeared 100 times\n",
      "   5. 'kin            ': avg score=0.0704, appeared  70 times\n",
      "   6. 'and            ': avg score=0.0645, appeared  47 times\n",
      "   7. 'e              ': avg score=0.0642, appeared  21 times\n",
      "   8. 'it             ': avg score=0.0582, appeared   6 times\n",
      "   9. 'on             ': avg score=0.0528, appeared   2 times\n",
      "  10. 'place          ': avg score=0.0521, appeared   1 times\n"
     ]
    }
   ],
   "source": [
    "# Esempio: Visualizza risultati per un singolo task\n",
    "# Prima esegui l'evaluation con enable_explainability=True per generare i file JSON\n",
    "\n",
    "explainability_file = 'evaluation_videos/task_00_explainability.json'\n",
    "if Path(explainability_file).exists():\n",
    "    visualize_explainability_results(\n",
    "        explainability_file,\n",
    "        top_n_frames=5,\n",
    "        top_n_tokens=10\n",
    "    )\n",
    "else:\n",
    "    print(f\"âš ï¸ File {explainability_file} non trovato.\")\n",
    "    print(\"Esegui prima evaluate_model con enable_explainability=True per generare i dati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CROSS-TASK EXPLAINABILITY ANALYSIS\n",
      "================================================================================\n",
      "Analyzing 10 tasks\n",
      "\n",
      "Most Important Words Across All Tasks:\n",
      "Rank   Token                Avg Score    Std        Appearances   Tasks   \n",
      "--------------------------------------------------------------------------------\n",
      "1      'plate             ' 0.1006       0.0218     1111          10      \n",
      "2      'ram               ' 0.0953       0.0284     343           3       \n",
      "3      'center            ' 0.0883       0.0150     103           1       \n",
      "4      'drawer            ' 0.0877       0.0150     119           1       \n",
      "5      'between           ' 0.0820       0.0162     117           1       \n",
      "6      'cabinet           ' 0.0790       0.0156     231           2       \n",
      "7      'stove             ' 0.0756       0.0060     115           1       \n",
      "8      'pick              ' 0.0732       0.0104     432           8       \n",
      "9      'wooden            ' 0.0707       0.0168     149           2       \n",
      "10     'the               ' 0.0706       0.0113     975           10      \n",
      "11     'table             ' 0.0703       0.0071     70            1       \n",
      "12     'on                ' 0.0690       0.0078     549           10      \n",
      "13     'kin               ' 0.0689       0.0090     204           3       \n",
      "14     'and               ' 0.0675       0.0094     414           10      \n",
      "15     'up                ' 0.0666       0.0064     212           8       \n",
      "16     'cookie            ' 0.0665       0.0084     52            2       \n",
      "17     'box               ' 0.0622       0.0017     22            2       \n",
      "18     'it                ' 0.0621       0.0053     178           10      \n",
      "19     'next              ' 0.0599       0.0025     6             1       \n",
      "20     'e                 ' 0.0598       0.0058     70            3       \n",
      "\n",
      "================================================================================\n",
      "Task Success Rates:\n",
      "================================================================================\n",
      " 1. Task  6 (100.0%): pick up the black bowl next to the cookie box and place it on the plate\n",
      " 2. Task  2 ( 80.0%): pick up the black bowl from table center and place it on the plate\n",
      " 3. Task  3 ( 80.0%): pick up the black bowl on the cookie box and place it on the plate\n",
      " 4. Task  1 ( 60.0%): pick up the black bowl next to the ramekin and place it on the plate\n",
      " 5. Task  9 ( 60.0%): pick up the black bowl on the wooden cabinet and place it on the plate\n",
      " 6. Task  7 ( 60.0%): pick up the black bowl on the stove and place it on the plate\n",
      " 7. Task  0 ( 40.0%): pick up the black bowl between the plate and the ramekin and place it on the plate\n",
      " 8. Task  8 ( 40.0%): pick up the black bowl next to the plate and place it on the plate\n",
      " 9. Task  4 ( 20.0%): pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate\n",
      "10. Task  5 ( 20.0%): pick up the black bowl on the ramekin and place it on the plate\n"
     ]
    }
   ],
   "source": [
    "# Analisi cross-task\n",
    "# Verifica che esistano file di explainability\n",
    "if list(Path('evaluation_videos').glob('task_*_explainability.json')):\n",
    "    token_stats, task_summaries = analyze_explainability_across_tasks(\n",
    "        video_dir='evaluation_videos'\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Nessun file di explainability trovato in evaluation_videos/\")\n",
    "    print(\"Esegui prima evaluate_model con enable_explainability=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "STUDIO APPROFONDITO: CORRELAZIONE TRA IMPORTANZA DELLE PAROLE E SUCCESSO/FALLIMENTO\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“Š Dataset Overview:\n",
      "   â€¢ Task totali: 10\n",
      "   â€¢ Task riusciti (>50%): 6\n",
      "   â€¢ Task falliti (â‰¤50%): 4\n",
      "   â€¢ Success rate medio: 56.0%\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "ANALISI 1: TOKEN DISCRIMINATIVI (Successo vs Fallimento)\n",
      "====================================================================================================\n",
      "\n",
      "Top 15 token piÃ¹ discriminativi (differenza di importanza tra successo e fallimento):\n",
      "\n",
      "Rank  Token                Success      Failure      Diff         Verdict        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     'ram               ' 0.1091       0.0881       +0.0210      âœ“ Pro-Success  \n",
      "2     'plate             ' 0.0928       0.1097       -0.0170      âœ— Pro-Failure  \n",
      "3     'wooden            ' 0.0640       0.0737       -0.0097      âœ— Pro-Failure  \n",
      "4     'e                 ' 0.0578       0.0625       -0.0047      âœ— Pro-Failure  \n",
      "5     'on                ' 0.0681       0.0636       +0.0045      âœ“ Pro-Success  \n",
      "6     'it                ' 0.0632       0.0588       +0.0044      âœ“ Pro-Success  \n",
      "7     'pick              ' 0.0725       0.0686       +0.0039      âœ“ Pro-Success  \n",
      "8     'the               ' 0.0708       0.0670       +0.0038      âœ“ Pro-Success  \n",
      "9     'and               ' 0.0666       0.0632       +0.0034      âœ“ Pro-Success  \n",
      "10    'place             ' 0.0570       0.0549       +0.0020      âœ“ Pro-Success  \n",
      "11    'cabinet           ' 0.0796       0.0783       +0.0013      âœ“ Pro-Success  \n",
      "12    'kin               ' 0.0686       0.0689       -0.0003      âœ— Pro-Failure  \n",
      "13    'up                ' 0.0656       0.0652       +0.0003      âœ“ Pro-Success  \n",
      "\n",
      "Interpretazione:\n",
      "  â€¢ Token con differenza positiva â†’ piÃ¹ importanti nei task RIUSCITI\n",
      "  â€¢ Token con differenza negativa â†’ piÃ¹ importanti nei task FALLITI\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "ANALISI 2: EVOLUZIONE TEMPORALE DELL'ATTENZIONE\n",
      "====================================================================================================\n",
      "\n",
      "Token con pattern temporale piÃ¹ marcato:\n",
      "\n",
      "Rank  Token                Early      Mid        Late       Trend        Pattern        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     'ram               ' 0.0965     0.0960     0.0919     -0.0047     Stabile        \n",
      "2     'the               ' 0.0706     0.0685     0.0660     -0.0046     Stabile        \n",
      "3     'place             ' 0.0550     0.0633     0.0594     +0.0044     Stabile        \n",
      "4     'plate             ' 0.0974     0.0997     0.1016     +0.0042     Stabile        \n",
      "5     'pick              ' 0.0716     0.0712     0.0682     -0.0034     Stabile        \n",
      "6     'e                 ' 0.0607     0.0602     0.0573     -0.0033     Stabile        \n",
      "7     'kin               ' 0.0701     0.0710     0.0674     -0.0027     Stabile        \n",
      "8     'up                ' 0.0666     0.0677     0.0650     -0.0016     Stabile        \n",
      "9     'and               ' 0.0670     0.0636     0.0660     -0.0011     Stabile        \n",
      "10    'it                ' 0.0620     0.0614     0.0613     -0.0007     Stabile        \n",
      "11    'on                ' 0.0667     0.0671     0.0671     +0.0004     Stabile        \n",
      "\n",
      "Interpretazione:\n",
      "  â€¢ Pattern Crescente â†’ attenzione aumenta nel tempo (esecuzione finale)\n",
      "  â€¢ Pattern Decrescente â†’ attenzione diminuisce (pianificazione iniziale)\n",
      "  â€¢ Pattern Stabile â†’ importanza costante durante tutto il task\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "ANALISI 3: CORRELAZIONE QUANTITATIVA (Token Importance vs Success Rate)\n",
      "====================================================================================================\n",
      "\n",
      "Token con correlazione piÃ¹ forte (positiva = predice successo, negativa = predice fallimento):\n",
      "\n",
      "Rank  Token                Correlation   Avg Importance   Tasks    Verdict             \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     'plate             ' -0.7382       0.0996           10       âœ—âœ— Forte predictor di FALLIMENTO\n",
      "2     'ram               ' +0.5697       0.0951           3        âœ“âœ“ Forte predictor di SUCCESSO\n",
      "3     'it                ' +0.5277       0.0614           10       âœ“âœ“ Forte predictor di SUCCESSO\n",
      "4     'the               ' +0.4820       0.0693           10       âœ“âœ“ Forte predictor di SUCCESSO\n",
      "5     'e                 ' -0.4806       0.0610           3        âœ—âœ— Forte predictor di FALLIMENTO\n",
      "6     'pick              ' +0.4732       0.0715           8        âœ“âœ“ Forte predictor di SUCCESSO\n",
      "7     'kin               ' +0.3905       0.0688           3        âœ“âœ“ Forte predictor di SUCCESSO\n",
      "8     'on                ' +0.2923       0.0663           10       âœ“ Lieve pro-success \n",
      "9     'place             ' +0.2736       0.0563           6        âœ“ Lieve pro-success \n",
      "10    'and               ' +0.1489       0.0652           10       âœ“ Lieve pro-success \n",
      "11    'up                ' +0.0743       0.0655           8        âœ“ Lieve pro-success \n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "ANALISI 4: STABILITÃ€ DELL'ATTENZIONE (Varianza)\n",
      "====================================================================================================\n",
      "\n",
      "StabilitÃ  media dell'attenzione (coefficiente di variazione):\n",
      "  â€¢ Task riusciti:  0.1060 Â± 0.0444\n",
      "  â€¢ Task falliti:   0.1388 Â± 0.0653\n",
      "\n",
      "  â†’ Task riusciti hanno attenzione PIÃ™ STABILE\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "RIEPILOGO CONCLUSIVO\n",
      "====================================================================================================\n",
      "\n",
      "Performance per task:\n",
      "\n",
      "  Task  6 (100.0%) âœ“ SUCCESSO: pick up the black bowl next to the cookie box and place it on the plat...\n",
      "           Top token: 'plate' (0.088), 'the' (0.072), 'pick' (0.071)\n",
      "\n",
      "  Task  2 ( 80.0%) âœ“ SUCCESSO: pick up the black bowl from table center and place it on the plate...\n",
      "           Top token: 'plate' (0.093), 'center' (0.088), 'pick' (0.080)\n",
      "\n",
      "  Task  3 ( 80.0%) âœ“ SUCCESSO: pick up the black bowl on the cookie box and place it on the plate...\n",
      "           Top token: 'plate' (0.093), 'pick' (0.082), 'the' (0.082)\n",
      "\n",
      "  Task  1 ( 60.0%) âœ“ SUCCESSO: pick up the black bowl next to the ramekin and place it on the plate...\n",
      "           Top token: 'ram' (0.109), 'plate' (0.090), 'kin' (0.069)\n",
      "\n",
      "  Task  7 ( 60.0%) âœ“ SUCCESSO: pick up the black bowl on the stove and place it on the plate...\n",
      "           Top token: 'plate' (0.099), 'stove' (0.076), 'the' (0.073)\n",
      "\n",
      "  Task  9 ( 60.0%) âœ“ SUCCESSO: pick up the black bowl on the wooden cabinet and place it on the plate...\n",
      "           Top token: 'plate' (0.094), 'cabinet' (0.080), 'and' (0.074)\n",
      "\n",
      "  Task  0 ( 40.0%) âœ— FALLIMENTO: pick up the black bowl between the plate and the ramekin and place it ...\n",
      "           Top token: 'plate' (0.106), 'ram' (0.082), 'between' (0.082)\n",
      "\n",
      "  Task  8 ( 40.0%) âœ— FALLIMENTO: pick up the black bowl next to the plate and place it on the plate...\n",
      "           Top token: 'plate' (0.099), 'pick' (0.072), 'on' (0.069)\n",
      "\n",
      "  Task  4 ( 20.0%) âœ— FALLIMENTO: pick up the black bowl in the top drawer of the wooden cabinet and pla...\n",
      "           Top token: 'plate' (0.133), 'drawer' (0.088), 'cabinet' (0.078)\n",
      "\n",
      "  Task  5 ( 20.0%) âœ— FALLIMENTO: pick up the black bowl on the ramekin and place it on the plate...\n",
      "           Top token: 'plate' (0.101), 'ram' (0.094), 'and' (0.072)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Studio approfondito: correlazione tra importanza delle parole e successo/fallimento\n",
    "\n",
    "def deep_explainability_analysis(video_dir: str = 'evaluation_videos'):\n",
    "    \"\"\"\n",
    "    Analisi approfondita della correlazione tra importanza dei token e successo nei task.\n",
    "    \n",
    "    Analizza:\n",
    "    1. Differenze di importanza tra task riusciti vs falliti\n",
    "    2. Evoluzione temporale dell'attenzione (early vs late frames)\n",
    "    3. Token discriminativi (piÃ¹ importanti per successo vs fallimento)\n",
    "    4. Pattern di stabilitÃ  dell'attenzione\n",
    "    \"\"\"\n",
    "    video_path = Path(video_dir)\n",
    "    json_files = sorted(video_path.glob('task_*_explainability.json'))\n",
    "    \n",
    "    if len(json_files) < 2:\n",
    "        print(\"âš ï¸ Serve almeno 2 task per l'analisi di correlazione\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"STUDIO APPROFONDITO: CORRELAZIONE TRA IMPORTANZA DELLE PAROLE E SUCCESSO/FALLIMENTO\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Raccogli dati da tutti i task\n",
    "    task_data = []\n",
    "    all_tokens = set()\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        task_id = data['metadata']['task_id']\n",
    "        task_prompt = data['metadata']['task_prompt']\n",
    "        success_rate = data['metadata']['success_rate']\n",
    "        frames = data['frames']\n",
    "        \n",
    "        # Analisi temporale: dividi in fasi\n",
    "        n_frames = len(frames)\n",
    "        early_frames = frames[:n_frames//3]\n",
    "        mid_frames = frames[n_frames//3:2*n_frames//3]\n",
    "        late_frames = frames[2*n_frames//3:]\n",
    "        \n",
    "        def get_token_importance(frame_list):\n",
    "            token_scores = {}\n",
    "            for frame in frame_list:\n",
    "                for token_data in frame['top_tokens'][:5]:\n",
    "                    token = token_data['token']\n",
    "                    score = token_data['score']\n",
    "                    all_tokens.add(token)\n",
    "                    if token in token_scores:\n",
    "                        token_scores[token].append(score)\n",
    "                    else:\n",
    "                        token_scores[token] = [score]\n",
    "            # Calcola media e varianza\n",
    "            return {\n",
    "                token: {\n",
    "                    'mean': np.mean(scores),\n",
    "                    'std': np.std(scores),\n",
    "                    'count': len(scores)\n",
    "                }\n",
    "                for token, scores in token_scores.items()\n",
    "            }\n",
    "        \n",
    "        task_data.append({\n",
    "            'task_id': task_id,\n",
    "            'prompt': task_prompt,\n",
    "            'success_rate': success_rate,\n",
    "            'early_importance': get_token_importance(early_frames),\n",
    "            'mid_importance': get_token_importance(mid_frames),\n",
    "            'late_importance': get_token_importance(late_frames),\n",
    "            'overall_importance': get_token_importance(frames),\n",
    "            'num_frames': n_frames\n",
    "        })\n",
    "    \n",
    "    # Separa task in successi (>50%) e fallimenti (<=50%)\n",
    "    successful_tasks = [t for t in task_data if t['success_rate'] > 0.5]\n",
    "    failed_tasks = [t for t in task_data if t['success_rate'] <= 0.5]\n",
    "    \n",
    "    print(f\"ðŸ“Š Dataset Overview:\")\n",
    "    print(f\"   â€¢ Task totali: {len(task_data)}\")\n",
    "    print(f\"   â€¢ Task riusciti (>50%): {len(successful_tasks)}\")\n",
    "    print(f\"   â€¢ Task falliti (â‰¤50%): {len(failed_tasks)}\")\n",
    "    print(f\"   â€¢ Success rate medio: {np.mean([t['success_rate'] for t in task_data]):.1%}\\n\")\n",
    "    \n",
    "    # ANALISI 1: Token discriminativi tra successo e fallimento\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"ANALISI 1: TOKEN DISCRIMINATIVI (Successo vs Fallimento)\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    def aggregate_token_importance(task_list):\n",
    "        \"\"\"Aggrega importanza dei token da una lista di task\"\"\"\n",
    "        agg = {}\n",
    "        for task in task_list:\n",
    "            for token, stats in task['overall_importance'].items():\n",
    "                if token in agg:\n",
    "                    agg[token]['scores'].append(stats['mean'])\n",
    "                    agg[token]['counts'].append(stats['count'])\n",
    "                else:\n",
    "                    agg[token] = {\n",
    "                        'scores': [stats['mean']],\n",
    "                        'counts': [stats['count']]\n",
    "                    }\n",
    "        return {\n",
    "            token: {\n",
    "                'mean': np.mean(data['scores']),\n",
    "                'std': np.std(data['scores']),\n",
    "                'total_count': sum(data['counts']),\n",
    "                'num_tasks': len(data['scores'])\n",
    "            }\n",
    "            for token, data in agg.items()\n",
    "        }\n",
    "    \n",
    "    if successful_tasks and failed_tasks:\n",
    "        success_tokens = aggregate_token_importance(successful_tasks)\n",
    "        fail_tokens = aggregate_token_importance(failed_tasks)\n",
    "        \n",
    "        # Trova token che appaiono in entrambi i gruppi\n",
    "        common_tokens = set(success_tokens.keys()) & set(fail_tokens.keys())\n",
    "        \n",
    "        # Calcola differenza di importanza (successo - fallimento)\n",
    "        token_differences = []\n",
    "        for token in common_tokens:\n",
    "            diff = success_tokens[token]['mean'] - fail_tokens[token]['mean']\n",
    "            token_differences.append({\n",
    "                'token': token,\n",
    "                'success_importance': success_tokens[token]['mean'],\n",
    "                'fail_importance': fail_tokens[token]['mean'],\n",
    "                'difference': diff,\n",
    "                'success_tasks': success_tokens[token]['num_tasks'],\n",
    "                'fail_tasks': fail_tokens[token]['num_tasks'],\n",
    "                'total_appearances': success_tokens[token]['total_count'] + fail_tokens[token]['total_count']\n",
    "            })\n",
    "        \n",
    "        # Ordina per differenza assoluta (token piÃ¹ discriminativi)\n",
    "        token_differences.sort(key=lambda x: abs(x['difference']), reverse=True)\n",
    "        \n",
    "        print(f\"Top 15 token piÃ¹ discriminativi (differenza di importanza tra successo e fallimento):\\n\")\n",
    "        print(f\"{'Rank':<5} {'Token':<20} {'Success':<12} {'Failure':<12} {'Diff':<12} {'Verdict':<15}\")\n",
    "        print(f\"{'-'*100}\")\n",
    "        \n",
    "        for i, td in enumerate(token_differences[:15], 1):\n",
    "            verdict = \"âœ“ Pro-Success\" if td['difference'] > 0 else \"âœ— Pro-Failure\"\n",
    "            print(f\"{i:<5} '{td['token']:<18}' {td['success_importance']:<12.4f} \"\n",
    "                  f\"{td['fail_importance']:<12.4f} {td['difference']:+.4f}      {verdict:<15}\")\n",
    "        \n",
    "        print(f\"\\nInterpretazione:\")\n",
    "        print(f\"  â€¢ Token con differenza positiva â†’ piÃ¹ importanti nei task RIUSCITI\")\n",
    "        print(f\"  â€¢ Token con differenza negativa â†’ piÃ¹ importanti nei task FALLITI\")\n",
    "    \n",
    "    # ANALISI 2: Evoluzione temporale dell'attenzione\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"ANALISI 2: EVOLUZIONE TEMPORALE DELL'ATTENZIONE\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Per ogni token frequente, analizza come cambia l'importanza nel tempo\n",
    "    token_temporal_patterns = {}\n",
    "    for task in task_data:\n",
    "        for phase, phase_name in [('early_importance', 'early'), \n",
    "                                   ('mid_importance', 'mid'), \n",
    "                                   ('late_importance', 'late')]:\n",
    "            for token, stats in task[phase].items():\n",
    "                if token not in token_temporal_patterns:\n",
    "                    token_temporal_patterns[token] = {\n",
    "                        'early': [], 'mid': [], 'late': [],\n",
    "                        'tasks_early': 0, 'tasks_mid': 0, 'tasks_late': 0\n",
    "                    }\n",
    "                token_temporal_patterns[token][phase_name].append(stats['mean'])\n",
    "                token_temporal_patterns[token][f'tasks_{phase_name}'] += 1\n",
    "    \n",
    "    # Calcola pattern temporale\n",
    "    temporal_analysis = []\n",
    "    for token, patterns in token_temporal_patterns.items():\n",
    "        if len(patterns['early']) >= 3:  # Minimo 3 task\n",
    "            early_mean = np.mean(patterns['early']) if patterns['early'] else 0\n",
    "            mid_mean = np.mean(patterns['mid']) if patterns['mid'] else 0\n",
    "            late_mean = np.mean(patterns['late']) if patterns['late'] else 0\n",
    "            \n",
    "            # Calcola trend (crescente, decrescente, stabile)\n",
    "            early_late_diff = late_mean - early_mean\n",
    "            \n",
    "            temporal_analysis.append({\n",
    "                'token': token,\n",
    "                'early': early_mean,\n",
    "                'mid': mid_mean,\n",
    "                'late': late_mean,\n",
    "                'trend': early_late_diff,\n",
    "                'pattern': 'Crescente' if early_late_diff > 0.01 else ('Decrescente' if early_late_diff < -0.01 else 'Stabile')\n",
    "            })\n",
    "    \n",
    "    temporal_analysis.sort(key=lambda x: abs(x['trend']), reverse=True)\n",
    "    \n",
    "    print(f\"Token con pattern temporale piÃ¹ marcato:\\n\")\n",
    "    print(f\"{'Rank':<5} {'Token':<20} {'Early':<10} {'Mid':<10} {'Late':<10} {'Trend':<12} {'Pattern':<15}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    for i, ta in enumerate(temporal_analysis[:15], 1):\n",
    "        print(f\"{i:<5} '{ta['token']:<18}' {ta['early']:<10.4f} {ta['mid']:<10.4f} \"\n",
    "              f\"{ta['late']:<10.4f} {ta['trend']:+.4f}     {ta['pattern']:<15}\")\n",
    "    \n",
    "    print(f\"\\nInterpretazione:\")\n",
    "    print(f\"  â€¢ Pattern Crescente â†’ attenzione aumenta nel tempo (esecuzione finale)\")\n",
    "    print(f\"  â€¢ Pattern Decrescente â†’ attenzione diminuisce (pianificazione iniziale)\")\n",
    "    print(f\"  â€¢ Pattern Stabile â†’ importanza costante durante tutto il task\")\n",
    "    \n",
    "    # ANALISI 3: Correlazione tra importanza e success rate\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"ANALISI 3: CORRELAZIONE QUANTITATIVA (Token Importance vs Success Rate)\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Per ogni token, calcola correlazione con success rate\n",
    "    correlations = []\n",
    "    for token in all_tokens:\n",
    "        success_rates = []\n",
    "        importance_scores = []\n",
    "        \n",
    "        for task in task_data:\n",
    "            if token in task['overall_importance']:\n",
    "                success_rates.append(task['success_rate'])\n",
    "                importance_scores.append(task['overall_importance'][token]['mean'])\n",
    "        \n",
    "        if len(success_rates) >= 3:  # Minimo 3 task\n",
    "            # Correlazione di Pearson\n",
    "            if np.std(success_rates) > 0 and np.std(importance_scores) > 0:\n",
    "                corr = np.corrcoef(success_rates, importance_scores)[0, 1]\n",
    "                \n",
    "                correlations.append({\n",
    "                    'token': token,\n",
    "                    'correlation': corr,\n",
    "                    'num_tasks': len(success_rates),\n",
    "                    'avg_importance': np.mean(importance_scores),\n",
    "                    'importance_std': np.std(importance_scores)\n",
    "                })\n",
    "    \n",
    "    # Ordina per correlazione assoluta\n",
    "    correlations.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "    \n",
    "    print(f\"Token con correlazione piÃ¹ forte (positiva = predice successo, negativa = predice fallimento):\\n\")\n",
    "    print(f\"{'Rank':<5} {'Token':<20} {'Correlation':<13} {'Avg Importance':<16} {'Tasks':<8} {'Verdict':<20}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    for i, c in enumerate(correlations[:20], 1):\n",
    "        corr_str = f\"{c['correlation']:+.4f}\"\n",
    "        if c['correlation'] > 0.3:\n",
    "            verdict = \"âœ“âœ“ Forte predictor di SUCCESSO\"\n",
    "        elif c['correlation'] < -0.3:\n",
    "            verdict = \"âœ—âœ— Forte predictor di FALLIMENTO\"\n",
    "        elif c['correlation'] > 0:\n",
    "            verdict = \"âœ“ Lieve pro-success\"\n",
    "        else:\n",
    "            verdict = \"âœ— Lieve pro-failure\"\n",
    "        \n",
    "        print(f\"{i:<5} '{c['token']:<18}' {corr_str:<13} {c['avg_importance']:<16.4f} \"\n",
    "              f\"{c['num_tasks']:<8} {verdict:<20}\")\n",
    "    \n",
    "    # ANALISI 4: StabilitÃ  dell'attenzione\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"ANALISI 4: STABILITÃ€ DELL'ATTENZIONE (Varianza)\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Alta varianza = attenzione instabile, bassa varianza = attenzione stabile\n",
    "    stability_analysis = []\n",
    "    for task in task_data:\n",
    "        for token, stats in task['overall_importance'].items():\n",
    "            if stats['count'] >= 5:  # Minimo 5 apparizioni\n",
    "                stability_analysis.append({\n",
    "                    'token': token,\n",
    "                    'task_id': task['task_id'],\n",
    "                    'success_rate': task['success_rate'],\n",
    "                    'mean_importance': stats['mean'],\n",
    "                    'std_importance': stats['std'],\n",
    "                    'stability': stats['std'] / stats['mean'] if stats['mean'] > 0 else 0  # Coefficient of variation\n",
    "                })\n",
    "    \n",
    "    # Confronta stabilitÃ  tra successi e fallimenti\n",
    "    if successful_tasks and failed_tasks:\n",
    "        success_stability = [s['stability'] for s in stability_analysis if s['success_rate'] > 0.5]\n",
    "        fail_stability = [s['stability'] for s in stability_analysis if s['success_rate'] <= 0.5]\n",
    "        \n",
    "        print(f\"StabilitÃ  media dell'attenzione (coefficiente di variazione):\")\n",
    "        print(f\"  â€¢ Task riusciti:  {np.mean(success_stability):.4f} Â± {np.std(success_stability):.4f}\")\n",
    "        print(f\"  â€¢ Task falliti:   {np.mean(fail_stability):.4f} Â± {np.std(fail_stability):.4f}\")\n",
    "        print(f\"\\n  â†’ {'Task riusciti hanno attenzione PIÃ™ STABILE' if np.mean(success_stability) < np.mean(fail_stability) else 'Task falliti hanno attenzione PIÃ™ STABILE'}\")\n",
    "    \n",
    "    # RIEPILOGO FINALE\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"RIEPILOGO CONCLUSIVO\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Task-specific summary\n",
    "    print(\"Performance per task:\\n\")\n",
    "    task_data.sort(key=lambda x: x['success_rate'], reverse=True)\n",
    "    for task in task_data:\n",
    "        status = \"âœ“ SUCCESSO\" if task['success_rate'] > 0.5 else \"âœ— FALLIMENTO\"\n",
    "        print(f\"  Task {task['task_id']:2d} ({task['success_rate']:6.1%}) {status}: {task['prompt'][:70]}...\")\n",
    "        \n",
    "        # Top 3 token per questo task\n",
    "        top_tokens = sorted(task['overall_importance'].items(), \n",
    "                          key=lambda x: x[1]['mean'], reverse=True)[:3]\n",
    "        token_str = \", \".join([f\"'{t[0]}' ({t[1]['mean']:.3f})\" for t in top_tokens])\n",
    "        print(f\"           Top token: {token_str}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'task_data': task_data,\n",
    "        'token_differences': token_differences if successful_tasks and failed_tasks else None,\n",
    "        'temporal_analysis': temporal_analysis,\n",
    "        'correlations': correlations,\n",
    "        'summary': {\n",
    "            'total_tasks': len(task_data),\n",
    "            'successful_tasks': len(successful_tasks),\n",
    "            'failed_tasks': len(failed_tasks),\n",
    "            'mean_success_rate': np.mean([t['success_rate'] for t in task_data])\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Esegui l'analisi approfondita\n",
    "if list(Path('evaluation_videos').glob('task_*_explainability.json')):\n",
    "    results = deep_explainability_analysis(video_dir='evaluation_videos')\n",
    "else:\n",
    "    print(\"âš ï¸ Nessun file di explainability trovato in evaluation_videos/\")\n",
    "    print(\"Esegui prima evaluate_model con enable_explainability=True\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STUDIO CORRELAZIONE: Text vs Visual Importance e Success Rate\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_multimodal_correlation(\n",
    "    checkpoint_path: str = 'models/back.pt',\n",
    "    action_stats_path: str = 'action_stats.json',\n",
    "    benchmark: str = 'libero_spatial',\n",
    "    task_ids: Optional[List[int]] = None,\n",
    "    env_num: int = 5,\n",
    "    max_steps: int = 600,\n",
    "    analysis_interval: int = 25,\n",
    "    visual_method: str = 'gradcam',\n",
    "    save_videos: bool = True,\n",
    "    video_dir: str = 'evaluation_videos'\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analizza la correlazione tra importanza text/visual e success rate.\n",
    "    \n",
    "    Per ogni task:\n",
    "    1. Esegue valutazione raccogliendo explainability multimodale\n",
    "    2. Calcola rapporto text/visual importance per ogni step\n",
    "    3. Analizza pattern temporali\n",
    "    4. Correla con outcome (success/failure)\n",
    "    \n",
    "    Returns:\n",
    "        Dict completo con analisi per task e correlazioni\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"ANALISI MULTIMODALE: CORRELAZIONE TEXT vs VISUAL IMPORTANCE\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Carica modello\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    cfg = _merge_training_config(ckpt.get('config', {}))\n",
    "    policy = build_policy_from_config(cfg, obs_shape=(3, 128, 128)).to(device)\n",
    "    policy.load_state_dict(ckpt['model_state_dict'])\n",
    "    policy.eval()\n",
    "    \n",
    "    # Action stats\n",
    "    stats = json.load(open(action_stats_path))\n",
    "    action_mean = torch.tensor(stats['mean'], device=device).unsqueeze(0)\n",
    "    action_std = torch.tensor(stats['std'], device=device).unsqueeze(0)\n",
    "    \n",
    "    # Benchmark\n",
    "    benchmark_map = {'libero_10': 'LIBERO_10', 'libero_spatial': 'LIBERO_SPATIAL', 'libero_goal': 'LIBERO_GOAL'}\n",
    "    suite = get_benchmark(benchmark_map.get(benchmark, benchmark))(0)\n",
    "    \n",
    "    if task_ids is None:\n",
    "        task_ids = list(range(10))\n",
    "    \n",
    "    # Inizializza explainer multimodale\n",
    "    multimodal_explainer = None\n",
    "    if hasattr(policy, 'prompt_encoder'):\n",
    "        multimodal_explainer = MultimodalExplainer(policy, policy.prompt_encoder, device)\n",
    "        print(\"âœ… MultimodalExplainer initialized\")\n",
    "    \n",
    "    # Raccogli dati per tutti i task\n",
    "    all_task_data = []\n",
    "    \n",
    "    for task_id in task_ids:\n",
    "        task = suite.get_task(task_id)\n",
    "        task_prompt = task.language\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Task {task_id}: {task_prompt[:60]}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Setup environment\n",
    "        env_args = {\n",
    "            'bddl_file_name': str(Path(get_libero_path('bddl_files')) / task.problem_folder / task.bddl_file),\n",
    "            'camera_heights': 128,\n",
    "            'camera_widths': 128\n",
    "        }\n",
    "        env = SequentialVectorEnv([lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)])\n",
    "        \n",
    "        try:\n",
    "            init_states = torch.load(\n",
    "                str(Path(get_libero_path('init_states')) / task.problem_folder / task.init_states_file),\n",
    "                map_location='cpu', weights_only=False\n",
    "            )\n",
    "            obs = env.reset()\n",
    "            env.set_init_state(init_states[0:env_num])\n",
    "            \n",
    "            dones = [False] * env_num\n",
    "            successes = np.zeros(env_num, dtype=bool)\n",
    "            \n",
    "            # Dati per questo task\n",
    "            task_analysis = {\n",
    "                'task_id': task_id,\n",
    "                'prompt': task_prompt,\n",
    "                'frames': [],\n",
    "                'original_frames': []\n",
    "            }\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                obs_batch = _stack_vector_obs(obs)\n",
    "                cam_key = _select_camera_key(obs_batch)\n",
    "                \n",
    "                alive = [i for i, d in enumerate(dones) if not d]\n",
    "                if not alive:\n",
    "                    break\n",
    "                \n",
    "                vis_batch = obs_batch[cam_key][alive]\n",
    "                p_in = _prepare_policy_input(vis_batch, device)\n",
    "                \n",
    "                # Action prediction\n",
    "                with torch.no_grad():\n",
    "                    prompt_batch = [task_prompt for _ in alive]\n",
    "                    actions_alive = policy(p_in, prompt_batch)\n",
    "                    actions_alive = actions_alive * action_std + action_mean\n",
    "                    full_actions = np.zeros((env_num, 7), dtype=np.float32)\n",
    "                    full_actions[alive] = actions_alive.detach().cpu().numpy()\n",
    "                \n",
    "                # Multimodal explainability\n",
    "                if multimodal_explainer is not None and step % analysis_interval == 0 and len(alive) > 0:\n",
    "                    try:\n",
    "                        single_obs = p_in[0:1]\n",
    "                        \n",
    "                        # Compute multimodal importance\n",
    "                        # Calcola importanza multimodale base\n",
    "                        mm_result = multimodal_explainer.compute_multimodal_importance(\n",
    "                            single_obs, task_prompt, visual_method='vanilla'\n",
    "                        )\n",
    "                        \n",
    "                        # Calcola TUTTE le saliency map per la griglia video\n",
    "                        all_saliency = multimodal_explainer.compute_all_visual_methods(\n",
    "                            single_obs, task_prompt\n",
    "                        )\n",
    "                        \n",
    "                        frame_data = {\n",
    "                            'step': step,\n",
    "                            'text_importance': mm_result['text_importance'],\n",
    "                            'visual_importance': mm_result['visual_importance'],\n",
    "                            'text_ratio': mm_result['text_ratio'],\n",
    "                            'visual_ratio': mm_result['visual_ratio'],\n",
    "                            'text_visual_ratio': mm_result['text_visual_ratio'],\n",
    "                            'top_tokens': mm_result['top_tokens'],\n",
    "                            # Tutte le saliency map per la griglia\n",
    "                            'vanilla_saliency': all_saliency['vanilla'],\n",
    "                            'smoothgrad_saliency': all_saliency['smoothgrad'],\n",
    "                            'gradcam_saliency': all_saliency['gradcam'],\n",
    "                            'integrated_saliency': all_saliency['integrated']\n",
    "                        }\n",
    "                        task_analysis['frames'].append(frame_data)\n",
    "                        \n",
    "                        # Salva frame originale per video\n",
    "                        # vis_batch Ã¨ in formato (N, H, W, C) con valori uint8 [0, 255]\n",
    "                        orig_frame = vis_batch[0].copy()\n",
    "                        if orig_frame.dtype != np.uint8:\n",
    "                            if orig_frame.max() <= 1.0:\n",
    "                                orig_frame = (orig_frame * 255).astype(np.uint8)\n",
    "                            else:\n",
    "                                orig_frame = np.clip(orig_frame, 0, 255).astype(np.uint8)\n",
    "                        task_analysis['original_frames'].append(orig_frame)\n",
    "                        \n",
    "                        if step % (analysis_interval * 4) == 0:\n",
    "                            print(f\"  Step {step:3d} | Text: {mm_result['text_ratio']:.2%} | \"\n",
    "                                  f\"Visual: {mm_result['visual_ratio']:.2%} | \"\n",
    "                                  f\"Top: '{mm_result['top_tokens'][0][0] if mm_result['top_tokens'] else 'N/A'}'\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ Error at step {step}: {str(e)[:50]}\")\n",
    "                \n",
    "                # Step environment\n",
    "                obs, reward, done_batch, info = env.step(full_actions)\n",
    "                \n",
    "                for i in alive:\n",
    "                    if reward[i] != 0.0:\n",
    "                        successes[i] = True\n",
    "                    dones[i] = dones[i] or bool(done_batch[i])\n",
    "            \n",
    "            success_rate = float(successes.mean())\n",
    "            task_analysis['success_rate'] = success_rate\n",
    "            \n",
    "            # Calcola statistiche aggregate per il task\n",
    "            if task_analysis['frames']:\n",
    "                text_ratios = [f['text_ratio'] for f in task_analysis['frames']]\n",
    "                visual_ratios = [f['visual_ratio'] for f in task_analysis['frames']]\n",
    "                tv_ratios = [f['text_visual_ratio'] for f in task_analysis['frames']]\n",
    "                \n",
    "                task_analysis['stats'] = {\n",
    "                    'mean_text_ratio': np.mean(text_ratios),\n",
    "                    'std_text_ratio': np.std(text_ratios),\n",
    "                    'mean_visual_ratio': np.mean(visual_ratios),\n",
    "                    'std_visual_ratio': np.std(visual_ratios),\n",
    "                    'mean_tv_ratio': np.mean(tv_ratios),\n",
    "                    'std_tv_ratio': np.std(tv_ratios)\n",
    "                }\n",
    "            \n",
    "            all_task_data.append(task_analysis)\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Task {task_id} Results:\")\n",
    "            print(f\"   Success Rate: {success_rate:.1%}\")\n",
    "            if 'stats' in task_analysis:\n",
    "                print(f\"   Mean Text Ratio: {task_analysis['stats']['mean_text_ratio']:.2%}\")\n",
    "                print(f\"   Mean Visual Ratio: {task_analysis['stats']['mean_visual_ratio']:.2%}\")\n",
    "            \n",
    "            # Genera video explainability con GRIGLIA di tutte le metodologie\n",
    "            if save_videos and task_analysis['frames'] and task_analysis['original_frames']:\n",
    "                video_path = str(Path(video_dir) / f\"task_{task_id:02d}_grid_explainability.mp4\")\n",
    "                generate_grid_explainability_video(\n",
    "                    task_analysis['frames'],\n",
    "                    task_analysis['original_frames'],\n",
    "                    video_path,\n",
    "                    fps=5\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error on task {task_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            env.close()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ANALISI DI CORRELAZIONE\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"RISULTATI ANALISI DI CORRELAZIONE\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Prepara dati per correlazione\n",
    "    valid_tasks = [t for t in all_task_data if 'stats' in t]\n",
    "    \n",
    "    if len(valid_tasks) < 2:\n",
    "        print(\"âš ï¸ Insufficient data for correlation analysis\")\n",
    "        return {'task_data': all_task_data}\n",
    "    \n",
    "    success_rates = [t['success_rate'] for t in valid_tasks]\n",
    "    text_ratios = [t['stats']['mean_text_ratio'] for t in valid_tasks]\n",
    "    visual_ratios = [t['stats']['mean_visual_ratio'] for t in valid_tasks]\n",
    "    tv_ratios = [t['stats']['mean_tv_ratio'] for t in valid_tasks]\n",
    "    \n",
    "    # Correlazioni di Pearson\n",
    "    corr_text_success = np.corrcoef(success_rates, text_ratios)[0, 1]\n",
    "    corr_visual_success = np.corrcoef(success_rates, visual_ratios)[0, 1]\n",
    "    corr_tv_success = np.corrcoef(success_rates, tv_ratios)[0, 1]\n",
    "    \n",
    "    print(\"ðŸ“Š CORRELAZIONE CON SUCCESS RATE:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Text Importance vs Success:   r = {corr_text_success:+.4f}\")\n",
    "    print(f\"   Visual Importance vs Success: r = {corr_visual_success:+.4f}\")\n",
    "    print(f\"   Text/Visual Ratio vs Success: r = {corr_tv_success:+.4f}\")\n",
    "    \n",
    "    # Interpretazione\n",
    "    print(f\"\\nðŸ“ˆ INTERPRETAZIONE:\")\n",
    "    if corr_text_success > 0.3:\n",
    "        print(f\"   âœ… Maggiore attenzione al TESTO â†’ predice SUCCESSO (r={corr_text_success:.3f})\")\n",
    "    elif corr_text_success < -0.3:\n",
    "        print(f\"   âš ï¸ Maggiore attenzione al TESTO â†’ predice FALLIMENTO (r={corr_text_success:.3f})\")\n",
    "    \n",
    "    if corr_visual_success > 0.3:\n",
    "        print(f\"   âœ… Maggiore attenzione VISIVA â†’ predice SUCCESSO (r={corr_visual_success:.3f})\")\n",
    "    elif corr_visual_success < -0.3:\n",
    "        print(f\"   âš ï¸ Maggiore attenzione VISIVA â†’ predice FALLIMENTO (r={corr_visual_success:.3f})\")\n",
    "    \n",
    "    # Analisi per gruppo (success vs failure)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ðŸ“Š CONFRONTO SUCCESSI vs FALLIMENTI:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    success_tasks = [t for t in valid_tasks if t['success_rate'] > 0.5]\n",
    "    failure_tasks = [t for t in valid_tasks if t['success_rate'] <= 0.5]\n",
    "    \n",
    "    if success_tasks:\n",
    "        success_text = np.mean([t['stats']['mean_text_ratio'] for t in success_tasks])\n",
    "        success_visual = np.mean([t['stats']['mean_visual_ratio'] for t in success_tasks])\n",
    "        print(f\"\\n   Task RIUSCITI ({len(success_tasks)} tasks):\")\n",
    "        print(f\"      Text Ratio medio:   {success_text:.2%}\")\n",
    "        print(f\"      Visual Ratio medio: {success_visual:.2%}\")\n",
    "    \n",
    "    if failure_tasks:\n",
    "        failure_text = np.mean([t['stats']['mean_text_ratio'] for t in failure_tasks])\n",
    "        failure_visual = np.mean([t['stats']['mean_visual_ratio'] for t in failure_tasks])\n",
    "        print(f\"\\n   Task FALLITI ({len(failure_tasks)} tasks):\")\n",
    "        print(f\"      Text Ratio medio:   {failure_text:.2%}\")\n",
    "        print(f\"      Visual Ratio medio: {failure_visual:.2%}\")\n",
    "    \n",
    "    if success_tasks and failure_tasks:\n",
    "        diff_text = success_text - failure_text\n",
    "        diff_visual = success_visual - failure_visual\n",
    "        print(f\"\\n   DIFFERENZA (Success - Failure):\")\n",
    "        print(f\"      Text:   {diff_text:+.2%}\")\n",
    "        print(f\"      Visual: {diff_visual:+.2%}\")\n",
    "        \n",
    "        if abs(diff_text) > abs(diff_visual):\n",
    "            modality = \"TESTUALE\"\n",
    "            direction = \"maggiore\" if diff_text > 0 else \"minore\"\n",
    "        else:\n",
    "            modality = \"VISIVA\"\n",
    "            direction = \"maggiore\" if diff_visual > 0 else \"minore\"\n",
    "        \n",
    "        print(f\"\\n   â†’ L'attenzione {modality} Ã¨ {direction} nei task riusciti\")\n",
    "    \n",
    "    # Analisi temporale multimodale\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ðŸ“Š EVOLUZIONE TEMPORALE TEXT vs VISUAL:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Dividi frames in fasi\n",
    "    for task in valid_tasks[:3]:  # Top 3 per esempio\n",
    "        frames = task['frames']\n",
    "        if len(frames) < 6:\n",
    "            continue\n",
    "        \n",
    "        n = len(frames)\n",
    "        early = frames[:n//3]\n",
    "        mid = frames[n//3:2*n//3]\n",
    "        late = frames[2*n//3:]\n",
    "        \n",
    "        early_text = np.mean([f['text_ratio'] for f in early])\n",
    "        mid_text = np.mean([f['text_ratio'] for f in mid])\n",
    "        late_text = np.mean([f['text_ratio'] for f in late])\n",
    "        \n",
    "        status = \"âœ“\" if task['success_rate'] > 0.5 else \"âœ—\"\n",
    "        print(f\"\\n   Task {task['task_id']} ({task['success_rate']:.0%}) {status}:\")\n",
    "        print(f\"      Early Text: {early_text:.1%} â†’ Mid: {mid_text:.1%} â†’ Late: {late_text:.1%}\")\n",
    "        \n",
    "        trend = \"â†— Crescente\" if late_text > early_text + 0.02 else (\n",
    "            \"â†˜ Decrescente\" if late_text < early_text - 0.02 else \"â†’ Stabile\"\n",
    "        )\n",
    "        print(f\"      Pattern: {trend}\")\n",
    "    \n",
    "    # Salva risultati\n",
    "    results = {\n",
    "        'task_data': all_task_data,\n",
    "        'correlations': {\n",
    "            'text_success': corr_text_success,\n",
    "            'visual_success': corr_visual_success,\n",
    "            'tv_ratio_success': corr_tv_success\n",
    "        },\n",
    "        'summary': {\n",
    "            'total_tasks': len(valid_tasks),\n",
    "            'mean_success_rate': np.mean(success_rates),\n",
    "            'mean_text_ratio': np.mean(text_ratios),\n",
    "            'mean_visual_ratio': np.mean(visual_ratios)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Salva JSON\n",
    "    output_path = Path(video_dir) / 'multimodal_analysis.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        # Converti numpy per JSON\n",
    "        def convert(obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, (np.float32, np.float64)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, (np.int32, np.int64)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert(v) for v in obj]\n",
    "            return obj\n",
    "        \n",
    "        # Rimuovi saliency maps per JSON (troppo grandi)\n",
    "        json_results = convert({\n",
    "            'correlations': results['correlations'],\n",
    "            'summary': results['summary'],\n",
    "            'per_task': [\n",
    "                {\n",
    "                    'task_id': t['task_id'],\n",
    "                    'prompt': t['prompt'],\n",
    "                    'success_rate': t['success_rate'],\n",
    "                    'stats': t.get('stats', {})\n",
    "                }\n",
    "                for t in all_task_data\n",
    "            ]\n",
    "        })\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Results saved to {output_path}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Avvio analisi multimodale Text vs Visual Importance...\n",
      "   Questo genererÃ :\n",
      "   1. Video con heatmap di saliency per ogni task\n",
      "   2. Analisi correlazione text/visual con success rate\n",
      "   3. Pattern temporali di attenzione\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "ANALISI MULTIMODALE: CORRELAZIONE TEXT vs VISUAL IMPORTANCE\n",
      "====================================================================================================\n",
      "\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "âœ… MultimodalExplainer initialized\n",
      "\n",
      "============================================================\n",
      "Task 8: pick up the black bowl next to the plate and place it on the...\n",
      "============================================================\n",
      "  Step   0 | Text: 29.63% | Visual: 70.37% | Top: 'plate'\n",
      "  Step  20 | Text: 60.45% | Visual: 39.55% | Top: 'plate'\n",
      "  Step  40 | Text: 25.20% | Visual: 74.80% | Top: 'plate'\n",
      "  Step  60 | Text: 58.09% | Visual: 41.91% | Top: 'plate'\n",
      "  Step  80 | Text: 62.56% | Visual: 37.44% | Top: 'plate'\n",
      "  Step 100 | Text: 66.41% | Visual: 33.59% | Top: 'plate'\n",
      "  Step 120 | Text: 70.75% | Visual: 29.25% | Top: 'plate'\n",
      "  Step 140 | Text: 68.57% | Visual: 31.43% | Top: 'plate'\n",
      "  Step 160 | Text: 48.68% | Visual: 51.32% | Top: 'plate'\n",
      "  Step 180 | Text: 38.57% | Visual: 61.43% | Top: 'plate'\n",
      "  Step 200 | Text: 35.97% | Visual: 64.03% | Top: 'plate'\n",
      "  Step 220 | Text: 35.10% | Visual: 64.90% | Top: 'plate'\n",
      "  Step 240 | Text: 48.08% | Visual: 51.92% | Top: 'plate'\n",
      "  Step 260 | Text: 43.97% | Visual: 56.03% | Top: 'plate'\n",
      "  Step 280 | Text: 48.93% | Visual: 51.07% | Top: 'plate'\n",
      "  Step 300 | Text: 60.88% | Visual: 39.12% | Top: 'plate'\n",
      "  Step 320 | Text: 41.19% | Visual: 58.81% | Top: 'plate'\n",
      "  Step 340 | Text: 38.35% | Visual: 61.65% | Top: 'plate'\n",
      "  Step 360 | Text: 35.33% | Visual: 64.67% | Top: 'plate'\n",
      "  Step 380 | Text: 36.29% | Visual: 63.71% | Top: 'plate'\n",
      "  Step 400 | Text: 36.58% | Visual: 63.42% | Top: 'plate'\n",
      "  Step 420 | Text: 37.83% | Visual: 62.17% | Top: 'plate'\n",
      "  Step 440 | Text: 43.25% | Visual: 56.75% | Top: 'plate'\n",
      "  Step 460 | Text: 40.65% | Visual: 59.35% | Top: 'plate'\n",
      "  Step 480 | Text: 41.82% | Visual: 58.18% | Top: 'plate'\n",
      "\n",
      "ðŸ“Š Task 8 Results:\n",
      "   Success Rate: 0.0%\n",
      "   Mean Text Ratio: 46.74%\n",
      "   Mean Visual Ratio: 53.26%\n",
      "ðŸ“¹ Generating grid video with 100 frames...\n",
      "   Frame size: 128x128, Panel size: 128x153, Grid size: 384x306\n",
      "âœ… Grid explainability video saved to evaluation_videos/task_08_grid_explainability.mp4 (100 frames)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "RISULTATI ANALISI DI CORRELAZIONE\n",
      "====================================================================================================\n",
      "\n",
      "âš ï¸ Insufficient data for correlation analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ESEGUI ANALISI MULTIMODALE COMPLETA\n",
    "# ============================================================================\n",
    "\n",
    "# Esegui l'analisi di correlazione text vs visual\n",
    "print(\"ðŸ” Avvio analisi multimodale Text vs Visual Importance...\")\n",
    "print(\"   Questo genererÃ :\")\n",
    "print(\"   1. Video con heatmap di saliency per ogni task\")\n",
    "print(\"   2. Analisi correlazione text/visual con success rate\")\n",
    "print(\"   3. Pattern temporali di attenzione\")\n",
    "print()\n",
    "\n",
    "multimodal_results = analyze_multimodal_correlation(\n",
    "    checkpoint_path='models/back.pt',\n",
    "    action_stats_path='action_stats.json',\n",
    "    benchmark='libero_spatial',\n",
    "    task_ids= [8], #list(range(10)),  # Tutti i 10 task\n",
    "    env_num=1,\n",
    "    max_steps=500,\n",
    "    analysis_interval=5,  # Analizza ogni 5 step\n",
    "    visual_method='gradcam',  # Usa GradCAM per heatmap\n",
    "    save_videos=True,\n",
    "    video_dir='evaluation_videos'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "VISUALIZZAZIONE RISULTATI ANALISI MULTIMODALE\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ“Š IMPORTANZA RELATIVA TEXT vs VISUAL PER TASK:\n",
      "================================================================================\n",
      "Task      Success Text                           Visual                        \n",
      "--------------------------------------------------------------------------------\n",
      "Task  2 âœ—     0%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 68.4% | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           31.6%\n",
      "Task  8 âœ—     0%  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  49.5% | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 50.5%\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "SCATTER: SUCCESS RATE vs TEXT IMPORTANCE RATIO\n",
      "====================================================================================================\n",
      "Success Rate %\n",
      "100 |\n",
      "100 |                                                            \n",
      "    |                                                            \n",
      " 80 |                                                            \n",
      "    |                                                            \n",
      " 60 |                                                            \n",
      "    |                                                            \n",
      " 40 |                                                            \n",
      "    |                                                            \n",
      " 20 |                                                            \n",
      "    |                             â—‹                  â—‹           \n",
      "  0 +------------------------------------------------------------\n",
      "      20%               40%               60%               80%\n",
      "                    Text Importance Ratio\n",
      "      â— = Success (>50%)    â—‹ = Failure (â‰¤50%)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "CONCLUSIONI DELL'ANALISI\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ”¬ CORRELAZIONI CHIAVE:\n",
      "   â€¢ Correlazione Text â†” Success: r = +nan\n",
      "   â€¢ Correlazione Visual â†” Success: r = +nan\n",
      "   â€¢ Correlazione Text/Visual Ratio â†” Success: r = +nan\n",
      "\n",
      "ðŸ“ˆ INTERPRETAZIONE:\n",
      "\n",
      "ðŸ’¡ RACCOMANDAZIONI:\n",
      "   â†’ L'ATTENZIONE VISIVA Ã¨ piÃ¹ correlata al successo\n",
      "   â†’ Migliorare il visual encoder potrebbe essere la prioritÃ \n",
      "\n",
      "ðŸ“Š STATISTICHE FINALI:\n",
      "   â€¢ Task analizzati: 2\n",
      "   â€¢ Success rate medio: 0.0%\n",
      "   â€¢ Text ratio medio: 58.9%\n",
      "   â€¢ Visual ratio medio: 41.1%\n",
      "\n",
      "ðŸ“ OUTPUT GENERATI:\n",
      "   â€¢ Video heatmap: evaluation_videos/task_XX_heatmap.mp4\n",
      "   â€¢ Analisi JSON: evaluation_videos/multimodal_analysis.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZZAZIONE RISULTATI E CONCLUSIONI\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_multimodal_results(results: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Visualizza i risultati dell'analisi multimodale con grafici ASCII.\n",
    "    \"\"\"\n",
    "    if not results or 'task_data' not in results:\n",
    "        print(\"âš ï¸ No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    valid_tasks = [t for t in results['task_data'] if 'stats' in t]\n",
    "    \n",
    "    if not valid_tasks:\n",
    "        print(\"âš ï¸ No valid task data\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"VISUALIZZAZIONE RISULTATI ANALISI MULTIMODALE\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Ordina per success rate\n",
    "    valid_tasks.sort(key=lambda x: x['success_rate'], reverse=True)\n",
    "    \n",
    "    # Grafico a barre ASCII: Text vs Visual per task\n",
    "    print(\"ðŸ“Š IMPORTANZA RELATIVA TEXT vs VISUAL PER TASK:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Task':<8} {'Success':>8} {'Text':<30} {'Visual':<30}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    for task in valid_tasks:\n",
    "        text_ratio = task['stats']['mean_text_ratio']\n",
    "        visual_ratio = task['stats']['mean_visual_ratio']\n",
    "        \n",
    "        text_bar = 'â–ˆ' * int(text_ratio * 50)\n",
    "        visual_bar = 'â–ˆ' * int(visual_ratio * 50)\n",
    "        \n",
    "        status = \"âœ“\" if task['success_rate'] > 0.5 else \"âœ—\"\n",
    "        print(f\"Task {task['task_id']:2d} {status} {task['success_rate']:>6.0%}  \"\n",
    "              f\"{text_bar:<25} {text_ratio:>4.1%} | \"\n",
    "              f\"{visual_bar:<25} {visual_ratio:>4.1%}\")\n",
    "    \n",
    "    # Scatter plot ASCII: Success Rate vs Text Importance\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"SCATTER: SUCCESS RATE vs TEXT IMPORTANCE RATIO\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(\"Success Rate %\")\n",
    "    print(\"100 |\", end=\"\")\n",
    "    \n",
    "    # Crea griglia 20x60\n",
    "    grid = [[' ' for _ in range(60)] for _ in range(10)]\n",
    "    \n",
    "    for task in valid_tasks:\n",
    "        sr = task['success_rate']\n",
    "        tr = task['stats']['mean_text_ratio']\n",
    "        \n",
    "        row = 9 - int(sr * 9)  # 0-9, invertito\n",
    "        col = int(tr * 100) - 20  # Assumendo text ratio tra 20% e 80%\n",
    "        col = max(0, min(59, col))\n",
    "        \n",
    "        if task['success_rate'] > 0.5:\n",
    "            grid[row][col] = 'â—'\n",
    "        else:\n",
    "            grid[row][col] = 'â—‹'\n",
    "    \n",
    "    for i, row in enumerate(grid):\n",
    "        sr_label = f\"{100 - i * 10:3d}\" if i % 2 == 0 else \"   \"\n",
    "        print(f\"\\n{sr_label} |{''.join(row)}\", end=\"\")\n",
    "    \n",
    "    print(f\"\\n  0 +{'-'*60}\")\n",
    "    print(f\"      20%{' '*15}40%{' '*15}60%{' '*15}80%\")\n",
    "    print(f\"                    Text Importance Ratio\")\n",
    "    print(f\"      â— = Success (>50%)    â—‹ = Failure (â‰¤50%)\")\n",
    "    \n",
    "    # Conclusioni\n",
    "    print(f\"\\n\\n{'='*100}\")\n",
    "    print(\"CONCLUSIONI DELL'ANALISI\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    corr = results.get('correlations', {})\n",
    "    \n",
    "    print(\"ðŸ”¬ CORRELAZIONI CHIAVE:\")\n",
    "    print(f\"   â€¢ Correlazione Text â†” Success: r = {corr.get('text_success', 0):+.4f}\")\n",
    "    print(f\"   â€¢ Correlazione Visual â†” Success: r = {corr.get('visual_success', 0):+.4f}\")\n",
    "    print(f\"   â€¢ Correlazione Text/Visual Ratio â†” Success: r = {corr.get('tv_ratio_success', 0):+.4f}\")\n",
    "    \n",
    "    # Interpretazione automatica\n",
    "    text_corr = corr.get('text_success', 0)\n",
    "    visual_corr = corr.get('visual_success', 0)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ INTERPRETAZIONE:\")\n",
    "    \n",
    "    if text_corr > 0.3:\n",
    "        print(\"   âœ… Il modello ha MIGLIORI performance quando presta MAGGIORE attenzione al TESTO\")\n",
    "        print(\"      â†’ Il language grounding Ã¨ cruciale per il successo\")\n",
    "    elif text_corr < -0.3:\n",
    "        print(\"   âš ï¸ Il modello ha PEGGIORI performance quando presta MAGGIORE attenzione al TESTO\")\n",
    "        print(\"      â†’ Potrebbe indicare over-reliance sul linguaggio vs percezione visiva\")\n",
    "    \n",
    "    if visual_corr > 0.3:\n",
    "        print(\"   âœ… Il modello ha MIGLIORI performance quando presta MAGGIORE attenzione all'INPUT VISIVO\")\n",
    "        print(\"      â†’ La percezione visiva Ã¨ fondamentale per l'esecuzione corretta\")\n",
    "    elif visual_corr < -0.3:\n",
    "        print(\"   âš ï¸ Il modello ha PEGGIORI performance quando presta MAGGIORE attenzione all'INPUT VISIVO\")\n",
    "        print(\"      â†’ Potrebbe indicare difficoltÃ  nel processing visivo\")\n",
    "    \n",
    "    # Raccomandazioni\n",
    "    print(f\"\\nðŸ’¡ RACCOMANDAZIONI:\")\n",
    "    \n",
    "    if abs(text_corr) > abs(visual_corr):\n",
    "        print(\"   â†’ L'ATTENZIONE TESTUALE Ã¨ piÃ¹ correlata al successo\")\n",
    "        print(\"   â†’ Migliorare il language grounding potrebbe essere la prioritÃ \")\n",
    "    else:\n",
    "        print(\"   â†’ L'ATTENZIONE VISIVA Ã¨ piÃ¹ correlata al successo\")\n",
    "        print(\"   â†’ Migliorare il visual encoder potrebbe essere la prioritÃ \")\n",
    "    \n",
    "    # Statistiche finali\n",
    "    summary = results.get('summary', {})\n",
    "    print(f\"\\nðŸ“Š STATISTICHE FINALI:\")\n",
    "    print(f\"   â€¢ Task analizzati: {summary.get('total_tasks', len(valid_tasks))}\")\n",
    "    print(f\"   â€¢ Success rate medio: {summary.get('mean_success_rate', 0):.1%}\")\n",
    "    print(f\"   â€¢ Text ratio medio: {summary.get('mean_text_ratio', 0):.1%}\")\n",
    "    print(f\"   â€¢ Visual ratio medio: {summary.get('mean_visual_ratio', 0):.1%}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ OUTPUT GENERATI:\")\n",
    "    print(f\"   â€¢ Video heatmap: evaluation_videos/task_XX_heatmap.mp4\")\n",
    "    print(f\"   â€¢ Analisi JSON: evaluation_videos/multimodal_analysis.json\")\n",
    "\n",
    "\n",
    "# Visualizza i risultati se disponibili\n",
    "if 'multimodal_results' in dir() and multimodal_results:\n",
    "    visualize_multimodal_results(multimodal_results)\n",
    "else:\n",
    "    print(\"âš ï¸ Esegui prima la cella di analisi multimodale\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
