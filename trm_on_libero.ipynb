{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libero dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo n | python LIBERO/benchmark_scripts/download_libero_datasets.py --datasets libero_spatial --use-huggingface > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir dataset | mv LIBERO/libero/datasets/* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# The Patch\n",
    "mock_mpl = MagicMock()\n",
    "sys.modules[\"matplotlib\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.pyplot\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.cm\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.colors\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.transforms\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.ticker\"] = mock_mpl\n",
    "sys.modules[\"matplotlib._path\"] = mock_mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict, replace\n",
    "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Vision\n",
    "import einops\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Transformers\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "# Device setup\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training and model hyperparameters container.\"\"\"\n",
    "\n",
    "    lr: float = 3e-4\n",
    "    hidden_dim: int = 256\n",
    "    num_recursions: int = 8\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 64\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    sched_T0: Optional[int] = None\n",
    "    sched_T_mult: int = 1\n",
    "    lr_min: float = 1e-6\n",
    "    warmup_epochs: int = 3\n",
    "    early_stop_patience: Optional[int] = None\n",
    "    save_path: str = 'model.pt'\n",
    "    freeze_backbone: bool = True\n",
    "    augmentation: bool = False\n",
    "    dropout: float = 0.1\n",
    "    encoder_dropout: float = 0.1\n",
    "    use_text_prompts: bool = True\n",
    "    text_encoder_name: str = 'openai/clip-vit-large-patch14'\n",
    "    train_text_encoder: bool = False\n",
    "    text_dropout: float = 0.1\n",
    "    double_visual_features: bool = False\n",
    "    use_attention: bool = True\n",
    "    attention_fusion: bool = False\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "    def label(self) -> str:\n",
    "        return f\"lr{self.lr}_h{self.hidden_dim}_rec{self.num_recursions}_bs{self.batch_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(dataset):\n",
    "    \"\"\"Load images from HDF5 dataset with robust dtype handling.\"\"\"\n",
    "    shape = dataset.shape\n",
    "\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.uint8)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float32)\n",
    "        dataset.read_direct(buffer)\n",
    "        if buffer.max() <= 1.0:\n",
    "            buffer = (buffer * 255).astype(np.uint8)\n",
    "        else:\n",
    "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float64)\n",
    "        dataset.read_direct(buffer)\n",
    "        if buffer.max() <= 1.0:\n",
    "            buffer = (buffer * 255).astype(np.uint8)\n",
    "        else:\n",
    "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.uint8)\n",
    "        dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
    "        return buffer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Cannot read dataset: {e}\")\n",
    "\n",
    "def load_actions(dataset):\n",
    "    \"\"\"Load actions from HDF5 dataset.\"\"\"\n",
    "    shape = dataset.shape\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float32)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float64)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Cannot read actions: {e}\")\n",
    "\n",
    "\n",
    "def explore_libero_dataset(data_path: Path):\n",
    "    \"\"\"Explore LIBERO dataset structure and display sample frames.\"\"\"\n",
    "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
    "    \n",
    "    if not hdf5_files:\n",
    "        print(f\"‚ö†Ô∏è No HDF5 files found in {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(hdf5_files)} HDF5 files\")\n",
    "    \n",
    "    demo_file = hdf5_files[0]\n",
    "    print(f\"\\nüìÑ Analyzing: {demo_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(demo_file, 'r') as f:\n",
    "            if 'data' not in f:\n",
    "                print(\"‚ö†Ô∏è Key 'data' not found\")\n",
    "                return hdf5_files\n",
    "            \n",
    "            data_group = f['data']\n",
    "            demo_keys = list(data_group.keys())\n",
    "            first_demo_key = demo_keys[0]\n",
    "            demo_0 = data_group[first_demo_key]\n",
    "            \n",
    "            imgs = None\n",
    "            \n",
    "            if 'obs' in demo_0:\n",
    "                obs_group = demo_0['obs']\n",
    "                \n",
    "                image_keys = ['agentview_rgb', 'agentview_image', 'rgb', 'image', 'robot0_eye_in_hand_image']\n",
    "                img_key = next((k for k in image_keys if k in obs_group), None)\n",
    "                \n",
    "                if img_key is None:\n",
    "                    img_key = next((k for k in obs_group.keys() if 'rgb' in k.lower() or 'image' in k.lower()), None)\n",
    "                \n",
    "                if img_key:\n",
    "                    print(f\"\\nüñºÔ∏è Using image key: '{img_key}'\")\n",
    "                    try:\n",
    "                        imgs = load_images(obs_group[img_key])\n",
    "                        print(f\"  ‚úÖ Images loaded: {imgs.shape}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Image error: {e}\")\n",
    "            \n",
    "            if 'actions' in demo_0:\n",
    "                try:\n",
    "                    actions = load_actions(demo_0['actions'])\n",
    "                    print(f\"\\nüéÆ Actions loaded: {actions.shape}\")\n",
    "                    print(f\"  Range: [{actions.min():.3f}, {actions.max():.3f}]\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Actions error: {e}\")\n",
    "\n",
    "            if imgs is not None and len(imgs) > 0:\n",
    "                print(\"\\nüé¨ Sample frames:\")\n",
    "                \n",
    "                num_frames = min(4, len(imgs))\n",
    "                indices = np.linspace(0, len(imgs) - 1, num_frames, dtype=int)\n",
    "                \n",
    "                for idx in indices:\n",
    "                    img_array = imgs[idx]\n",
    "                    \n",
    "                    if img_array.dtype != np.uint8:\n",
    "                         img_array = (np.clip(img_array, 0, 1) * 255).astype(np.uint8)\n",
    "                    \n",
    "                    pil_img = Image.fromarray(img_array)\n",
    "                    \n",
    "                    print(f\"--- Frame {idx} ---\")\n",
    "                    display(pil_img)\n",
    "            else:\n",
    "                print(\"\\n‚ö†Ô∏è No valid images to display\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error opening file: {e}\")\n",
    "    \n",
    "    return hdf5_files\n",
    "\n",
    "hdf5_files = explore_libero_dataset(Path('dataset/libero_spatial'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIBERODataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for LIBERO demonstrations.\n",
    "    \n",
    "    Loads visual observations and actions from HDF5 files.\n",
    "    Supports data augmentation and normalization.\n",
    "    Uses demo-level split: splits demos within each file (e.g., 80% train, 20% val).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf5_files: List[Path],\n",
    "        sequence_length: int = 1,\n",
    "        image_size: Tuple[int, int] = (128, 128),\n",
    "        normalize_actions: bool = True,\n",
    "        augmentation: bool = False,\n",
    "        max_demos_per_task: Optional[int] = None,\n",
    "        demo_split_ratio: float = 0.8,\n",
    "        is_train: bool = True,\n",
    "        action_stats: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hdf5_files: list of paths to HDF5 files\n",
    "            sequence_length: sequence length (1 = single-step prediction)\n",
    "            image_size: image dimensions\n",
    "            normalize_actions: if True, normalize actions with z-score\n",
    "            augmentation: if True, apply data augmentation\n",
    "            max_demos_per_task: max demos per task (for debugging)\n",
    "            demo_split_ratio: percentage of demos for training (default 0.8 = 80%)\n",
    "            is_train: if True, use first demo_split_ratio% demos; otherwise use rest\n",
    "            action_stats: pre-computed action statistics (for validation set)\n",
    "        \"\"\"\n",
    "        self.hdf5_files = hdf5_files\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_size = (int(image_size[0]), int(image_size[1]))\n",
    "        self.augmentation = augmentation and is_train\n",
    "        self.normalize_actions = normalize_actions\n",
    "        self.demo_split_ratio = demo_split_ratio\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.data = []\n",
    "        self.action_stats = action_stats if action_stats is not None else {'mean': None, 'std': None}\n",
    "        self.samples: List[Tuple[int, int]] = []  \n",
    "        \n",
    "        split_name = \"TRAIN\" if is_train else \"VAL\"\n",
    "        print(f\"Loading {len(hdf5_files)} HDF5 files for {split_name} (demo split: {demo_split_ratio:.0%})...\")\n",
    "        all_actions = []\n",
    "        \n",
    "        for hdf5_file in hdf5_files:\n",
    "            try:\n",
    "                with h5py.File(hdf5_file, 'r') as f:\n",
    "                    if 'data' not in f:\n",
    "                        print(f\"‚ö†Ô∏è 'data' key not found in {hdf5_file.name}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    demo_keys = list(f['data'].keys())\n",
    "                    \n",
    "                    if max_demos_per_task is not None:\n",
    "                        demo_keys = demo_keys[:max_demos_per_task]\n",
    "                    \n",
    "                    n_demos = len(demo_keys)\n",
    "                    n_train_demos = int(n_demos * demo_split_ratio)\n",
    "                    \n",
    "                    if is_train:\n",
    "                        selected_demo_keys = demo_keys[:n_train_demos]\n",
    "                    else:\n",
    "                        selected_demo_keys = demo_keys[n_train_demos:]\n",
    "                    \n",
    "                    if len(selected_demo_keys) == 0:\n",
    "                        print(f\"‚ö†Ô∏è No demos selected from {hdf5_file.name} for {split_name}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    task_prompt = self._prompt_from_filename(hdf5_file)\n",
    "\n",
    "                    for demo_key in selected_demo_keys:\n",
    "                        try:\n",
    "                            demo = f[f'data/{demo_key}']\n",
    "                            \n",
    "                            obs_group = demo['obs']\n",
    "                            img_key = self._find_image_key(obs_group)\n",
    "                            \n",
    "                            if img_key is None:\n",
    "                                print(f\"‚ö†Ô∏è No image key found in {hdf5_file.name}/{demo_key}, skipping...\")\n",
    "                                continue\n",
    "                            \n",
    "                            obs = self._load_images(obs_group[img_key])\n",
    "                            actions = self._load_actions(demo['actions'])\n",
    "                            \n",
    "                            min_len = min(len(obs), len(actions))\n",
    "                            if min_len < self.sequence_length:\n",
    "                                print(f\"‚ö†Ô∏è Demo too short ({min_len} < {self.sequence_length}), skipping...\")\n",
    "                                continue\n",
    "                            \n",
    "                            obs = obs[:min_len]\n",
    "                            actions = actions[:min_len]\n",
    "                            \n",
    "                            self.data.append({\n",
    "                                'observations': obs,\n",
    "                                'actions': actions,\n",
    "                                'prompt': task_prompt\n",
    "                            })\n",
    "                            \n",
    "                            all_actions.append(actions)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è Error loading demo {demo_key} from {hdf5_file.name}: {e}\")\n",
    "                            continue\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error opening file {hdf5_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.data)} demonstrations for {split_name}\")\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(f\"No valid demonstrations loaded for {split_name}!\")\n",
    "        \n",
    "        if self.normalize_actions and len(all_actions) > 0 and action_stats is None:\n",
    "            all_actions_concat = np.concatenate(all_actions, axis=0)\n",
    "        \n",
    "            mean = all_actions_concat.mean(axis=0).astype(np.float32)\n",
    "            std  = all_actions_concat.std(axis=0).astype(np.float32)\n",
    "            std_clipped = np.clip(std, 0.1, None)\n",
    "        \n",
    "            print(f\"üìä Action statistics computed from {split_name} set:\")\n",
    "            print(f\"   Mean: {np.round(mean, 3)}\")\n",
    "            print(f\"   Std (clipped to >=0.1): {np.round(std_clipped, 3)}\")\n",
    "        \n",
    "            self.action_stats['mean'] = mean\n",
    "            self.action_stats['std']  = std_clipped\n",
    "        \n",
    "        elif action_stats is not None:\n",
    "            print(f\"üìä Using provided action statistics\")\n",
    "            self.action_stats = {\n",
    "                'mean': action_stats['mean'].astype(np.float32),\n",
    "                'std':  np.clip(action_stats['std'], 0.1, None).astype(np.float32)\n",
    "            }\n",
    "\n",
    "        self.samples = self._build_sample_index()\n",
    "        print(f\"üì¶ Generated {len(self.samples)} transitions for {split_name}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _prompt_from_filename(hdf5_file: Path) -> str:\n",
    "        \"\"\"Convert HDF5 filename to natural language prompt.\"\"\"\n",
    "        name = hdf5_file.stem\n",
    "        if name.endswith('_demo'):\n",
    "            name = name[:-5]\n",
    "        name = name.replace('_', ' ').replace('-', ' ')\n",
    "        return ' '.join(name.split()).strip()\n",
    "\n",
    "    \n",
    "    def _find_image_key(self, obs_group) -> Optional[str]:\n",
    "        \"\"\"Find correct image key in observation group.\"\"\"\n",
    "        possible_keys = [\n",
    "            'agentview_rgb',\n",
    "            'agentview_image', \n",
    "            'rgb',\n",
    "            'image',\n",
    "            'robot0_eye_in_hand_image',\n",
    "            'frontview_image',\n",
    "            'sideview_image'\n",
    "        ]\n",
    "        \n",
    "        obs_keys = list(obs_group.keys())\n",
    "        \n",
    "        for key in possible_keys:\n",
    "            if key in obs_keys:\n",
    "                return key\n",
    "        \n",
    "        for key in obs_keys:\n",
    "            if 'rgb' in key.lower() or 'image' in key.lower():\n",
    "                return key\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _load_images(self, dataset) -> np.ndarray:\n",
    "        \"\"\"Load images from HDF5 dataset with robust dtype handling.\"\"\"\n",
    "        shape = dataset.shape\n",
    "        \n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.uint8)\n",
    "            dataset.read_direct(buffer)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float32)\n",
    "            dataset.read_direct(buffer)\n",
    "            if buffer.max() <= 1.0:\n",
    "                buffer = (buffer * 255).astype(np.uint8)\n",
    "            else:\n",
    "                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float64)\n",
    "            dataset.read_direct(buffer)\n",
    "            if buffer.max() <= 1.0:\n",
    "                buffer = (buffer * 255).astype(np.uint8)\n",
    "            else:\n",
    "                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.uint8)\n",
    "            dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Cannot read image dataset: {e}\")\n",
    "    \n",
    "    def _load_actions(self, dataset) -> np.ndarray:\n",
    "        \"\"\"Load actions from HDF5 dataset.\"\"\"\n",
    "        shape = dataset.shape\n",
    "        \n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float32)\n",
    "            dataset.read_direct(buffer)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float64)\n",
    "            dataset.read_direct(buffer)\n",
    "            return buffer.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Cannot read actions dataset: {e}\")\n",
    "    \n",
    "    def _build_sample_index(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Pre-compute (demo_idx, start_idx) indices for each transition.\"\"\"\n",
    "        indices: List[Tuple[int, int]] = []\n",
    "        for demo_idx, demo in enumerate(self.data):\n",
    "            demo_transitions = len(demo['observations']) - self.sequence_length + 1\n",
    "            if demo_transitions <= 0:\n",
    "                continue\n",
    "            indices.extend((demo_idx, start) for start in range(demo_transitions))\n",
    "        if not indices:\n",
    "            raise ValueError(\"Dataset index is empty after preprocessing\")\n",
    "        return indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Return a transition (observation, action).\"\"\"\n",
    "        demo_idx, start_idx = self.samples[idx]\n",
    "        demo = self.data[demo_idx]\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "\n",
    "        obs = demo['observations'][start_idx:end_idx].copy()\n",
    "        actions = demo['actions'][start_idx:end_idx].copy()\n",
    "\n",
    "        obs = self._preprocess_obs(obs)\n",
    "        actions = self._preprocess_actions(actions)\n",
    "\n",
    "        if self.sequence_length == 1:\n",
    "            obs = obs[0]\n",
    "            actions = actions[0]\n",
    "\n",
    "        # Convert HWC -> CHW for PyTorch\n",
    "        if obs.ndim == 3:\n",
    "            obs = np.transpose(obs, (2, 0, 1))\n",
    "        elif obs.ndim == 4:\n",
    "            obs = np.transpose(obs, (0, 3, 1, 2))\n",
    "\n",
    "        return {\n",
    "            'observations': torch.from_numpy(obs).float(),\n",
    "            'actions': torch.from_numpy(actions).float(),\n",
    "            'prompt': demo.get('prompt', '')\n",
    "        }\n",
    "    \n",
    "    def _preprocess_obs(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess observations.\"\"\"\n",
    "        processed = []\n",
    "        target_h, target_w = self.image_size\n",
    "        for img in obs:\n",
    "            if img.shape[0] != target_h or img.shape[1] != target_w:\n",
    "                img = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "            processed.append(img)\n",
    "        obs = np.stack(processed, axis=0)\n",
    "\n",
    "        obs = obs.astype(np.float32) / 255.0\n",
    "\n",
    "        if self.augmentation:\n",
    "            obs = self._augment_obs(obs)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def _augment_obs(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply data augmentation to observations.\"\"\"\n",
    "        if np.random.rand() < 0.5:\n",
    "            brightness = np.random.uniform(0.8, 1.2)\n",
    "            obs = np.clip(obs * brightness, 0, 1)\n",
    "        \n",
    "        if np.random.rand() < 0.3:\n",
    "            contrast = np.random.uniform(0.8, 1.2)\n",
    "            mean = obs.mean(axis=(1, 2), keepdims=True)\n",
    "            obs = np.clip((obs - mean) * contrast + mean, 0, 1)\n",
    "        \n",
    "        if np.random.rand() < 0.3:\n",
    "            crop_ratio = np.random.uniform(0.85, 0.95)\n",
    "            crop_size_h = int(self.image_size[0] * crop_ratio)\n",
    "            crop_size_w = int(self.image_size[1] * crop_ratio)\n",
    "            \n",
    "            start_y = np.random.randint(0, self.image_size[0] - crop_size_h + 1)\n",
    "            start_x = np.random.randint(0, self.image_size[1] - crop_size_w + 1)\n",
    "            \n",
    "            cropped = []\n",
    "            for img in obs:\n",
    "                img_crop = img[start_y:start_y+crop_size_h, start_x:start_x+crop_size_w]\n",
    "                img_resized = cv2.resize(img_crop, (self.image_size[1], self.image_size[0]))\n",
    "                cropped.append(img_resized)\n",
    "            obs = np.stack(cropped)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _preprocess_actions(self, actions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess actions with z-score normalization.\"\"\"\n",
    "        actions = actions.astype(np.float32)\n",
    "        if self.action_stats['mean'] is not None:\n",
    "            actions = (actions - self.action_stats['mean']) / self.action_stats['std']\n",
    "        return actions\n",
    "    \n",
    "    def get_action_stats(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Return action statistics for denormalization.\"\"\"\n",
    "        return self.action_stats.copy()\n",
    "    \n",
    "    def denormalize_actions(self, actions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Denormalize actions for simulator execution.\"\"\"\n",
    "        if self.action_stats['mean'] is not None:\n",
    "            return actions * self.action_stats['std'] + self.action_stats['mean']\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedVisualEncoder(nn.Module):\n",
    "    \"\"\"Visual encoder based on ResNet18 with adaptive head.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int = 256, freeze_backbone: bool = True, dropout: float = 0.1, double_visual_features: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, hidden_dim * 2) if double_visual_features else nn.Linear(512, hidden_dim)\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(hidden_dim * (2 if double_visual_features else 1))\n",
    "\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "\n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = (x - self.mean) / self.std\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        output = self.adapter(features) \n",
    "        return output\n",
    "\n",
    "class PromptEncoder(nn.Module):\n",
    "    \"\"\"Encodes natural-language task prompts via CLIP ViT-L/14 text tower.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        model_name: str = 'openai/clip-vit-large-patch14',\n",
    "        trainable: bool = False,\n",
    "        dropout: float = 0.3,\n",
    "        max_length: int = 77\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "        self.text_model = CLIPTextModel.from_pretrained(model_name)\n",
    "        self.max_length = min(max_length, getattr(self.text_model.config, 'max_position_embeddings', max_length))\n",
    "\n",
    "        if not trainable:\n",
    "            self.text_model.eval()\n",
    "            for param in self.text_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.text_hidden = self.text_model.config.hidden_size\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.LayerNorm(self.text_hidden),\n",
    "            nn.Linear(self.text_hidden, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self._token_cache: Dict[str, Dict[str, torch.Tensor]] = {}\n",
    "\n",
    "    def _tokenize(self, prompt: str) -> Dict[str, torch.Tensor]:\n",
    "        if prompt not in self._token_cache:\n",
    "            tokens = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            self._token_cache[prompt] = {k: v for k, v in tokens.items()}\n",
    "        cached = self._token_cache[prompt]\n",
    "        return {k: v.clone() for k, v in cached.items()}\n",
    "\n",
    "    def forward(self, prompts: List[str], device: torch.device) -> torch.Tensor:\n",
    "        if len(prompts) == 0:\n",
    "            raise ValueError(\"PromptEncoder received an empty batch of prompts\")\n",
    "\n",
    "        token_batches = [self._tokenize(p) for p in prompts]\n",
    "        batch = {\n",
    "            key: torch.cat([tokens[key] for tokens in token_batches], dim=0).to(device)\n",
    "            for key in token_batches[0]\n",
    "        }\n",
    "\n",
    "        outputs = self.text_model(**batch)\n",
    "        pooled = outputs.pooler_output if outputs.pooler_output is not None else outputs.last_hidden_state[:, -1, :]\n",
    "        return self.adapter(pooled)\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    \"\"\"TRM recursive block with self-attention and MLP.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=256, num_heads=4, dropout=0.1, use_attention: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        if self.use_attention:\n",
    "            self.attention = nn.MultiheadAttention(\n",
    "                hidden_dim,\n",
    "                num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.attention = None\n",
    "            self.dropout = None\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, h, x_cond):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (B, D) current hidden state\n",
    "            x_cond: (B, D) conditioning from input\n",
    "        Returns:\n",
    "            (B, D) new hidden state\n",
    "        \"\"\"\n",
    "        h_in = h.unsqueeze(1)\n",
    "\n",
    "        if self.use_attention:\n",
    "            cond = x_cond.unsqueeze(1)\n",
    "            attn_out, _ = self.attention(h_in, cond, cond)\n",
    "            attn_out = self.dropout(attn_out)\n",
    "            x = self.norm(attn_out + self.mlp(attn_out))\n",
    "        else:\n",
    "            x = self.norm(h_in + self.mlp(h_in))\n",
    "        return x.squeeze(1)\n",
    "\n",
    "class CrossAttentionFusion(nn.Module): \n",
    "    \"\"\"Cross-attention fusion for multimodal features.\"\"\"\n",
    "    \n",
    "    def __init__(self, visual_dim: int, text_dim: int, hidden_dim: int, num_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(visual_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.resid_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.out_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, visual_feats: torch.Tensor, text_feats: torch.Tensor) -> torch.Tensor:\n",
    "        q = self.query_proj(visual_feats).unsqueeze(1)\n",
    "        k = self.key_proj(text_feats).unsqueeze(1)\n",
    "        v = self.value_proj(text_feats).unsqueeze(1)\n",
    "        attn_out, _ = self.attn(q, k, v)\n",
    "        fused = self.resid_norm(q + attn_out)\n",
    "        fused = self.out_norm(fused + self.ffn(fused))\n",
    "        return fused.squeeze(1)\n",
    "\n",
    "class TRMPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy based on Tiny Recursive Models for robotic control.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Visual Encoder: ResNet18 pre-trained for image feature extraction\n",
    "    2. Text Prompt Encoder: CLIP text tower for prompt encoding\n",
    "    3. Recursive Block: applied N times for iterative reasoning\n",
    "    4. Action Head: predicts actions from final hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_shape=(3, 128, 128),\n",
    "        action_dim=7,\n",
    "        hidden_dim=256,\n",
    "        num_heads=4,\n",
    "        num_recursions=8,\n",
    "        dropout=0.1,\n",
    "        freeze_backbone=True,\n",
    "        encoder_dropout=0.1,\n",
    "        use_text_prompts=True,\n",
    "        text_encoder_name='openai/clip-vit-large-patch14',\n",
    "        train_text_encoder=False,\n",
    "        text_dropout=0.1,\n",
    "        double_visual_features=False,\n",
    "        use_attention: bool = True,\n",
    "        attention_fusion: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_recursions = num_recursions\n",
    "        self.obs_shape = obs_shape\n",
    "        self.use_text_prompts = use_text_prompts\n",
    "        self.use_attention = use_attention\n",
    "        self.attention_fusion = attention_fusion and use_text_prompts\n",
    "\n",
    "        self.encoder = PretrainedVisualEncoder(\n",
    "            hidden_dim=hidden_dim,\n",
    "            freeze_backbone=freeze_backbone,\n",
    "            dropout=encoder_dropout,\n",
    "            double_visual_features=double_visual_features\n",
    "        )\n",
    "        \n",
    "        visual_dim = hidden_dim * (2 if double_visual_features else 1)\n",
    "        self.fusion_adapter: nn.Module\n",
    "        if self.use_text_prompts:\n",
    "            self.prompt_encoder = PromptEncoder(\n",
    "                hidden_dim=hidden_dim,\n",
    "                model_name=text_encoder_name,\n",
    "                trainable=train_text_encoder,\n",
    "                dropout=text_dropout\n",
    "            )\n",
    "            if self.attention_fusion:\n",
    "                self.fusion_adapter = CrossAttentionFusion(\n",
    "                    visual_dim=visual_dim,\n",
    "                    text_dim=hidden_dim,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            else:\n",
    "                fusion_in = visual_dim + hidden_dim\n",
    "                self.fusion_adapter = nn.Sequential(\n",
    "                    nn.LayerNorm(fusion_in),\n",
    "                    nn.Linear(fusion_in, hidden_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "        else:\n",
    "            self.prompt_encoder = None\n",
    "            fusion_in = visual_dim \n",
    "            if fusion_in != hidden_dim:\n",
    "                self.fusion_adapter = nn.Sequential(\n",
    "                    nn.LayerNorm(fusion_in),\n",
    "                    nn.Linear(fusion_in, hidden_dim),\n",
    "                    nn.GELU(),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            else:\n",
    "                self.fusion_adapter = nn.Identity()\n",
    "\n",
    "        self.recursive_block = RecursiveBlock(hidden_dim, num_heads, dropout, use_attention=self.use_attention)\n",
    "        \n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs, prompts: Optional[List[str]] = None, return_all_states=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs: (B, C, H, W) visual observations\n",
    "            prompts: list of strings (B) with task descriptions\n",
    "            return_all_states: if True, return all hidden states\n",
    "        Returns:\n",
    "            actions: (B, action_dim) predicted actions\n",
    "            (optional) states: list of hidden states\n",
    "        \"\"\"\n",
    "        B = obs.shape[0]\n",
    "        \n",
    "        visual_features = self.encoder(obs)\n",
    "\n",
    "        if self.use_text_prompts:\n",
    "            text_features = self.prompt_encoder(prompts, device=obs.device)\n",
    "            if self.attention_fusion:\n",
    "                x_cond = self.fusion_adapter(visual_features, text_features)\n",
    "            else:\n",
    "                x_cond = self.fusion_adapter(torch.cat([visual_features, text_features], dim=-1))\n",
    "        else:\n",
    "            x_cond = self.fusion_adapter(visual_features)\n",
    "        h = x_cond.clone()\n",
    "        \n",
    "        states = [h] if return_all_states else None\n",
    "\n",
    "        for t in range(self.num_recursions):\n",
    "            h = self.recursive_block(h, x_cond)\n",
    "            if return_all_states:\n",
    "                states.append(h)\n",
    "        \n",
    "        actions = self.action_head(h)\n",
    "        \n",
    "        if return_all_states:\n",
    "            return actions, states\n",
    "        return actions\n",
    "    \n",
    "def build_policy_from_config(config: TrainingConfig, obs_shape: Tuple[int, int, int] = (3, 128, 128)) -> TRMPolicy:\n",
    "    \"\"\"Build a TRMPolicy from TrainingConfig.\"\"\"\n",
    "\n",
    "    return TRMPolicy(\n",
    "        obs_shape=obs_shape,\n",
    "        action_dim=7,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        num_recursions=config.num_recursions,\n",
    "        dropout=config.dropout,\n",
    "        freeze_backbone=config.freeze_backbone,\n",
    "        encoder_dropout=config.encoder_dropout,\n",
    "        use_text_prompts=config.use_text_prompts,\n",
    "        text_encoder_name=config.text_encoder_name,\n",
    "        train_text_encoder=config.train_text_encoder,\n",
    "        text_dropout=config.text_dropout,\n",
    "        double_visual_features=config.double_visual_features,\n",
    "        use_attention=getattr(config, 'use_attention', True),\n",
    "        attention_fusion=getattr(config, 'attention_fusion', False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorCloningTrainer:\n",
    "    \"\"\"Trainer for Behavior Cloning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        config: TrainingConfig,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.steps_per_epoch = max(len(train_loader), 1)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.lr,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "\n",
    "        self.scheduler = None\n",
    "        \n",
    "        self.use_amp = (self.device.type == 'cuda')\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
    "        self.grad_clip = config.grad_clip\n",
    "        self.early_stop_patience = config.early_stop_patience\n",
    "        self._epochs_no_improve = 0\n",
    "        self.early_stop_patience = 5 if config.early_stop_patience is None else config.early_stop_patience\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_path = config.save_path\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Complete training loop.\"\"\"\n",
    "        \n",
    "        for epoch in range(self.config.epochs):\n",
    "            train_metrics = self._train_epoch(epoch)\n",
    "            val_metrics = self._validate_epoch(epoch)\n",
    "            print(f\"Epoch {epoch}, training loss: {train_metrics['loss']:.4f}, validation loss: {val_metrics['loss']:.4f}\")\n",
    "            \n",
    "            if val_metrics['loss'] < self.best_val_loss:\n",
    "                self.best_val_loss = val_metrics['loss']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_metrics['loss'],\n",
    "                    'config': self.config.to_dict()\n",
    "                }, self.best_model_path)\n",
    "                print(f\"  ‚úì Saved best model (val_loss: {val_metrics['loss']:.4f})\")\n",
    "                self._epochs_no_improve = 0\n",
    "            else:\n",
    "                self._epochs_no_improve += 1\n",
    "\n",
    "            if self.early_stop_patience and self._epochs_no_improve >= self.early_stop_patience:\n",
    "                print(\"‚èπÔ∏è  Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed! Best val loss: {self.best_val_loss:.4f}\")\n",
    "        return self.best_val_loss\n",
    "    \n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"Training for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        action_mse = 0\n",
    "        action_l1 = 0\n",
    "        \n",
    "        for step, batch in enumerate(self.train_loader):\n",
    "            obs = batch['observations'].to(self.device, non_blocking=True)\n",
    "            target_actions = batch['actions'].to(self.device, non_blocking=True)\n",
    "            prompts = batch.get('prompt')\n",
    "\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "                pred_actions = self.model(obs, prompts=prompts)\n",
    "                mse = F.mse_loss(pred_actions, target_actions)\n",
    "                l1 = F.l1_loss(pred_actions, target_actions)\n",
    "                loss = 0.7 * mse + 0.3 * l1\n",
    "\n",
    "            if self.use_amp:\n",
    "                self.scaler.scale(loss).backward()\n",
    "                if self.grad_clip:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if self.grad_clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            action_mse += mse.item()\n",
    "            action_l1 += l1.item()\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        n_batches = len(self.train_loader)\n",
    "        return {\n",
    "            'loss': total_loss / n_batches,\n",
    "            'action_mse': action_mse / n_batches,\n",
    "            'action_l1': action_l1 / n_batches\n",
    "        }\n",
    "    \n",
    "    def _validate_epoch(self, epoch):\n",
    "        \"\"\"Validation for one epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        action_mse = 0\n",
    "        action_l1 = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                obs = batch['observations'].to(self.device)\n",
    "                target_actions = batch['actions'].to(self.device)\n",
    "                prompts = batch.get('prompt')\n",
    "                \n",
    "                pred_actions = self.model(obs, prompts=prompts)\n",
    "                \n",
    "                mse = F.mse_loss(pred_actions, target_actions)\n",
    "                l1 = F.l1_loss(pred_actions, target_actions)\n",
    "                loss = 0.7 * mse + 0.3 * l1\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                action_mse += mse.item()\n",
    "                action_l1 += l1.item()\n",
    "        \n",
    "        n_batches = len(self.val_loader)\n",
    "        return {\n",
    "            'loss': total_loss / n_batches,\n",
    "            'action_mse': action_mse / n_batches,\n",
    "            'action_l1': action_l1 / n_batches\n",
    "        }\n",
    "\n",
    "\n",
    "def build_dataloaders(\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    loader_kwargs: Dict[str, Any]\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Create data loaders from datasets.\"\"\"\n",
    "\n",
    "    kwargs = loader_kwargs.copy()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def _set_dataset_augmentation(dataset, flag: bool):\n",
    "    \"\"\"Temporarily set augmentation and return restore function.\"\"\"\n",
    "\n",
    "    if not hasattr(dataset, 'augmentation'):\n",
    "        return lambda: None\n",
    "\n",
    "    original = dataset.augmentation\n",
    "    dataset.augmentation = flag\n",
    "\n",
    "    def restore():\n",
    "        dataset.augmentation = original\n",
    "\n",
    "    return restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    config: TrainingConfig,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    loader_kwargs: Dict[str, Any],\n",
    "    device\n",
    ") -> Tuple[nn.Module, float]:\n",
    "    \"\"\"Execute model training with specified configuration.\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üéØ MODEL TRAINING\")\n",
    "    print(f\"Config: {config.label()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    train_loader, val_loader = build_dataloaders(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        config.batch_size,\n",
    "        loader_kwargs\n",
    "    )\n",
    "\n",
    "    model = build_policy_from_config(config)\n",
    "    trainer = BehaviorCloningTrainer(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        config,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    restore_aug = _set_dataset_augmentation(train_dataset, config.augmentation)\n",
    "    try:\n",
    "        final_val_loss = trainer.train()\n",
    "    finally:\n",
    "        restore_aug()\n",
    "\n",
    "    if os.path.exists(config.save_path):\n",
    "        checkpoint = torch.load(config.save_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"\\n‚úÖ Model trained! Val loss: {final_val_loss:.4f}\")\n",
    "\n",
    "    return model, final_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(\n",
    "    data_path: str = 'dataset/libero_spatial',\n",
    "    train: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Main pipeline for TRM Robotics project.\n",
    "    \n",
    "    Args:\n",
    "        data_path: path to LIBERO data\n",
    "        train: if True, execute model training\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    {'='*80}\n",
    "    ü§ñ TinyRecursiveModels for Robotic Control\n",
    "    {'='*80}\n",
    "    \"\"\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"‚úì Using device: {device}\\n\")\n",
    "    \n",
    "    # STEP 1: Load Dataset\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 1: Loading Dataset\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    data_path = Path(data_path)\n",
    "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
    "    \n",
    "    if not hdf5_files:\n",
    "        print(f\"‚ùå No HDF5 files found in {data_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"‚úì Found {len(hdf5_files)} HDF5 files (tasks)\")\n",
    "    \n",
    "    demo_split_ratio = 0.8\n",
    "    print(f\"\\nüìä Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per task\")\n",
    "    \n",
    "    print(\"\\nCreating TRAIN dataset...\")\n",
    "    train_dataset = LIBERODataset(\n",
    "        hdf5_files,\n",
    "        sequence_length=1,\n",
    "        image_size=(128, 128),\n",
    "        augmentation=False,\n",
    "        max_demos_per_task=50,\n",
    "        demo_split_ratio=demo_split_ratio,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    train_action_stats = train_dataset.action_stats\n",
    "    \n",
    "    print(\"\\nCreating VAL dataset...\")\n",
    "    val_dataset = LIBERODataset(\n",
    "        hdf5_files,\n",
    "        sequence_length=1,\n",
    "        image_size=(128, 128),\n",
    "        augmentation=False,\n",
    "        max_demos_per_task=50,\n",
    "        demo_split_ratio=demo_split_ratio,\n",
    "        is_train=False,\n",
    "        action_stats=train_action_stats\n",
    "    )\n",
    "    \n",
    "    num_workers = min(4, os.cpu_count() or 1)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    loader_common = {\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': use_cuda,\n",
    "        'persistent_workers': num_workers > 0\n",
    "    }\n",
    "    if num_workers > 0:\n",
    "        loader_common['prefetch_factor'] = 2\n",
    "\n",
    "    print(f\"\\n‚úì Datasets created with demo-level split\")\n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Val samples: {len(val_dataset)}\")\n",
    "    \n",
    "    action_stats = train_dataset.action_stats\n",
    "    with open('action_stats.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'mean': action_stats['mean'].tolist(),\n",
    "            'std': action_stats['std'].tolist()\n",
    "        }, f)\n",
    "    \n",
    "    # STEP 2: Training\n",
    "    trained_model = None\n",
    "    \n",
    "    if train:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"STEP 2: Training\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        config = TrainingConfig(\n",
    "            lr=0.0001, \n",
    "            hidden_dim=256,\n",
    "            num_recursions=16,\n",
    "            epochs=30,\n",
    "            batch_size=512,\n",
    "            weight_decay=0.1,\n",
    "            grad_clip=1.0,\n",
    "            sched_T0=None,\n",
    "            sched_T_mult=1,\n",
    "            lr_min=1e-06,\n",
    "            warmup_epochs=3,\n",
    "            early_stop_patience=10,  \n",
    "            freeze_backbone=False,\n",
    "            augmentation=True,\n",
    "            dropout=0.1,\n",
    "            encoder_dropout=0.1,\n",
    "            use_text_prompts=True,\n",
    "            text_encoder_name=\"openai/clip-vit-large-patch14\",\n",
    "            train_text_encoder=False,\n",
    "            text_dropout=0.1,\n",
    "            double_visual_features=True,\n",
    "            use_attention=False,\n",
    "            attention_fusion=False,\n",
    "            save_path='models/model.pt'\n",
    "        )\n",
    "\n",
    "        trained_model, final_val_loss = train_model(\n",
    "            config,\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            loader_common,\n",
    "            device\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ Pipeline completed!\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipeline(train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualExplainer:\n",
    "    \"\"\"\n",
    "    Computes saliency maps using gradient-based methods.\n",
    "    \n",
    "    Available methods:\n",
    "    - Vanilla Gradient\n",
    "    - SmoothGrad\n",
    "    - GradCAM\n",
    "    - Integrated Gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register hooks for GradCAM.\"\"\"\n",
    "        def forward_hook(module, input, output):\n",
    "            self.feature_maps = output.detach()\n",
    "        \n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        \n",
    "        if hasattr(self.model, 'encoder') and hasattr(self.model.encoder, 'backbone'):\n",
    "            backbone = self.model.encoder.backbone\n",
    "            for name, module in backbone.named_modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    self.last_conv = module\n",
    "            \n",
    "            if hasattr(self, 'last_conv'):\n",
    "                self.last_conv.register_forward_hook(forward_hook)\n",
    "                self.last_conv.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    def compute_saliency(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        prompt: str,\n",
    "        method: str = 'gradcam'\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Compute saliency map with specified method.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        if method == 'vanilla':\n",
    "            saliency = self._vanilla_gradient(obs, prompt)\n",
    "        elif method == 'smoothgrad':\n",
    "            saliency = self._smoothgrad(obs, prompt)\n",
    "        elif method == 'gradcam':\n",
    "            saliency = self._gradcam(obs, prompt)\n",
    "        elif method == 'integrated':\n",
    "            saliency = self._integrated_gradients(obs, prompt)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        saliency = saliency - saliency.min()\n",
    "        if saliency.max() > 0:\n",
    "            saliency = saliency / saliency.max()\n",
    "        \n",
    "        return saliency\n",
    "    \n",
    "    def compute_all_methods(self, obs: torch.Tensor, prompt: str) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute saliency with all available methods.\"\"\"\n",
    "        methods = ['vanilla', 'smoothgrad', 'gradcam', 'integrated']\n",
    "        results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            try:\n",
    "                results[method] = self.compute_saliency(obs, prompt, method=method)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error computing {method}: {e}\")\n",
    "                results[method] = np.zeros((obs.shape[2], obs.shape[3]), dtype=np.float32)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _forward_with_grad(self, obs: torch.Tensor, prompt: str) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with gradients.\"\"\"\n",
    "        obs = obs.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        if hasattr(self.model, 'use_text_prompts') and self.model.use_text_prompts:\n",
    "            actions = self.model(obs, [prompt])\n",
    "        else:\n",
    "            actions = self.model(obs, None)\n",
    "        \n",
    "        loss = actions.norm()\n",
    "        loss.backward()\n",
    "        \n",
    "        return obs.grad\n",
    "    \n",
    "    def _vanilla_gradient(self, obs: torch.Tensor, prompt: str) -> np.ndarray:\n",
    "        grad = self._forward_with_grad(obs, prompt)\n",
    "        saliency = grad.abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "        return gaussian_filter(saliency, sigma=2)\n",
    "    \n",
    "    def _smoothgrad(self, obs: torch.Tensor, prompt: str, n_samples: int = 20, noise: float = 0.1) -> np.ndarray:\n",
    "        saliency_sum = None\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            noisy_obs = obs + torch.randn_like(obs) * noise\n",
    "            grad = self._forward_with_grad(noisy_obs, prompt)\n",
    "            saliency = grad.abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            if saliency_sum is None:\n",
    "                saliency_sum = saliency\n",
    "            else:\n",
    "                saliency_sum += saliency\n",
    "        \n",
    "        return gaussian_filter(saliency_sum / n_samples, sigma=2)\n",
    "    \n",
    "    def _gradcam(self, obs: torch.Tensor, prompt: str) -> np.ndarray:\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        obs_grad = obs.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        if hasattr(self.model, 'use_text_prompts') and self.model.use_text_prompts:\n",
    "            actions = self.model(obs_grad, [prompt])\n",
    "        else:\n",
    "            actions = self.model(obs_grad, None)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        actions.norm().backward()\n",
    "        \n",
    "        if self.feature_maps is not None and self.gradients is not None:\n",
    "            weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "            cam = (weights * self.feature_maps).sum(dim=1).squeeze()\n",
    "            cam = F.relu(cam)\n",
    "            cam = cam.cpu().numpy()\n",
    "            cam = cv2.resize(cam, (obs.shape[3], obs.shape[2]))\n",
    "            return gaussian_filter(cam, sigma=3)\n",
    "        else:\n",
    "            return self._vanilla_gradient(obs, prompt)\n",
    "    \n",
    "    def _integrated_gradients(self, obs: torch.Tensor, prompt: str, steps: int = 50) -> np.ndarray:\n",
    "        baseline = torch.zeros_like(obs)\n",
    "        grads_sum = None\n",
    "        \n",
    "        for i in range(1, steps + 1):\n",
    "            scaled_input = baseline + (float(i) / steps) * (obs - baseline)\n",
    "            grad = self._forward_with_grad(scaled_input, prompt)\n",
    "            grad_np = grad.abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            if grads_sum is None:\n",
    "                grads_sum = grad_np\n",
    "            else:\n",
    "                grads_sum += grad_np\n",
    "        \n",
    "        avg_grads = grads_sum / steps\n",
    "        integrated = avg_grads * (obs - baseline).abs().sum(dim=1).squeeze().cpu().numpy()\n",
    "        return gaussian_filter(integrated, sigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap_video(\n",
    "    frames_data: List[Dict],\n",
    "    original_frames: List[np.ndarray],\n",
    "    output_path: str,\n",
    "    fps: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a video with 2x3 grid containing all saliency methods.\n",
    "    \n",
    "    Grid layout:\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ    Original     ‚îÇ Vanilla Gradient‚îÇ    SmoothGrad   ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "    ‚îÇ    Grad-CAM     ‚îÇ   Integrated    ‚îÇ   Step Info     ‚îÇ\n",
    "    ‚îÇ                 ‚îÇ   Gradients     ‚îÇ                 ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\"\n",
    "    if len(frames_data) == 0 or len(original_frames) == 0:\n",
    "        print(\"‚ö†Ô∏è No frames to generate video\")\n",
    "        return\n",
    "    \n",
    "    n_frames = min(len(frames_data), len(original_frames))\n",
    "    print(f\"üìπ Generating heatmap video with {n_frames} frames...\")\n",
    "    \n",
    "    title_height = 25\n",
    "    h, w = original_frames[0].shape[:2]\n",
    "    panel_h = h + title_height\n",
    "    panel_w = w\n",
    "    grid_h = panel_h * 2\n",
    "    grid_w = panel_w * 3\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (grid_w, grid_h))\n",
    "    \n",
    "    if not out.isOpened():\n",
    "        print(f\"‚ùå Failed to open VideoWriter for {output_path}\")\n",
    "        return\n",
    "    \n",
    "    method_titles = {\n",
    "        'original': 'Original Input',\n",
    "        'vanilla': 'Vanilla Gradient',\n",
    "        'smoothgrad': 'SmoothGrad',\n",
    "        'gradcam': 'Grad-CAM',\n",
    "        'integrated': 'Integrated Grad',\n",
    "        'info': 'Step Info'\n",
    "    }\n",
    "    \n",
    "    def add_title_bar(img: np.ndarray, title: str) -> np.ndarray:\n",
    "        \"\"\"Add title bar to image.\"\"\"\n",
    "        result = np.zeros((img.shape[0] + title_height, img.shape[1], 3), dtype=np.uint8)\n",
    "        result[:title_height, :] = (40, 40, 40)\n",
    "        result[title_height-3:title_height, :] = (100, 100, 255)\n",
    "        result[title_height:, :] = img\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text_size = cv2.getTextSize(title, font, 0.4, 1)[0]\n",
    "        text_x = (img.shape[1] - text_size[0]) // 2\n",
    "        cv2.putText(result, title, (text_x, 17), font, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        return result\n",
    "    \n",
    "    def create_heatmap_overlay(frame: np.ndarray, saliency: np.ndarray, alpha: float = 0.4) -> np.ndarray:\n",
    "        \"\"\"Create heatmap overlay on image.\"\"\"\n",
    "        if saliency is None or saliency.size == 0:\n",
    "            return frame.copy()\n",
    "        \n",
    "        saliency = np.array(saliency, dtype=np.float32)\n",
    "        if saliency.shape[:2] != frame.shape[:2]:\n",
    "            saliency = cv2.resize(saliency, (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        if saliency.max() > saliency.min():\n",
    "            saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
    "        \n",
    "        heatmap = (saliency * 255).astype(np.uint8)\n",
    "        heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        return cv2.addWeighted(frame, 1 - alpha, heatmap_colored, alpha, 0)\n",
    "    \n",
    "    def create_info_panel(frame: np.ndarray, step: int) -> np.ndarray:\n",
    "        \"\"\"Create info panel.\"\"\"\n",
    "        panel = np.zeros_like(frame)\n",
    "        panel[:] = (30, 30, 30)\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(panel, f\"Step: {step}\", (10, 40), font, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        return panel\n",
    "    \n",
    "    frames_written = 0\n",
    "    for i in range(n_frames):\n",
    "        try:\n",
    "            frame = original_frames[i].copy()\n",
    "            data = frames_data[i]\n",
    "            \n",
    "            if frame.dtype != np.uint8:\n",
    "                frame = ((frame * 255) if frame.max() <= 1.0 else np.clip(frame, 0, 255)).astype(np.uint8)\n",
    "            \n",
    "            if len(frame.shape) == 2:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "            elif frame.shape[2] == 4:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "            \n",
    "            if frame.shape[0] != h or frame.shape[1] != w:\n",
    "                frame = cv2.resize(frame, (w, h))\n",
    "            \n",
    "            panels = []\n",
    "            \n",
    "            # Row 1: Original, Vanilla, SmoothGrad\n",
    "            panels.append(add_title_bar(frame.copy(), method_titles['original']))\n",
    "            \n",
    "            vanilla = data.get('vanilla', data.get('saliency_map', None))\n",
    "            panels.append(add_title_bar(create_heatmap_overlay(frame, vanilla), method_titles['vanilla']))\n",
    "            \n",
    "            smoothgrad = data.get('smoothgrad', data.get('saliency_map', None))\n",
    "            panels.append(add_title_bar(create_heatmap_overlay(frame, smoothgrad), method_titles['smoothgrad']))\n",
    "            \n",
    "            # Row 2: GradCAM, Integrated, Info\n",
    "            gradcam = data.get('gradcam', data.get('saliency_map', None))\n",
    "            panels.append(add_title_bar(create_heatmap_overlay(frame, gradcam), method_titles['gradcam']))\n",
    "            \n",
    "            integrated = data.get('integrated', data.get('saliency_map', None))\n",
    "            panels.append(add_title_bar(create_heatmap_overlay(frame, integrated), method_titles['integrated']))\n",
    "            \n",
    "            step = data.get('step', i)\n",
    "            panels.append(add_title_bar(create_info_panel(frame, step), method_titles['info']))\n",
    "            \n",
    "            for idx, p in enumerate(panels):\n",
    "                if p.shape[0] != panel_h or p.shape[1] != panel_w:\n",
    "                    panels[idx] = cv2.resize(p, (panel_w, panel_h))\n",
    "            \n",
    "            row1 = np.hstack([panels[0], panels[1], panels[2]])\n",
    "            row2 = np.hstack([panels[3], panels[4], panels[5]])\n",
    "            grid = np.vstack([row1, row2])\n",
    "            \n",
    "            grid_bgr = cv2.cvtColor(grid, cv2.COLOR_RGB2BGR)\n",
    "            out.write(grid_bgr)\n",
    "            frames_written += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing frame {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    out.release()\n",
    "    \n",
    "    if frames_written > 0:\n",
    "        print(f\"‚úÖ Heatmap video saved: {output_path} ({frames_written} frames)\")\n",
    "    else:\n",
    "        print(f\"‚ùå No frames written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib mock to prevent kernel crash due to NumPy/ABI conflicts\n",
    "mock_mpl = MagicMock()\n",
    "sys.modules[\"matplotlib\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.pyplot\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.cm\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.colors\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.transforms\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.ticker\"] = mock_mpl\n",
    "sys.modules[\"matplotlib._path\"] = mock_mpl\n",
    "\n",
    "# LIBERO imports\n",
    "LIBERO_REPO_ROOT = Path('LIBERO')\n",
    "if LIBERO_REPO_ROOT.exists() and str(LIBERO_REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(LIBERO_REPO_ROOT))\n",
    "\n",
    "try:\n",
    "    from robosuite.utils.numba import jit_decorator\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from libero.libero import get_libero_path\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "\n",
    "print(\"‚úì LIBERO imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def _merge_training_config(stored: Dict[str, Any]):\n",
    "    \"\"\"Merge stored config into a simple object.\"\"\"\n",
    "    class ConfigObj:\n",
    "        def __init__(self, **entries): \n",
    "            self.__dict__.update(entries)\n",
    "    return ConfigObj(**stored)\n",
    "\n",
    "def _stack_vector_obs(obs: Any) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Stack observations from vectorized environment.\"\"\"\n",
    "    if isinstance(obs, list):\n",
    "        keys = obs[0].keys()\n",
    "        return {k: np.stack([o[k] for o in obs], axis=0) for k in keys}\n",
    "    return obs\n",
    "\n",
    "def _select_camera_key(obs_batch: Dict[str, np.ndarray]) -> str:\n",
    "    \"\"\"Select appropriate camera key from observation.\"\"\"\n",
    "    for key in ('agentview_rgb', 'agentview_image', 'robot0_agentview_image'):\n",
    "        if key in obs_batch:\n",
    "            return key\n",
    "    return list(obs_batch.keys())[0]\n",
    "\n",
    "def _prepare_policy_input(images: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Prepare images for policy input.\"\"\"\n",
    "    imgs = torch.from_numpy(images).to(device=device, dtype=torch.float32) / 255.0\n",
    "    return imgs.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "\n",
    "class SequentialVectorEnv:\n",
    "    \"\"\"Simple sequential vectorized environment wrapper.\"\"\"\n",
    "    \n",
    "    def __init__(self, env_fns: List[Callable]):\n",
    "        self.envs = [fn() for fn in env_fns]\n",
    "    \n",
    "    def step(self, actions):\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        obs_list, rews_list, dones_list, infos_list = zip(*results)\n",
    "        return list(obs_list), np.array(rews_list), np.array(dones_list), list(infos_list)\n",
    "    \n",
    "    def reset(self):\n",
    "        return [env.reset() for env in self.envs]\n",
    "    \n",
    "    def seed(self, seed):\n",
    "        for i, env in enumerate(self.envs):\n",
    "            if hasattr(env, 'seed'):\n",
    "                env.seed(seed + i)\n",
    "    \n",
    "    def set_init_state(self, states):\n",
    "        return [env.set_init_state(s) for env, s in zip(self.envs, states)]\n",
    "    \n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    checkpoint_path: str = 'models/model.pt',\n",
    "    action_stats_path: str = 'action_stats.json',\n",
    "    benchmark: str = 'libero_spatial',\n",
    "    task_id: int = 0,\n",
    "    env_num: int = 10,\n",
    "    max_steps: int = 600,\n",
    "    seed: int = 42,\n",
    "    save_videos: bool = True,\n",
    "    video_dir: str = 'evaluation_videos',\n",
    "    camera_height: int = 128,\n",
    "    camera_width: int = 128,\n",
    "    video_skip: int = 1,\n",
    "    generate_heatmaps: bool = False,\n",
    "    heatmap_interval: int = 10\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate model on a LIBERO task.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: path to model checkpoint\n",
    "        action_stats_path: path to action statistics\n",
    "        benchmark: benchmark name ('libero_spatial', 'libero_goal', etc.)\n",
    "        task_id: task ID to evaluate\n",
    "        env_num: number of parallel environments\n",
    "        max_steps: maximum steps per episode\n",
    "        seed: random seed\n",
    "        save_videos: whether to save execution videos\n",
    "        video_dir: video output directory\n",
    "        camera_height, camera_width: camera dimensions\n",
    "        video_skip: frame skip for videos\n",
    "        generate_heatmaps: whether to generate saliency heatmap videos\n",
    "        heatmap_interval: sampling interval for heatmaps\n",
    "    \n",
    "    Returns:\n",
    "        Dict with success_rate and other metrics\n",
    "    \"\"\"\n",
    "    print(f\"üîç Evaluating Task {task_id} on {benchmark}...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    cfg = _merge_training_config(ckpt.get('config', {}))\n",
    "    policy = build_policy_from_config(cfg, obs_shape=(3, camera_height, camera_width)).to(device)\n",
    "    policy.load_state_dict(ckpt['model_state_dict'])\n",
    "    policy.eval()\n",
    "    \n",
    "    stats = json.load(open(action_stats_path))\n",
    "    action_mean = torch.tensor(stats['mean'], device=device).unsqueeze(0)\n",
    "    action_std = torch.tensor(stats['std'], device=device).unsqueeze(0)\n",
    "    action_dim = int(action_mean.shape[-1])\n",
    "    \n",
    "    benchmark_map = {\n",
    "        'libero_10': 'LIBERO_10',\n",
    "        'libero_spatial': 'LIBERO_SPATIAL',\n",
    "        'libero_goal': 'LIBERO_GOAL'\n",
    "    }\n",
    "    suite = get_benchmark(benchmark_map.get(benchmark, benchmark))(0)\n",
    "    task = suite.get_task(task_id)\n",
    "    task_prompt = task.language\n",
    "    use_prompts = getattr(policy, 'use_text_prompts', False)\n",
    "    \n",
    "    if use_prompts:\n",
    "        print(f\"   Prompt: '{task_prompt}'\")\n",
    "    \n",
    "    visual_explainer = None\n",
    "    heatmap_frames = []\n",
    "    original_frames = []\n",
    "    \n",
    "    if generate_heatmaps:\n",
    "        visual_explainer = VisualExplainer(policy, device)\n",
    "        print(f\"   Heatmap generation enabled (interval: {heatmap_interval})\")\n",
    "    \n",
    "    env_args = {\n",
    "        'bddl_file_name': str(Path(get_libero_path('bddl_files')) / task.problem_folder / task.bddl_file),\n",
    "        'camera_heights': camera_height,\n",
    "        'camera_widths': camera_width\n",
    "    }\n",
    "    env = SequentialVectorEnv([lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)])\n",
    "    \n",
    "    try:\n",
    "        init_states = torch.load(\n",
    "            str(Path(get_libero_path('init_states')) / task.problem_folder / task.init_states_file),\n",
    "            map_location='cpu', weights_only=False\n",
    "        )\n",
    "        obs = env.reset()\n",
    "        env.seed(seed)\n",
    "        env.set_init_state(init_states[0:env_num])\n",
    "        \n",
    "        dones = [False] * env_num\n",
    "        successes = np.zeros(env_num, dtype=bool)\n",
    "        video_path = Path(video_dir) / f\"task_{task_id:02d}\"\n",
    "        \n",
    "        with VideoWriter(str(video_path), save_videos) as video_writer:\n",
    "            for step in range(max_steps):\n",
    "                obs_batch = _stack_vector_obs(obs)\n",
    "                cam_key = _select_camera_key(obs_batch)\n",
    "                \n",
    "                alive = [i for i, d in enumerate(dones) if not d]\n",
    "                if not alive:\n",
    "                    break\n",
    "                \n",
    "                vis_batch = obs_batch[cam_key][alive]\n",
    "                p_in = _prepare_policy_input(vis_batch, device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    prompt_batch = [task_prompt for _ in alive] if use_prompts else None\n",
    "                    actions_alive = policy(p_in, prompt_batch)\n",
    "                    actions_alive = actions_alive * action_std + action_mean\n",
    "                    full_actions = np.zeros((env_num, action_dim), dtype=np.float32)\n",
    "                    full_actions[alive] = actions_alive.detach().cpu().numpy()\n",
    "                \n",
    "                if visual_explainer is not None and step % heatmap_interval == 0 and len(alive) > 0:\n",
    "                    try:\n",
    "                        single_obs = p_in[0:1]\n",
    "                        saliency_maps = visual_explainer.compute_all_methods(single_obs, task_prompt)\n",
    "                        \n",
    "                        heatmap_frames.append({\n",
    "                            'step': step,\n",
    "                            'vanilla': saliency_maps['vanilla'],\n",
    "                            'smoothgrad': saliency_maps['smoothgrad'],\n",
    "                            'gradcam': saliency_maps['gradcam'],\n",
    "                            'integrated': saliency_maps['integrated']\n",
    "                        })\n",
    "                        \n",
    "                        orig_frame = vis_batch[0].copy()\n",
    "                        if orig_frame.dtype != np.uint8:\n",
    "                            orig_frame = ((orig_frame * 255) if orig_frame.max() <= 1.0 else orig_frame).astype(np.uint8)\n",
    "                        original_frames.append(orig_frame)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Heatmap error at step {step}: {e}\")\n",
    "                \n",
    "                obs, reward, done_batch, info = env.step(full_actions)\n",
    "                \n",
    "                for i in alive:\n",
    "                    if reward[i] != 0.0:\n",
    "                        successes[i] = True\n",
    "                    dones[i] = dones[i] or bool(done_batch[i])\n",
    "                \n",
    "                if save_videos and step % video_skip == 0:\n",
    "                    video_writer.append_vector_obs(obs, dones, camera_name=cam_key)\n",
    "        \n",
    "        success_rate = float(successes.mean())\n",
    "        print(f\"   Result: {successes.sum()}/{env_num} successes ({success_rate:.0%})\")\n",
    "        \n",
    "        results = {\n",
    "            'success_rate': success_rate,\n",
    "            'episodes': int(env_num),\n",
    "            'max_steps': int(max_steps),\n",
    "            'task_id': task_id,\n",
    "            'task_prompt': task_prompt\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        env.close()\n",
    "    \n",
    "    if save_videos:\n",
    "        src = Path(video_dir) / f\"task_{task_id:02d}\" / \"video.mp4\"\n",
    "        dst = Path(video_dir) / f\"task_{task_id:02d}.mp4\"\n",
    "        if src.exists():\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            src.rename(dst)\n",
    "            try:\n",
    "                src.parent.rmdir()\n",
    "            except OSError:\n",
    "                pass\n",
    "    \n",
    "    if generate_heatmaps and heatmap_frames and original_frames:\n",
    "        heatmap_video_path = str(Path(video_dir) / f\"task_{task_id:02d}_heatmaps.mp4\")\n",
    "        generate_heatmap_video(heatmap_frames, original_frames, heatmap_video_path, fps=5)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for task_id in range(10):\n",
    "    result = evaluate_model(\n",
    "        task_id=task_id,\n",
    "        env_num=5,\n",
    "        max_steps=600,\n",
    "        save_videos=True,\n",
    "        generate_heatmaps=True,\n",
    "        heatmap_interval=10\n",
    "    )\n",
    "    final_results.append(result['success_rate'])\n",
    "\n",
    "mean_success = np.mean(final_results)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean success rate: {mean_success:.2%}\")\n",
    "print(f\"Success rates: {[f'{r:.0%}' for r in final_results]}\")\n",
    "\n",
    "top_3 = sorted(range(len(final_results)), key=lambda i: final_results[i], reverse=True)[:3]\n",
    "print(f\"Best tasks: {top_3} ({[f'{final_results[i]:.0%}' for i in top_3]})\")\n",
    "\n",
    "worst_3 = sorted(range(len(final_results)), key=lambda i: final_results[i])[:3]\n",
    "print(f\"Worst tasks: {worst_3} ({[f'{final_results[i]:.0%}' for i in worst_3]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
